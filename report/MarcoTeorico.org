#+TITLE: Reconocimiento visual empleando técnicas de apredizaje profundo
#+AUTHOR:  Rubén Ezequiel Torti López
#+EMAIL:   ret0110@famaf.unc.edu.ar
#+CREATOR: Rubén Ezequiel Torti López
#+LANGUAGE: es
#+OPTIONS: H:5 title:nil creator:nil timestamp:nil skip:nil toc:nil
#+STARTUP: indent hideblocks
#+TAGS: noexport(n)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+PROPERTY: session *R* 

#+BEGIN_COMMENT
######################## ATENCION ###################################

La generacion de figuras de matplotlib esta desactivada para generar
el reporte mas rapido (algunos code blocks incluyen tareas de
clasificacion REALES hechas con scipy/numpy y tardan un toque en
evaluarse).

Como consecuencia, para exportar este documento usando Emacs org-mode
es necesario primero generar todas las figuras de matplotlib mediante
el shortcut C-c C-v C-b.

Luego se exporta a un LaTEX con C-c C-e l (dependiendo de la version
de Emacs, es probable que haya que apretar "l" dos veces para
seleccionar la opcion "LaTEX").

Luego se corre el script ./compile_report.sh en esta misma carpeta
para generar y abrir el pdf.

Una vez generadas las figuras, ya no es necesario correr los code
blocks de nuevo.
#####################################################################
#+END_COMMENT

#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{ifthen,figlatex}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEZ_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage{url}\urlstyle{sf}
#+LATEX_HEADER: \usepackage{amscd}
#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATES_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage{babelbib}

#+LATEX_HEADER: \newcommand{\cl}{\textit{clasificadores lineales}}
#+LATEX_HEADER: \newcommand{\losss}{\textit{funciones de pérdida}}
#+LATEX_HEADER: \newcommand{\dg}{\textit{descenso de gradiente}}
#+LATEX_HEADER: \newcommand{\back}{\textit{backpropagation}}
#+LATEX_HEADER: \newcommand{\nn}{\textit{redes neuronales}}
#+LATEX_HEADER: \newcommand{\svms}{\textit{Support Vector Machines}}
#+LATEX_HEADER: \newcommand{\bow}{\textit{Bag of Words}}
#+LATEX_HEADER: \newcommand{\features}{\textit{features}}
#+LATEX_HEADER: \newcommand{\scores}{\textit{scores}}
#+LATEX_HEADER: \newcommand{\sift}{\textit{SIFT}}
#+LATEX_HEADER: \newcommand{\weights}{\(\boldsymbol{W}\)}
#+LATEX_HEADER: \newcommand{\img}{\(\boldsymbol{x_i}\)}
#+LATEX_HEADER: \newcommand{\bias}{\(\boldsymbol{b}\)}
#+LATEX_HEADER: \newcommand{\func}{\(\boldsymbol{f}\)}
#+LATEX_HEADER: \newcommand{\loss}{\(\boldsymbol{L}\)}

#+LATEX_HEADER: \newcommand{\ml}{\textit{machine learning}}
#+LATEX_HEADER: \newcommand{\ML}{\textit{Machine Learning}}
#+LATEX_HEADER: \newcommand{\dl}{\textit{deep learning}}
#+LATEX_HEADER: \newcommand{\DL}{\textit{Deep Learning}}
#+LATEX_HEADER: \newcommand{\cnn}{\textit{convolutional neural networks}}
#+LATEX_HEADER: \newcommand{\CNN}{\textit{Convolutional Neural Networks}}

* Introducción

Para los seres humanos, percibir el mundo que nos rodea es una tarea
fácil. Millones de años de evolución nos han dotado con un sistema
visual altamente sofisticado que nos permite reconocer patrones muy
complejos del mundo tridimensional en el que vivimos. Distinguir
formas, sombras y color, o incluso cosas más generales, como
movimiento, potenciales amenazas y rostros de conocidos, son algunas
de las actividades que nuestros cerebros realizan casi de manera
inconsciente. Y si bien estas tareas pueden ser fáciles para nosotros,
no es tal el caso para las computadoras.

Richard Szeliski caracteriza a la visión por computadoras como un
\textit{problema inverso}, es decir, se busca describir el mundo que
uno ve en una o más imágenes y reconstruir sus propiedades tales como
forma, iluminación y distribuciones de color \cite{szeliski}

Una de las áreas principales de de la visión por computadoras es la
clasificación de imágenes. Es decir, dada una imagen y un conjunto de
categorías, determinar a cual de las categorías pertenece esa
imagen. Tener un buen entendimiento de los algoritmos de clasificación
es crucial para desarrollar otras tareas dentro de la visión por
computadoras, dado que muchos problemas pueden ser reducidos a
clasificación: detección de objetos, descripcion de imágenes y
segmentación entre otros.

Sin embargo, es un problema más difícil de lo que aparenta. Una
fotografía es solamente un conjunto de números (píxeles), entonces
¿cómo darle significado semántico a un conjunto de números? Se podría
pensar en elaborar alguna métrica de distancia con los píxeles otra
imagen la cual sepamos está en la misma categoría, y si la distancia
es menor a un cierto umbral sabríamos que ambas pertenecen a la misma
clase. Sin embargo este enfoque carece de robustez, ya que el menor
cambio de \textit{iluminación} (o sea, variaciones en los píxeles)
podría alterar las métricas y confundir a nuestro modelo. No solamente
eso, los objetos en las imágenes podrían estar ocultos por el ambiente
(\textit{oclusión}), o en una posición diferente
(\textit{deformación}), o ser exactamente iguales, pero variar en
colores y pequeños detalles (\textit{variación intraclase}).

Diferentes métodos se han utilizado a lo largo de la historia para
taclear este problema: \textit{máquinas de vectores de soporte},
árboles de decisión, \textit{bolsas de palabras}, etc. En la mayoría
de los casos las \features y descriptores que se extraían de las
imágenes debían ser implementadas manualmente (por ejemplo:
histogramas de colores o descriptores \sift \cite{Lowe-SIFT}).

En los últimos años toda la parafernalia relativa a la clasificación
de imágenes fue ampliamente superada por las Redes Neuronales
Convolucionales. Desde el año 2010, todos los equipos ganadores del
desafío Imagenet usaron Redes Neuronales Convolucionales, cada vez con
resultados más precisos \cite{imagenet}.

A pesar de haber tenido su golpe de popularidad en los últimos años,
los primeros esbozos de modelar redes neuronales artificiales datan de
1958, cuando Frank Rosenblatt ideó el \textit{perceptron}, un
algoritmo para reconocimiento de patrones basado en una red de dos
capas de aprendizaje \cite{perceptron}. Sin embargo, en 1969 se
estableció que el poder de cómputo disponible en ese entonces no
bastaba para poder entrenar y correr grandes redes neuronales,
implicando que el área se estancara durante años \cite{minsky}. Tan
así es, que recién en 2006, con el abaratamiento de costos en hardware
de alto desempeño se pudieron implementar arquitecturas más complejas
(no necesariamente nuevas) y redes neuronales más profundas, algo que
se conoce como Aprendizaje Profundo (\textit{Deep Learning}).

Para poder entrenar Redes Neuronales Profundas, es necesario contar
con un conjunto de datos anotados muy grande, cuyos tamaños pueden ir
de las decenas de miles hasta millones de imágenes. Generalmente se
requiere un gran esfuerzo humano para etiquetar tantas imágenes.

Actualmente contamos con una increíble cantidad de imágenes para
etiquetar. Para 2016, Cisco estima que el 51\% del trafico de Internet
va a provenir de dispositivos WiFi, tales como celulares,
\textit{tablets}, \textit{smart TV's}, etc.  Más aún, se estima que
para el 2019 el 80\% del tráfico IP va a ser en forma de píxeles
(multimedia), superando al 67\% que existente en 2014
\cite{ciscostats}. Como tendencia podemos nombrar a Youtube, donde la
mitad de los videos son subidos desde dispositivos móviles
\cite{youtustats}. Por otro lado, las redes sociales más populares
como Facebook, Flickr o Instagram almacenan una gigantesca cantidad de
imágenes, contando la última con más de 80 millones de imágenes
subidas por día \cite{instastats}. Sin embargo, es practicamente
imposible contar con los recursos suficientes para anotar tantos
videos, imágenes y demás contenido multimedia.

Una posible solución al entrenamiento de redes profundas cuando no se
puede anotar una gran cantidad de datos es la propuesta por Agrawal et
al. \cite{LSM2015}. En la misma proponen utilizar información
odométrica disponible en cámaras (giróscopos, acelerómetros, etc) para
pre-entrenar redes neuronales profundas, y luego realizar una
transferencia de aprendizaje sobre un conjunto de datos anotados que
se desee, obteniendo la ventaja de no necesitar un conjunto tan grande
para lograr buena precisión.

En este trabajo final nos proponemos reproducir este paper, analizando
cada paso para luego proponer mejoras y aplicaciones al mundo real.

El trabajo está organizado como sigue: en la Sección \ref{sec:marco}
se introducirán conceptos del aprendizaje automático y las redes
neuronales artificiales para concluir con redes convolucionales, en la
Sección \ref{sec:siamesa} se presentará un método para entrenar
modelos de aprendizaje profundo mediante redes siamesas, que luego se
utilizará para reproducir los resultados de \cite{LSM2015} en la
Sección \ref{sec:odometry}. Finalmente, la Sección \ref{sec:concl}
presenta pensamientos finales y trabajo a futuro.

* Marco Teórico
#+LaTeX: \label{sec:marco}
** Aprendizaje Automático

Las técnicas de aprendizaje automático tienen como objetivo
identificar patrones en conjuntos de datos utilizando herramientas de
la estadística, teoría de la información, cálculo y optimización entre
otras. De esta manera se pueden automatizar tareas, como por ejemplo
del filtro de correo basura (\textit{spam}), la verificación de
rostros o incluso predicción de precios en el mercado.

El aprendizaje automático adquiere relevancia cuando las tareas que se
desean automatizar son demasiado complejas para programarse
directamente. Por ejemplo la verificación de rostros debe tener en
cuenta detalles como variaciones en sombra, color, orientación, por no
mencionar las diferentes características que hay que extraer de una
cara para diferenciarla de otra (arrugas, prominencias y otros
rasgos).  Otro caso es el análisis de grandes volúmenes de datos, como
estadísticas del clima para crear nuevos modelos.

Hay varios paradigmas o ejes dentro del aprendizaje automatico que
definen los tipos de algoritmos, las técnicas de
entrenamiento y las potenciales aplicaciones de esos modelos:

**** Aprendizaje supervisado vs. no supervisado

Cuando se poseen anotaciones o alguna clase de etiqueta sobre los
datos a aprender, hablamos de aprendizaje supervisado. Retomando el
caso del verificador de rostros, las etiquetas serían el nombre o
algun identificador de cada persona y nuestro clasificador aprendería
a diferenciar las caras tomando como referencia las anotaciones.

Por otro lado, cuando los datos no estan categorizados de antemano
hablamos de aprendizaje no supervisado. Por ejemplo, si se contara con
una lista de casas con sus respectivos precios, su área en metros
cúbicos y cantidad de habitaciones y quisieramos encontrar alguna
relación entre ellas. Los algoritmos no supervisados trabajan
netamente sobre los datos \textit{tal como están}.

**** Aprendizaje pasivo vs. activo

El aprendizaje pasivo implica utilizar solamente los datos ya
existentes. El aprendizaje activo se refiere a interactuar con el
ambiente para obtener información, como por ejemplo preguntar a un
usuario si un rostro es de cierta persona y utilizar los datos
proporcionados durante su etrenamiento.

**** Aprendizaje \textit{online} vs. estadistico (\textit{batch learning})

En el aprendizaje \textit{online} los datos estan disponibles de
manera secuencial, actualizando el modelo en cada paso para lograr
mejores predicciones/clasificaciones. En el aprendizaje estadístico
primero se analiza una gran cantidad de datos (tal vez la totalidad
del conjunto) y solamente luego de haberlos analizado se pueden
obtener conclusiones o un modelo final.

*** Clasificadores lineales

Un clasificador lineal combina linealmente las caracteristicas (o
\textit{features}) de los datos de entrada para determinar a que clase
pertenecen los mismos, usualmente entrenado mediante técnicas de
aprendizaje supervisado.

Imaginemos que queremos clasificar imágenes, es decir, asignar una
etiqueta a un conjunto de píxeles. Para ello vamos a definir una
función \func{} que mapee píxeles \(\boldsymbol{x}\) a probablidades
de cada etiqueta (\textit{scores}). Supongamos que contamos con un
conjunto de datos de imágenes \(\boldsymbol{x_i} \in
\boldsymbol{R^{D}}\), donde \(\boldsymbol{i = 1\cdots N}\),
\(\boldsymbol{D}\) es la dimensión de cada imagen y \(\boldsymbol{y_i
= 1\cdots K}\) es la etiqueta asociada. Es decir, tenemos
\(\boldsymbol{N}\) imágenes y \(\boldsymbol{K}\) categorías.

Definamos ahora una función \(\boldsymbol{f\colon R^{D} \mapsto
R^{K}}\) como un mapeo lineal entre píxeles y \scores:

\begin{equation}
     \boldsymbol{f(x_i, W, b)= W x_i + b}
\end{equation}

Asumimos que la imagen \img{} es un vector de una sola columna
\([D \times 1]\), \weights{} es una matriz \([K \times D]\) y \bias{} es
otro vector \([K \times 1]\). A menudo la matriz \weights{} es llamada
los \textit{pesos} de \func{}, y a \bias{} el \textit{vector de sesgo}
dado que influencia los \scores{} de salida, pero sin interactuar con
\img{}.

Para entender mejor a los clasificadores lineales, podemos verlos de
la siguiente manera: si la imagen tiene \(32 \times 32\) píxeles y la
representamos con un vector columna de dimensión \(D\) (en este caso
\(D=1024=32 \times 32\)), entonces en ese espacio \textit{D-dimensional} la
imagen es solamente un punto. Como se observa en la Figura
\ref{fig:cl} de manera simplificada, nuestro clasificador lineal
define una ``línea'' (un hiperplano en realidad) que separa cada clase
dentro de ese espacio multidimensional. Notar que en realidad la
multiplicación \(\boldsymbol{W x_i}\) está evaluado \(\boldsymbol{K}\)
clasificadores en paralelo, donde cada uno es una fila de \weights{}.


#+name: linear-classifier
#+begin_src python :session :exports none :results silent :cache yes :eval no-export
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import style
import os
import random
import numpy as np
from sklearn import svm

style.use("ggplot")

try:
    os.mkdir("images")
except:
    pass

matplotlib.use('Agg')

N = 50
x1 = np.random.normal(2, 0.5, size=50)
y1 = np.random.normal(2, 0.5, size=50)

x2 = np.random.normal(3.5, 0.5, size=50)
y2 = np.random.normal(3.5, 0.5, size=50)

fig = plt.figure()
axes = plt.gca()
axes.set_xlim([0,5])
axes.set_ylim([0,5])
ax1 = fig.add_subplot(111)
ax1.set_axis_bgcolor('white')  
ax1.grid(False, which='both')
#plt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off')
#plt.tick_params(axis='y', which='both', bottom='off', top='off', labelbottom='off')
ax1.axes.get_xaxis().set_ticks([])
ax1.axes.get_yaxis().set_ticks([])
ax1.spines['right'].set_visible(False)
ax1.spines['top'].set_visible(False)
for spine in ['left', 'bottom']:
    ax1.spines[spine].set_color('k')

ax1.scatter(x1, y1, s=20, c='b', marker="s")
ax1.scatter(x2, y2, s=20, c='r', marker="o")

# Fit a linear classifier
X = zip(x1,y1) + zip(x2,y2)
Y = [0]*50 + [1]*50 # 2 classes
linear_clf = svm.LinearSVC()
linear_clf.fit(X, Y)

# Get parameters and plot
coef = linear_clf.coef_[0]
a = -coef[0] / coef[1]
xx = np.linspace(-10,10)

yy = a * xx - linear_clf.intercept_[0] / coef[1]
yy2 = a * xx - linear_clf.intercept_[0] / coef[1] + 1
yy3 = a * xx - linear_clf.intercept_[0] / coef[1] - 1

ax1.plot(xx, yy, 'k-',  c='orchid', label=r'$Wx + b$')
ax1.plot(xx, yy2, '--',  c='mediumaquamarine', label=r'$Wx + (b + m)$')
ax1.plot(xx, yy3, '--',  c='sandybrown', label=r'$Wx + (b - m)$')

plt.legend(frameon=False)

plt.savefig('images/linear-classifier.pdf')
#+end_src

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:cl
#+caption: Clasificador lineal. Cada punto representa una muestra en un espacio de dimensión \(\boldsymbol{D}=2\) con \(\boldsymbol{K}=2\) categorías. La tarea del clasificador es establecer un hiperplano entre las dos clases de datos, definido por la ecuación \(f(x_i, W, b)= W x_i + b\). A modo de ejemplo están graficados dos clasificadores lineales más con el \textit{vector de sesgo} ligeramente modificado. Se puede observar que \bias{} no afecta al clasificador sino que simplemente lo traslada a lo largo de las dimensiones.
[[file:images/linear-classifier.pdf]]

Mas adelante veremos como definir \weights{} y \bias{} para obtener un
buen clasificador.

*** Entrenamiento
En el caso de los clasificadores lineales, entrenar un modelo se
traduce en encontrar buenos valores de \weights{} y \bias{} que
minimicen el error.

Es muy común, cuando se cuenta con un conjunto de datos lo
suficientemente grande, dividirlo en al menos 3 subconjuntos
disjuntos: uno para entrenar el modelo, un segundo para validar el
modelo durante el entrenamiento y un tercero para probar el modelo una
vez entrenado. De esta manera se puede medir que tan bien el modelo
aprendió características relevantes a la clasificación y las pudo
aplicar a un conjunto de datos completamente nuevo (conjunto de
pruebas). Si la precisión que obtuvo nuestro modelo sobre este
conjunto de pruebas es muy baja, es un síntoma de que algo no anda
bien en el entrenamiento (ver problema de \textit{sobre-ajuste} en la
Sección \ref{sec:regular}).

A grandes rasgos, podemos describir el proceso de entrenamiento de un
clasificador de la siguiente manera:

\begin{enumerate}

\item Primero se mide el error actual del model con el conjunto (o un
      subconjunto) de datos de entrenamiento

\item Luego se actualizan los parámetros del clasificador (\weights{} y
      \bias{}) para minimizar ese error.

\item Se repiten los pasos anteriores hasta lograr la precisión deseada

\end{enumerate}

Por lo tanto hay dos aspectos a tener en cuenta antes de
entrenar un modelo: cómo medir efectivamente la tasa de error y cómo
actualizar sus parámetros para minimizarla. Para el primer caso se
define lo que se llama una \textit{función de pérdida}, mientras que
para el segundo analizaremos una técnica muy utilizada en aprendizaje
automático denominada \textit{descenso de gradiente}. Esto no
significa que sea la única alternativa para entrenar modelos, pero al
ser ampliamente utilizada en redes neuronales artifiales será la única
que analizaremos.

**** Función de costo

Una función de costo nos ayuda a saber que tan bien o mal está
actuando nuestro clasificador. Es decir, si la tasa de error del
clasificador es muy alta, el costo o la \textit{pérdida} será muy alta
y viceversa. Hay muchos tipos de funciones de costo, pero la idea
subyacente es la misma y puede ser expresada en la siguiente ecuación:

\begin{equation}
\boldsymbol{L}(\theta) = \frac{1}{N} \sum^{N}_{i} L(f(x_i;\theta), y_i)
\end{equation}

donde \(L\) es la función de pérdidad individual de cada muestra en el
conjunto de datos, \(f(x_i;\theta)\) es la predicción del modelo sobre
una muestra \(x_i\) con parámetros \(\theta\), \(y_i\) es el objetivo
(por ejemplo, la etiqueta de cada muestra del conjunto de datos en una
tarea de clasificación). Notar que para el caso de un clasificador
lineal los parámetros \(\theta\) son \weights{} y \bias{}. De ahora en
adelante utilizaremos \(\theta\) o \weights{} indiferentemente para
hablar de los parámetros de nuestro modelo.

Un ejemplo de función de perdida popular es la función de pérdida de
\textit{máquinas de vectores de soporte multiclase}. Sea \(f(x_i;
\theta)_j\) el puntaje asignado por el clasificador \(f\) a la clase
\(j\) con \(x_i\) como dato de entrada y parámetros \(\theta\) y sea
\(f(x_i, \theta)_{y_i}\) el puntaje asignado por \(f\) a la clase
verdadera de \(x_i\), o sea \(y_i\), entonces la pérdida para \(x_i\)
se calcula de la siguiente manera:

\begin{equation}
     L_i = \sum_{j \neq y_i} \max{(0, f(x_i; \theta)_j - f(x_i; \theta)_{y_{i}} + \Delta) }
\end{equation}

Se puede observar que esta función de pérdida busca que la clase
correcta tenga un puntaje más alto que las otras por un margen
\(\Delta\).

Cuando tenemos una función con la forma \( \max{(0,\_)} \) a menudo se
la llama función de pérdida bisagra (\textit{hinge loss} en inglés).

**** Descenso de Gradiente
#+LaTeX: \label{sec:sgd}

Ya contamos con una función para medir que tan bien o que tan mal está
comportándose nuestro modelo, la \textit{función de pérdida}. Como se
puede observar, esta función depende de nuestro \weights{} y las
imágenes (o \features{} que estemos usando). Nosotros no tenemos
control sobre nuestro conjunto de datos de entrenamiento, pero sí
podemos modificar los parámetros de \weights{} para producir la menor
pérdida posible.

El descenso de gradiente se utiliza para optimizar los pesos partiendo
de la premisa que el modelo es diferenciable con respecto a
\weights{}. Dado que queremos minimizar la funcion de costo \loss{},
lo que vamos a hacer es calcular su gradiente \(\boldsymbol{\nabla
L}\) respecto a cada parámetro y luego modificar cada uno ligeramente
con el objetivo de alcanzar un mínimo en la función. Entonces, si
\(\theta_{n}\) son nuestros parámetros en el paso \(n\) del
entrenamiento, calculamos \(\theta_{n+1}\) de la siguiente manera:

\begin{equation}
    \theta_{n+1} = \theta_n - \epsilon \frac{1}{m} \sum^{m}_{i} \nabla_{\theta_{n}} L(f(x_i; \theta_n), y_i)
\end{equation}

Donde \(\epsilon\) es conocido como la \textit{tasa de aprendizaje} y
\(m\) es la cantidad de elementos en el conjunto de datos. En la
ecuación se puede observar que se modifican los parámetros con
respecto a la dirección opuesta al gradiente, dado que el mismo indica
la dirección de crecimiento de una función pero nosotros queremos
encontrar un mínimo (Figura \ref{fig:gd}).

#+name: gradient-descent
#+begin_src python :session :exports none :results silent :cache yes :eval no-export
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.mlab as mlab
from matplotlib import style
import numpy as np
from sklearn import svm

style.use("ggplot")

matplotlib.use('Agg')

fig = plt.figure()
axes = plt.gca()
axes.set_xlim([-2.5,2.5])
axes.set_ylim([-3,3])
ax1 = fig.add_subplot(111)
ax1.set_axis_bgcolor('white')  
ax1.grid(False, which='both')
ax1.axes.get_xaxis().set_ticks([])
ax1.axes.get_yaxis().set_ticks([])
ax1.spines['right'].set_visible(False)
ax1.spines['top'].set_visible(False)
ax1.spines['left'].set_visible(False)
ax1.spines['bottom'].set_visible(False)

delta = 0.025
x = np.arange(-3.0, 3.0, delta)
y = np.arange(-2.0, 2.0, delta)
X, Y = np.meshgrid(x, y)
Z1 = mlab.bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)

CS = plt.contour(X, Y, Z1)

ax1.annotate(r'$w_3$', fontsize=15, xy=(-0.22, 0.16), xytext=(-0.49, 0.49),
             arrowprops=dict(width=2, headwidth=4, facecolor='black', shrink=0.01),)

ax1.annotate(r'$w_2$',  fontsize=15, xy=(-0.53, 0.53), xytext=(-0.85, 0.73),
             arrowprops=dict(width=2, headwidth=4, facecolor='black', shrink=0.01),)

ax1.annotate(r'$w_1$', fontsize=15, xy=(-0.90, 0.76), xytext=(-1.02, 1.27),
             arrowprops=dict(width=2, headwidth=4, facecolor='black', shrink=0.01),)

ax1.annotate(r'$w_0$', fontsize=15, xy=(-1.04, 1.30), xytext=(-1.61, 1.25),
             arrowprops=dict(width=2, headwidth=4, facecolor='black', shrink=0.01),)

plt.savefig('images/gradient-descent.pdf')
#+end_src

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:gd
#+caption: Descenso de gradiente. Sea \(w\) un parámetro de \(\theta\), el gráfico ilustra cómo nuestra función de clasificación \(f\) alcaza un mínimo en \(w\) a medida que se actualiza su valor mediante la substracción del gradiente calculado en ese parámetro. Puesto de otra manera, podemos imaginar a \(w\) como una pelota que se suelta desde la cima de una montaña y desciende por la fuerza de la gravedad hasta el valle (instancia \(w_3\)). Una vez que la pelota alcanza el punto más bajo, se detendrá.
[[file:images/gradient-descent.pdf]]

Existen variantes mas sofisticadas del descenso de gradiente
(\textit{Momentum}, \textit{Nesterov Momentum}, \textit{Adagrad},
\textit{Rmsprop} entre otros). Lo más comun es utilizar alguna de
ellas con una técnica llamada \textit{descenso de gradiente
estocástico}, que se basa en calcular el gradiente de un subconjunto
del total de datos (\textit{batch}) y actualizar \weights{} al final
de cada \textit{batch}. Esto es muy útil dado que es
computacionalmente costoso calcular el gradiente de todo un conjunto
de datos con miles de imágenes a la vez y calcular el gradiente de un
\textit{batch} aproxima bastante bien el gradiente del total.

**** Clasificador \textit{Softmax}
Antes de saltar de lleno a las redes neuronales artificiales vamos a
describir brevemente un tipo de clasificador muy utilizado en las
mismas, el clasificador \textit{Softmax}.

La función \textit{Softmax} tiene la siguiente forma:

\begin{equation}
    \sigma(x)_j =  \frac{e^{f(x;\theta)_{j}}} {\sum_{k} e^{f(x;\theta)_{k}}}
\end{equation}

Actúa tomando un vector de valores reales arbitrarios y
transformándolos en un vector de probabilidades normalizadas (cuya
suma da uno).

Por lo tanto el clasificador Softmax tiene una función de pérdida
diferente. En vez de tratar a los resultados como puntajes para cada
clase (lo cual puede ser confuso y dificíl de comparar), Softmax
devuelve probabilidades normalizadas para cada clase.

Un clasificador Softmax no modifica la funcion \(f\) que ya
conocemos, pero sí interpreta los puntajes como probabilidades
logarítmicas sin normalizar, reemplazando la pérdida bisagra por una
\textit{entropía cruzada}:

\begin{equation}
     L_i = - \log \bigg( \frac{e^{f(x_i, \theta)_{y_{i}}}} {\sum_j e^{f(x_i, \theta)_j}}\bigg)
\end{equation}

Que es equivalente a:

\begin{equation}
     L_i = - f(x_i, \theta)_{y_{i}} + \log {\Big( \sum_j e^{f(x_i, \theta)_j}\Big)}
\end{equation}

Donde \(f_{y_{i}}\) representa el elemento \textit{j}-ésimo del vector
de puntajes calculado por \(f\). Nuevamente, la pérdida total es el
promedio de las pérdidas de cada imagen.

** Redes Neuronales Artificiales

Hasta ahora analizamos clasificadores lineales y un tipo particular
llamado softmax. Si conectáramos la salida de un clasificador lineal
\(s_1=W_1x+b_1\) con la entrada de otro clasificador \(s_2=W_2y+b_2\),
entonces obtendríamos un tercero:

\begin{equation}
s_3 = W_2 (W_1x + b_1) + b_2 = (W_2 W_1) x + (W_2 b_1) + b_2
\end{equation}

\begin{equation}
s_3 = W_3 x + b_3
\end{equation}

Es fácil hacer un chequeo de dimensiones para ver que efectivamente
podemos ``colapsar'' las matrices \(W_2\) y \(W_1\) en una sola, por lo
cual terminamos con otro clasificador lineal.

Notemos que por más que combinemos miles de clasificadores lineales
vamos a obtener un nuevo clasificador también lineal.  Una manera de
romper la linealidad de estas ``capas'' de clasificadores es, por
ejemplo, agregar lo que se llama \textit{función de activación}:
 
\begin{equation}
    s = W_2 \max{(0, W_1 x + b_1)} + b_2
\end{equation}
 
 
Lo que acabamos de definir es una red neuronal básica de dos capas, de
una neurona cada una.

Una sola neurona puede funcionar como un clasificador también (notar
el parecido con los clasificadores lineales), siempre que se eliga la
función de pérdida adecuada.
 
*** Funciones de activación comunes

Se han propuesto varias funciones de activación a lo largo de los
años. 

Inicialmente se intentó simular el comportamiento de las conexiones
sinápticas mediante la función de activación Sigmoide (\(\sigma\)),
por tener la propiedad de transformar la entrada a un rango entre 0 y
1, y tener además una derivada fácil de calcular (útil para
\textit{backpropagation}).  Luego se propuso la Tangente Hiperbólica
(\(\tanh\)), pero ambas fueron desplazadas en favor de las unidades
\textit{ReLU}, o \textit{Rectifier Linear Unit} en inglés, que son más
fáciles de calcular y no sufren del problema de saturación de
gradiente que poseen \(\sigma\) y \(\tanh\).

Nos concentraremos entonces en las unidades \textit{ReLU}, actualmente
muy populares en las redes convolucionales debido a sus buenas
propiedades.

Hay tres tipos de rectificadores lineales:

**** \textit{ReLU}
Una unidad ReLU establece un umbral en \(0\) a la salida de la
neurona. Es decir, la activación de una neurona va a ser \(0\) si su
salida fue negativa o un numero positivo en caso contrario (Figura
\ref{fig:relu}.a):

\begin{equation}
f(x) = \max{(0,x)}
\end{equation}

Comparada con \(\sigma\) y \(\tanh\), requiere menos operaciones, no
es saturante y converge hasta 6 veces más rápido que las funciones
sigmoide y tanh \cite{NIPS2012_4824}.

Una desventaja de las \textit{ReLU} es que pueden provocar la ``muerte''
de neuronas durante el entrenamiento. Si un gran gradiente fluye a
través de una \textit{ReLU} durante el proceso de
\textit{backpropagation} entonces va a actualizar los pesos de esa
neurona de tal manera que no se vuelva a activar. Pensemos que el
proceso de actualización de \weights{} implica restar un porcentaje
del gradiente en \weights{}. Si el gradiente es muy grande entonces
los pesos sobre los que se realice la actualización terminarán siendo
muy pequeños. Como consecuencia, la unidad \textit{ReLU} no volverá a
activarse, pues sus valores de entrada siempre van a ser valores
negativos. Esta situación puede agravarse si la tasa de
aprendizaje es muy grande.

Una vez que la ReLU alcanza este estado, es improbable que vuelva a
activarse, dado que su gradiente en \(0\) es también \(0\), por lo que
un entrenamiendo mediante descenso de gradiente y
\textit{backpropagation} no va a modificar los pesos locales, dejando
a esa neurona ``muerta''.

**** \textit{Leaky ReLU}

Se puede solucionar el problema de la muerte de neuronas agregando
una pequeña pendiente negativa (de 0.01 por ejemplo) en los valores
negativos de la \textit{ReLU}. Esta función de activación es la que se
conoce como \textit{Leaky ReLU} \cite{zhang2014improving} (Figura
\ref{fig:relu}.b):

\begin{equation}
f(x) = 1(x<0)(\alpha x) + 1(x >= 0)(x)
\end{equation}

De esta manera nos aseguramos que al menos un pequeño gradiente fluya
durante \textit{backpropagation} cuando la neurona emite resultados
negativos, permitiendo que se normalicen los pesos a mediano/largo
plazo. Sin embargo no está demostrado del todo que las \textit{Leaky
ReLU} presenten una mejora sustancial en el entrenamiento de las
redes, por lo que las \{ReLU} convencionales siguen siendo ampliamente
usadas.

**** \textit{Maxout}

\begin{equation}
f(x) = \max{(w^{T}_{1} x + b_{1}, w^{T}_{2} x + b_{2})}
\end{equation}

\textit{Maxout} \cite{Maxout} es una generalización de las funciones
\textit{ReLU}, y obtiene lo mejor de los dos mundos: por un lado la
forma lineal y no saturante de las \textit{ReLUs} y por el otro evita
el problema de las neuronas que se mueren. A pesar de ello tiene la
desventaja de duplicar los parámetros para cada neurona, lo cual no
siempre es deseable, pues implica más tiempo de entrenamiento y
más consumo de memoria y recursos, sobre todo en redes profundas.
 
Notar que una \textit{ReLU} normal es básicamente una \textit{maxout}
con \(w_1,b_1 = 0\).

#+name: relus
#+begin_src python :session :exports none :results silent :cache yes :eval no-export
import matplotlib
matplotlib.use('Agg')

import matplotlib.pyplot as plt
from matplotlib import style
import numpy as np

style.use("ggplot")

def config_ax(ax):
    ax.set_axis_bgcolor('white')
    ax.grid(False, which='both')
    ax.axes.get_xaxis().set_ticks([])
    ax.axes.get_yaxis().set_ticks([])
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.plot((0, 0), (0, 5), 'grey')
    ax.plot((-10, 10), (0, 0), 'grey')

#fig, axes = plt.subplots(nrows=1, ncols=1)
fig = plt.figure(figsize=(4, 4))
ax = plt.subplot(111)
#for ax in axes:
config_ax(ax)
ax.set_xlim([-5, 5])
ax.set_ylim([-1, 10])
X = np.arange(-10, 10)
ax.plot(X, np.maximum(0, X))
#ax.set_xlabel('x')
ax.text(0,-0.5, "0")
ax.text(4.7,-0.3, r'$+x$')
ax.text(-5,-0.3, r'$-x$')
ax.text(-0.6,4.6, r'$+y$')
#h = ax.set_ylabel('y')
#h.set_rotation(0)
plt.tight_layout()
plt.savefig('images/relu1.pdf')


fig = plt.figure(figsize=(4, 4))
ax = plt.subplot(111)
#for ax in axes:
config_ax(ax)
ax.set_xlim([-5, 5])
ax.set_ylim([-1, 10])
X = np.arange(0, 10)
Y = np.arange(0, 10)
X2 = np.arange(-10, 0)
Y2 = X2 *  0.1
X = np.append(X2, X)
Y = np.append(Y2, Y)
ax.text(0,-0.5, "0")
ax.text(4.7,-0.3, r'$+x$')
ax.text(-5,-0.3, r'$-x$')
ax.text(-0.6,4.6, r'$+y$')
ax.plot(X, Y)
plt.tight_layout()
plt.savefig('images/relu2.pdf')
#+end_src

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{images/relu1.pdf}
        \caption{\textit{ReLU}}
        \label{fig:relu1}
    \end{subfigure}
\quad
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{images/relu2.pdf}
        \caption{\textit{Leaky ReLU}}
        \label{fig:relu2}
    \end{subfigure}
    \caption{\textit{ReLU} vs. \textit{Leaky ReLU}. Se puede observar la pendiente negativa en \ref{fig:relu2} para \(x<0\), la cual produce un gradiente \(\neq 0\) y evita la muerte de neuronas.}\label{fig:relu}
\end{figure}
#+END_LaTeX

*** Inspiración biológica de las redes neuronales artificiales

Nuestro cerebro está compuesto de más de doscientos tipos de neuronas,
sin embargo nos centraremos en el tipo estándar de neurona, un modelo
\textit{näive}.

#+name: neuron
#+begin_src python :session :exports none :results silent :cache yes :eval no-export
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.cbook as cbook

image = plt.imread("images/neuron.png")

fig, ax = plt.subplots()
im = ax.imshow(image)

ax.annotate('dendrita', xy=(540.73, 63.20), xytext=(422.78, 26.51),
             arrowprops=dict(width=2, headwidth=6, facecolor='black', shrink=0.01),)
ax.annotate('nucleo', xy=(493.11,151.44), xytext=(355.95, 70.19),
             arrowprops=dict(width=2, headwidth=6, facecolor='black', shrink=0.01),)
ax.annotate('axon', xy=(302.65,143.58), xytext=(305.28, 230.95),
             arrowprops=dict(width=2, headwidth=6, facecolor='black', shrink=0.01),)
ax.annotate('terminal', xy=(144.52, 326.18), xytext=(192.57, 362.87),
             arrowprops=dict(width=2, headwidth=6, facecolor='black', shrink=0.01),)

plt.axis('off')

plt.savefig('images/neuron.pdf')
#+end_src

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:neuron
#+caption: Esquema \textit{näive} de una neurona biológica.
[[file:images/neuron.pdf]]

El cuerpo de la neurona esta compuesto de tres partes principales, el
arbol dendrítico, el cuerpo celular o soma y una extensión llamada
axón con ramificaciones en su extremo (Figura \ref{fig:neuron}). El
árbol dendrítico se conecta con los axones de otras neuronas y recibe
impulsos de éstas.  Una neurona puede estar conectada a cientos de
neuronas desde las que recibe impulsos, los cuales pueden o no activar
a la neurona en cuestión.  Si la esta se activa, entonces
retransmite el impulso a través de su axón.

Las conexiones del árbol dendrítico con otras neuronas no son todas
iguales.  Los axones y las extremidades del arbol tienen un
pequeño espacio entre ellos llamado espacio sináptico. Si la exitación
que proviene de un axón es suficiente, entonces el impulso se
transmite a la neurona, caso contrario no. Decimos que la primera es
una transmisión exitadora, pues aumenta la posibilidad de transmitir
el impulso, mientras que el otro caso se denomina transmisión
inhibidora, dado que reduce esa posibilidad.  De esta manera surge la
noción de \textit{pesos sinápticos}, que determinan cuando se activan
las conexiones y cuando no.  Otro aspecto a tener en cuenta es el
\textit{estímulo acumulativo}, en donde los estímulos de varios
receptores del arbol dendrítico se combinan de alguna manera para
activar o no la neurona (no hay punto intermedio). Este sistema de
tomar los impulsos de entrada y determinar si emitir el impulso de
salida se corresponde con nuestra \textit{función de activación} que
ya vimos. La función de activación representa entonces la frecuencia
con la que se activa la neurona.

#+name: neuron
#+begin_src python :session :exports none :results silent :cache yes :eval no-export
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.cbook as cbook

image = plt.imread("images/artificial-neuron.png")

fig, ax = plt.subplots()
im = ax.imshow(image)

props = {'ha': 'center', 'va': 'center'}
ax.text(75.50, 31.16, r'$x_1$', props, rotation=0, fontsize=15)
ax.text(75.50, 237.25, r'$x_2$', props, rotation=0, fontsize=15)
ax.text(75.50, 361.72, r'$x_3$', props, rotation=0, fontsize=15)
ax.text(84.50, 650.98, r'$x_{n-1}$', props, rotation=0, fontsize=15)
ax.text(75.50, 822.76, r'$x_n$', props, rotation=0, fontsize=15)

ax.text(419.30, 140.16, r'$\times w_1$', props, rotation=-25, fontsize=15)
ax.text(419.30, 285.41, r'$\times w_2$', props, rotation=-15, fontsize=15)
ax.text(419.30, 387.19, r'$\times w_3$', props, rotation=-8, fontsize=15)
ax.text(450.30, 598.43, r'$\times w_{n-1}$', props, rotation=11, fontsize=15)
ax.text(419.30, 729.36, r'$\times w_n$', props, rotation=28, fontsize=15)

ax.text(856.71, 470.73, r'$\mathbf{\sum_{i=0}^n w_i x_i + b}$', props, rotation=0, fontsize=18)

ax.text(1219.67, 433.80, r'$\mathbf{Y}$', props, rotation=0, fontsize=18)

ax.text(1529.7, 480.88, r'$\mathbf{f(Y)}$', props, rotation=0, fontsize=18)

ax.text(1808, 433.80, r'$\mathbf{\widetilde{Y}}$', props, rotation=0, fontsize=18)

plt.axis('off')

plt.savefig('images/artificial-neuron.pdf')
#+end_src

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:art-neuron
#+caption: Esquema de una neurona artificial. Las entradas (\(x\)) interactuan multiplicativamente con los pesos sinapticos (\(w\)) y se adicionan junto con un vector de sesgo (\(b\)). El resultado pasa por una funcion de activacion (\(f\)) que decide si la señal debe propagarse o no.
[[file:images/artificial-neuron.pdf]]

En la Figura \ref{fig:art-neuron} vemos un model formal de una neurona
estandar, en el que las entradas \(x_i\) interactúan
multiplicativamente con los pesos \(w_i\). En nuestra analogía con la
neurona biológica, esas son las sinapsis. Luego, el el cuerpo de la
célula, esos resultados se suman junto con un vector de sesgo y sobre
eso se calcula la función de activación que decide si transmitir o no
la salida.

** Entrenamiento de redes neuronales artificiales

Entrenar una red neuronal artificial no es muy distinto a entrenar un
clasificador lineal. Necesitamos definir una función de pérdida y un
método para ajustar los parámetros. Veremos además, como en otras
tareas de aprendizaje automático, que hay que tener en cuenta el
formato de los datos de entrada al model (tal vez eliminar ruido o
redundancia, normalizar las dimensiones). Esta tarea se denomina
preprocesamiento de datos.

También analizaremos varias técnicas para evitar el sobre-ajuste de
modelos. El sobre-ajuste surge cuando un modelo aprende ``ruido'' y
detalles particulares del conjunto de datos en vez de características
generales que ayuden a la tarea de clasificación. 

Finalizaremos esta sección con un análisis de la organización interna
de las redes neuronales artificiales y qué algoritmos se utilizan
para entrenar.

*** Preprocesamiento de datos

Antes de comenzar con el entrenamiento de una red neuronal artificial
es conveniente analizar los datos y si es necesario normalizarlos para
que todos estén en el mismo rango de valores.

El preprocesamiento de datos, como alinear imágenes o normalizar
valores ayuda a una mejor convergencia de los modelos. Las dos
técnicas más comunes de preprocesamiento de datos para redes
neuronales son la substracción de la media y la normalización.

**** Substracción de la media

Como su nombre lo indica, se le resta la media a cada elemento del
conjunto de datos con el objetivo de \textit{centrar} los datos
alrededor del origen en todas las dimensiones (Figura
\ref{fig:mean2}). En las redes convolucionales esto equivale a
restarle el valor medio de los píxeles a cada píxel de la imagen de
entrada.

#+name: mean-substraction
#+begin_src python :session :exports none :results silent :cache yes :eval no-export
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import style
from matplotlib import gridspec
import numpy as np

matplotlib.use('Agg')
style.use("ggplot")

def plot_norm(x, y, color, index='0'):
    fig = plt.figure(figsize=(4, 4))
    ax = plt.subplot(111)
    ax.set_axis_bgcolor('white')
    ax.grid(False, which='both')
    ax.axes.get_xaxis().set_ticks([])
    ax.axes.get_yaxis().set_ticks([])
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.plot((0, 0), (-4, 4), 'grey')
    ax.plot((-4, 4), (0, 0), 'grey')
    ax.set_xlim([-4, 4])
    ax.set_ylim([-4, 4])
    ax.scatter(x, y, s=30, c=color, marker="o")
    plt.tight_layout()
    plt.savefig('images/mean'+index+'.pdf')

   
N = 150
x1 = np.random.normal(2, 0.5, size=N)
y1 = np.random.normal(2, 0.5, size=N)
plot_norm(x1, y1, 'royalblue', '1')

mx = np.mean(x1)
my = np.mean(y1)
x2 = x1 - mx
y2 = y1 - my
plot_norm(x2, y2, 'seagreen', '2')

x3 = x2 / 2.0
y3 = y2 / 2.0
plot_norm(x3, y3, 'firebrick', '3')
#+end_src

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/mean1.pdf}
        \caption{Original}
        \label{fig:mean1}
    \end{subfigure}
\quad
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/mean2.pdf}
        \caption{Substraer media}
        \label{fig:mean2}
    \end{subfigure}
\quad
   \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/mean3.pdf}
        \caption{Normalizacion}
        \label{fig:mean3}
    \end{subfigure}
    \caption{Se puede observar en \ref{fig:mean2} cómo al substraer la media de \ref{fig:mean1} logramos ``centrar'' nuestro conjunto de datos. En \ref{fig:mean3} podemos apreciar los resultados de la normalización de datos, logrando que todos los elementos pertenezcan al mismo rango de valores.}\label{fig:mean}
\end{figure}
#+END_LaTeX

**** Normalización

Una manera de normalizar los datos es dividir cada dimensión por su
desviación estándar una vez que haya sido centrada en cero. De esta
manera se logra que las dimensiones tengan aproximadamente la misma
escala (Figura \ref{fig:mean3}). Notar que en general los píxeles
tienen valores en el rango de 0 a 255, por lo que sus dimensiones ya
se encuentran en escalas parecidas y cuando se trabaja con redes
convolucionales no es estrictamente necesario normalizar los datos de
entrada.

**** Otras maneras de preprocesar datos

A la hora de entrenar redes convolucionales importan dos cosas: la
calidad de los datos y la cantidad. Es necesario que además de
preprocesar los datos con las técnicas usuales (substracción de media
por ejemplo), se tengan en cuenta aspectos de más alto nivel. Por ejemplo,
si estuvieramos entrenando una red de reconocimiento de rostros, es
mucho mejor contar con un dataset de caras alineadas en vez de un
dataset de caras en diferentes posiciones y ángulos. De esa manera
vamos a lograr que la red aprenda mejor que \textit{features} extraer
de las imágenes.

Además no siempre se puede contar con un dataset de millones de
imágenes para entrenar nuestra red, por lo que hay que aumentar
nuestros datos con técnicas de \textit{data augmentation}: repetir la
misma imagen pero con diferentes variaciones en el color, brillo,
saturación, incluso hacer leves desplazamientos y rotaciones.

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{images/face-original.jpg}
        \caption{Original}
        \label{fig:forig}
    \end{subfigure}
\quad
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{images/face-aligned.jpg}
        \caption{Alineacion}
        \label{fig:falign}
    \end{subfigure}
\quad
   \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{images/face-augmented1.jpg}
        \caption{Color}
        \label{fig:faug1}
    \end{subfigure}
\quad
   \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{images/face-augmented2.jpg}
        \caption{Traslaciones}
        \label{fig:faug2}
    \end{subfigure}
    \caption{Diferentes formas de aumentar datos. Si trabajamos con rostros es muy común alinearlos, por ejemplo, sobre el eje que conforman los ojos \ref{fig:falign}. Traslaciones, recortes en la imagen y cambios en el brillo, contraste y color son otras técnicas muy usadas (\ref{fig:faug1} y \ref{fig:faug2}).}\label{fig:augm}
\end{figure}
#+END_LaTeX

*** Inicialización de pesos

A la hora de inicializar los pesos es escencial romper con la
simetría. Imaginemos que inicializamos todos los pesos en \(0\), algo
que podría parecer razonable. En una capa completamente conectada,
entonces todas las neuronas van a recibir el mismo valor de entrada
\(0\) (\(f(x)=\sum_i w_i x\) con \(w_i=0\)) por lo que sus salidas van
a ser todas iguales y por ende los gradientes que se calculen serán
los mismos, produciendo que los pesos se actualicen todos iguales y la
red no aprenda.

En cambio podemos inicializar los pesos con pequeños números
aleatorios cercanos a cero. Una opción común es utilizar una
distribución gaussiana con media cero y desviación estándar 0.01. Este
método, si bien es bastante \textit{ad-hoc}, es bastante usado. Hay
muchas otras maneras más sofisticadas de inicializar los pesos de una
red, pero su análisis escapa al alcance de este trabajo final.

*** Evitando el sobre-ajuste: Regularización y Dropout
#+LaTeX: \label{sec:regular}

Cuando se aprende un modelo sobre un conjunto de datos, puede surgir
el problema del \textit{sobre-ajuste}, más conocido por su nombre en
inglés \textit{overfitting}. El \textit{overfitting} significa que
nuestro modelo ajustó sus parámetros demasiado bien al conjunto de
datos de entrenamiento, provocando que aprendiera detalles
insignificantes del mismo, principalmente \textit{ruido}
aleatorio. Como consecuencia, cuando se lo aplica en un conjunto de
datos nuevos, el modelo presenta un bajo rendimiento. En contrapartida
al \textit{overfitting}, a veces puede pasar que nuestro modelo
aprendió pocas características de nuestro conjunto de entrenamiento y
termina siendo muy genérico e inflexible a la hora de ser aplicado en
un conjunto nuevo, obteniendo también baja precisión (Figura
\ref{fig:overfit}).

#+name: overfitting
#+begin_src python :session :exports none :results silent :cache yes :eval no-export
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.datasets import make_moons
from sklearn.cross_validation import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
import numpy as np
from sklearn.cross_validation import train_test_split, cross_val_score


def detect_plot_dimension(X, h=0.02, b=0.05):
    x_min, x_max = X[:, 0].min() - b, X[:, 0].max() + b
    y_min, y_max = X[:, 1].min() - b, X[:, 1].max() + b
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    dimension = xx, yy
    return dimension


def detect_decision_boundary(dimension, model):
    xx, yy = dimension # unpack the dimensions
    boundary = model.predict(np.c_[xx.ravel(), yy.ravel()])
    boundary = boundary.reshape(xx.shape) # Put the result into a color plot
    return boundary


def plot_decision_boundary(panel, dimension, boundary, colors=['paleturquoise', 'khaki']):
    xx, yy = dimension # unpack the dimensions
    panel.contourf(xx, yy, boundary, cmap=ListedColormap(colors), alpha=1)
    panel.contour(xx, yy, boundary, colors="brown", alpha=1, linewidths=0.5) # the decision boundary in green


def plot_dataset(panel, X, y, colors=["darkorange","darkcyan"], markers=["s", "o"]):
    panel.scatter(X[y == 1, 0], X[y == 1, 1], color=colors[0], marker=markers[0])
    panel.scatter(X[y == 0, 0], X[y == 0, 1], color=colors[1], marker=markers[1])


def calculate_prediction_error(model, X, y):
    yPred = model.predict(X)
    score = 1 - round(metrics.accuracy_score(y, yPred), 2)
    return score


def explore_fitting_boundaries(model, n_neighbors, datasets, width, index):
    # determine the height of the plot given the
    # aspect ration of each panel should be equal
    height = float(width)/len(n_neighbors) * len(datasets.keys())
    nrows = len(datasets.keys())
    ncols = len(n_neighbors)
    # set up the plot
    figure, axes = plt.subplots(
        1,
        1,
        figsize=(width, height),
        sharex=True,
        sharey=True
    )
    dimension = detect_plot_dimension(X, h=0.02) # the dimension each subplot based on the data
    # Plotting the dataset and decision boundaries
    i = 0
    for n in n_neighbors:
        model.n_neighbors = n
        model.fit(datasets["Training Set"][0], datasets["Training Set"][1])
        boundary = detect_decision_boundary(dimension, model)
        j = 0
        for d in datasets.keys():
            try:
                panel = axes[j, i]
            except (TypeError, IndexError):
                if (nrows * ncols) == 1:
                    panel = axes
                elif nrows == 1: # if you only have one dataset
                    panel = axes[i]
                elif ncols == 1: # if you only try one number of neighbors
                    panel = axes[j]
            plot_decision_boundary(panel, dimension, boundary) # plot the decision boundary
            plot_dataset(panel, X=datasets[d][0], y=datasets[d][1]) # plot the observations
            score = calculate_prediction_error(model, X=datasets[d][0], y=datasets[d][1])
            # make compacted layout
            panel.set_frame_on(False)
            panel.set_xticks([])
            panel.set_yticks([])
            j += 1
        i += 1
        plt.subplots_adjust(hspace=0, wspace=0) # make compacted layout
    plt.savefig('images/overfitting'+index+'.pdf')


X, y = make_moons(
n_samples=500,
random_state=1,
noise=0.3
)
# Split into training and test sets
XTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1, test_size=0.5)

# specify the model and settings
model = KNeighborsClassifier()
datasets = {"Training Set": [XTrain, yTrain]}
width = 20

# explore_fitting_boundaries(model, n_neighbors, datasets, width)
explore_fitting_boundaries(model=model, n_neighbors=[200], datasets=datasets, width=width, index='1')
explore_fitting_boundaries(model=model, n_neighbors=[23], datasets=datasets, width=width, index='2')
explore_fitting_boundaries(model=model, n_neighbors=[1], datasets=datasets, width=width, index='3')
#+end_src

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/overfitting1.pdf}
        \caption{\textit{Underfitting}}
        \label{fig:over1}
    \end{subfigure}
\quad
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/overfitting2.pdf}
        \caption{Aceptable}
        \label{fig:over2}
    \end{subfigure}
\quad
   \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/overfitting3.pdf}
        \caption{\textit{Overfitting}}
        \label{fig:over3}
    \end{subfigure}
    \caption{Ejemplo de \textit{overfitting} y \textit{underfitting}. Se puede observar un gran sesgo en el caso de \textit{underfitting} (\ref{fig:over1}) que si bien permite una mayor generalización no logra distinguir el límite entre ambas clases, lo cual se traduce en menor precisión a la hora de evaluar el modelo. En \ref{fig:over3} observamos un típico caso de \textit{overfitting} con mucha variación y sensibilidad a los datos de entrenamiento, lo cual implica poca generalización a nuevos datos. En \ref{fig:over2} se observa un buen ajuste del conjunto de datos.}\label{fig:overfit}
\end{figure}
#+END_LaTeX

Queremos elegir los mejores parámetros de \weights{} para evitar estos
problemas, y eso lo podemos hacer agregando una penalidad de
regularización \(R(W)\). Lo que buscamos con esto es poner
preferencias para algunos conjuntos de \weights{} sobre otros.

De esta manera, nuestra función de pérdida ahora cuenta con dos
componentes: \textit{pérdida de los datos} y \textit{componente de
regularización}:

\begin{equation}
     \boldsymbol{ L =\frac{1}{N} \sum_{i} L_i + \lambda R(W)}
\end{equation}

Notar que la pérdida total es el promedio de las pérdidas de cada
imagen, y que la penalización de la regularización sólo se suma una
vez.

Las técnicas de regularizacion más usadas son:

**** L2

Para cada peso de la red se calcula \(\frac{1}{2} \lambda w^2\) donde
\(\lambda\) es la tasa de regularización y se le suma a la función
objetivo. 

Una buena propiedad de la regularizacion es que al penalizar los pesos
grandes, obliga a \weights{} a generalizar y contemplar todas las
clases a la hora de clasificar. De esa manera, nuestro clasificador
final va a tomar en cuenta todas las dimensiones de entrada (algunas
con más o menos probabilidad) sin dar prioridad a una sola.

**** L1

Similar a la aterior, sólo que se le adiciona \(\lambda |w|\) a la
función objetivo. Los pesos tienden a converger a cero bajo la
regularización L1 y las redes tienden a usar un subconjunto de los
datos de entrada, convirtiendose en invariantes al ruido. En general
se prefiere la regularización L2 por obtenerse mejores resultados.

**** \textit{Dropout}

La técnica de \textit{dropout} \cite{dropout} consiste en mantener
activa una neurona con una probabilidad \(\boldsymbol{p}\), o
establecer su salida a cero en caso contrario.

Si consideramos una red neuronal con \(L\) capas, sea \(l \in
\{1,\dots,L\}\) el índice de cada capa oculta de la red. Sea
\(\boldsymbol{z}^{(l)}\) el vector de entrada a la capa \(l\),
\(\boldsymbol{y}^{(l)}\) el vector de salidas de la capa \(l\)
(\(\boldsymbol{y}^{(0)} = \boldsymbol{x}\) son los datos de entrada a
la red). \(W^{(l)}\) y \(\boldsymbol{b}^{(l)}\) son los parametros de
la capa \(l\). Dada una neurona \(i\) de la capa \(l\), la operacion
de \textit{feed-forward} de la red puede ser descripta como:

\begin{equation}
z^{(l+1)}_{i} = \boldsymbol{w}^{(l+1)}_{i} \boldsymbol{y}^{l} + b^{(l+1)}_{i},
\end{equation}

\begin{equation}
y^{(l+1)}_{i} = A(z^{(l+1)}_{i})
\end{equation}

Donde \(A\) es una función de activación. Si ahora agregamos \textit{dropout}:

\begin{equation}
r^{(l)}_{j} \sim Bernoulli(p),
\end{equation}

\begin{equation}
\tilde{\boldsymbol{y}}^{(l)} = \boldsymbol{r}^{(l)} \odot \boldsymbol{y}^{(l)},
\end{equation}

\begin{equation}
z^{(l+1)}_{i} = \boldsymbol{w}^{(l+1)}_{i} \tilde{\boldsymbol{y}}^{l} + b^{(l+1)}_{i},
\end{equation}

\begin{equation}
y^{(l+1)}_{i} = A(z^{(l+1)}_{i})
\end{equation}

Aquí \(\odot\) denota el producto elemento a elemento y
\(\boldsymbol{r}^{(l)}\) es un vector de variables aleatorias de
Bernoulli con probabilidad \(p\) de ser \(1\). Para cada capa se
calcula este vector \(\boldsymbol{r}^{(l)}\) y luego se lo multiplica
elemento a elemento por \(\boldsymbol{y}^{(l)}\) para reducir la
cantidad de neuronas activas, obteniendo como resultado
\(\tilde{\boldsymbol{y}}^{(l)}\) que a su vez va a ser la entrada de
la capa siguiente.

En cada iteración del entrenamiento, \textit{dropout} elimina neuronas
aleatoriamente y utiliza una \textit{subred} con menos neuronas que la
original, lo cual impide a las neuronas co-adaptarse entre si. La
co-adaptación ocurre cuando dos o más neuronas consecutivas dependen
mucho entre ellas para detectar \textit{features}, en vez de que cada
neurona busque un tipo particular de \textit{feature}.

Otra manera de pensarlo es la siguiente: una red neuronal con \(n\)
neuronas puede ser vista como una colección de \(2^n\)
\textit{subredes} que comparten todas los mismos pesos (\weights{}),
por lo cual la cantidad total de parámetros sigue siendo a lo sumo
\(O(n^2)\). Para cada elemento en el conjunto de entrenamiento se
elige una de estas \(2^n\) redes y apenas se la entrena. Esto es
comparable a entrenar distintos modelos y luego promediar sus
predicciones, algo que en general es muy útil en aprendizaje
automático pero rara vez se hace en aprendizaje profundo debido a que
cada modelo tarda mucho en entrenarse y es muy tedioso elegir buenos
hiperparámetros.

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:dropout
#+caption: A la izquierda, una red neuronal normal; a la derecha, una red neuronal luego de aplicar \textit{dropout}. Los nodos punteados son las neuronas que se desactivan temporalmente durante esa iteración.
[[file:images/dropout.pdf]]

*** Organización de las redes neuronales

Las redes neuronales estan organizadas como un grafo acíclico de
neuronas, donde las salidas de unas se transforma en la entrada de
otras. Las neuronas se organizan en distintas capas conectadas, de esa
manera los cálculos se hacen con operaciones entre matrices, algo que
no podríamos hacer tan fácil si tuvieramos neuronas conectadas
aleatoriamente entre ellas.

El tipo más común de capa de neuronas es la capa \textit{totalmente
conectada} (de ahora en más FC, abreviación de su nombre en inglés
\textit{Fully Connected}), en donde cada neurona de la capa anterior
se conecta con todas las neuronas de la capa siguiente, pero no
comparten conexiones dentro de la misma capa.

Usualmente no se cuenta a la capa de entrada de las redes como una
capa más, y la capa de salida no tiene funciones de activación, dado
que generalmente representan las puntuaciones de cada clase (en
clasificación) o alguna métrica (en regresión). Las redes neuronales
suelen tener una o más capas intermedias entre la entrada y la salida,
denominadas \textit{capas ocultas} (Figura \ref{fig:nnet}).

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{images/neural-net.pdf}
    \caption{Diagrama de una red neuronal con entrada de dimensión \(D=3\), una capa oculta completamente conectada y dos neuronas de salida.}\label{fig:nnet}
\end{figure}
#+END_LaTeX

Las redes neuronales se entrenan partiendo del principio del descenso
del gradiente que se explicó en la Sección \ref{sec:sgd}. Notemos que
\loss{} es una función que depende de las imágenes de entrada \img{},
\weights{} y \bias{}. Sin embargo, como ya dijimos, el conjunto de
datos de entrenamiento es algo fijo en nuestro modelo, por lo que sólo
nos interesa calcular el gradiente sobre \weights{} y \bias{} para
poder actualizar sus parámetros. Ahora bien, derivar una función con
millones de parámetros (cantidad que suelen tener las redes neuronales
artificiales) es computacionalmente costoso, por lo que para
actualizar \weights{} con los nuevos pesos se utiliza un algoritmo
llamado \textit{backpropagation}.

*** Entrenamiento: \textit{Backpropagation}
El entrenamiento de las redes neuronales se basa en el algoritmo de
\textit{descenso de gradiente} que ya vimos. Sea \(g\) una funcion de
una dimension, entonces su derivada se expresa como:

\begin{equation}
     \boldsymbol{\frac{dg(x)}{dx} = \lim_{h\to 0} \frac{g(x + h) - g(x)}{h} }
\end{equation}

Cuando la función toma un vector de números en vez de uno solo, a las
derivadas las llamamos derivadas parciales y el \textit{gradiente} es
simplemente un vector de esas derivadas. Por ejemplo, sea \(g\) una
función que toma dos parámetros \(x\) e \(y\), entonces el gradiente
de \(g\) es \(\nabla g = [\frac{\partial g}{\partial x},
\frac{\partial g}{\partial y}]\)

Usualmente podemos diferenciar con métodos numéricos, asignando a
\(h\) números muy pequeños por ejemplo, pero esto requiere de muchos
cálculos, es lento y tan sólo una aproximación. Veremos más
adelante que la función \loss{} de las redes neuronales suele tener
decenas de millones de parámetros, y realizar tantas
operaciones para una sola actualización de \weights{} no es
conveniente. En la práctica usaremos el cálculo analítico del
gradiente, en el cual derivamos una fórmula directa que es muy rápida
de computar valiéndonos de la \textit{regla de la cadena}.

La \textit{regla de la cadena} nos ayuda a descomponer el cálculo del
gradiente de expresiones complejas en pequeños pasos. Por ejemplo,
tomemos nuevamente una función \(g\):

\begin{equation}
    g(x,y,z) = \frac{x}{y^2} + z
\end{equation}

Si quisieramos obtener su gradiente en \(x\) de la manera tradicional,
calculando el cociente de \(g(x+h) - g(x)\) con \(h\) cuando \({h \to
0}\) deberíamos realizar muchos cálculos computacionalmente
costosos. En cambio, podemos ver a \(g\) como una composición de
funciones:

\begin{equation}
    g(x,y,z) = \frac{x}{y^2} + z = q + z
\end{equation}

Y calcular su gradiente valiéndonos de la \textit{regla de la cadena}:

\begin{equation}
\frac{\partial g}{\partial z} = q
\end{equation}

\begin{equation}
\frac{\partial g}{\partial q} = z
\end{equation}

\begin{equation}
\frac{\partial g}{\partial x} = \frac{\partial g}{\partial q} \frac{\partial q}{\partial x} = \frac{z}{y^2}
\end{equation}

\begin{equation}
\frac{\partial g}{\partial y} = \frac{\partial g}{\partial q} \frac{\partial q}{\partial y} = \frac{-2zx}{y^3}
\end{equation}

Ahora podemos comenzar a estructurar nuestro algoritmo de optimización
en dos pasos: primero, evaluamos nuestra función \loss{} en los
parámetros actuales (\textit{forward pass}). Luego, partiendo de ese
resultado calculamos el gradiente en cada variable utilizando la
\textit{regla de la cadena}. De esta manera ``propagamos'' el error de la
predicción hacia atrás (\textit{backpropagation}) y corregimos
ligeramente los pesos para mejorar las futuras predicciones.

Una vez que contamos con el gradiente, actualizamos los parámetros de
\loss{} restándole un porcentaje del gradiente negativo calculado
(negativo porque queremos ir en dirección opuesta a donde crece la
función, o sea, ir a su mínimo). Ese porcentaje es llamado
\textit{tasa de aprendizaje} (\textit{learning rate}) y suele ser uno
de los parámetros más difíciles de elegir, ya que la calidad y rapidez
de aprendizaje dependen de él.

Idealmente computaríamos el gradiente sobre todo el conjunto de datos,
actualizaríamos los parámetros, y repetiríamos el ciclo hasta
conseguir un buen resultado. Sin embargo los conjuntos de datos para
entrenar las redes neuronales suelen tener cientos de miles o incluso
millones de imágenes, por lo cual se utiliza una técnica llamada
\textit{Descenso de Gradiente Estocástico} o \textit{SGD} por sus
siglas en inglés, en el cual se calcula el gradiente para una cantidad
predeterminada de imágenes (\textit{batches}), se actualizan los
parámetros y se vuelve a repetir el ciclo con otro subconjunto
distinto. Esto parte de la suposición de que todas las imágenes del
conjunto de datos estan correlacionadas entre sí. En teoría
\textit{SGD} utiliza una sola imagen por batch, pero dada la alta
paralelización que provee el hardware actual, conviene hacer lotes de
imágenes de 62, 128, 512 imágenes. El tamaño de los \textit{batches}
no es estrictamente un hiperparámetro que uno pueda
\textit{cross-}validar, sino que más bien depende del hardware sobre
el que se esté entrenando la red (en general se eligen potencias de
dos por cuestiones de eficiencia).

**** Transferencia de aprendizaje

Entrenar un modelo con un tipo específico de problema y luego utilizar
su \textit{conocimiento} para resolver otro problema nuevo, tal vez
incluso en un área distinta a la que fue pensado originalmente, es lo
que se llama transferencia de aprendizaje. Esta técnica ha cobrado
importancia en \textit{deep learning} dado que a menudo las redes son
muy profundas y tardan semanas en entrenarse, por lo que contar con
modelos preentrenados sobre los cuales se puedan ajustar ligeramente
los parámetros para resolver un nuevo problema es una ventaja. La
mayoría de los \textit{frameworks} modernos para implementar redes
neuronales soportan realizar transferencia de aprendizaje utilizando
modelos pre-entrenados.

** Redes Neuronales Convolucionales

Antes de introducirnos en el mundo de las redes convolucionales es
necesario definir qué es una convolución.

Una matriz de convolución, o \textit{kernel}, es una matriz
generalmente cuadrada y pequeña utilizada extensamente para detectar o
resaltar bordes y enfocar o desenfocar imágenes dependiendo de sus
valores y tamaño. Son muy utilizadas en el mundo de la visión por
computadoras, sobre todo en los pasos de extracción de
\textit{features} de una imagen a la hora de aplicar algoritmos de
aprendizaje automático.

Una convolución entre un \textit{kernel} y una imagen se realiza
sumando las multiplicaciones elemento a elemento entre \textit{kernel}
y una región de la imagen. Por ejemplo, sea \(C\) un \textit{kernel}
de \(3\times3\) y \(A\) un pedazo de una imagen también de dimension
\(3\times3\), entonces la convolución entre \(C\) y \(A\) (denotada con el
símbolo \(*\)) es:

\begin{equation}
C
=
\begin{bmatrix}
    0  & 1 & 0 \\
    1  & -4 & 1 \\
    0  & 1 & 0 \\
\end{bmatrix}
\end{equation}

\begin{equation}
A
=
\begin{bmatrix}
    a  & b & c \\
    d  & e & f \\
    g  & h & i \\
\end{bmatrix}
\end{equation}

\begin{equation}
A * C = (a\times0) + (b\times1) + (c\times0) + (d\times1) + (e\times-4) + (f\times1) + (g\times0) + (h\times1) + (i\times0)
\end{equation}

Observemos dos cosas: 
\begin{enumerate}

\item El resultado de la convolución entre un \textit{kernel} y su
correspondiente pedazo en la imagen (que hasta ahora es de un canal)
es un valor escalar, o sea un píxel correspondiente al canal de la
imagen sobre el cual hayamos convolucionado. Es decir, si
convolucionamos un \textit{kernel} a través de los 3 canales de una
imagen (rojo, verde y azul) vamos a obtener como resultado 3 valores
que se van a combinar para crear un píxel RGB (\textit{Red-Green-Blue}
por sus siglas en inglés).

\item Uno no realiza una sola convolución con un solo pedazo de la imagen,
sino que lo hace sobre \textit{toda} la imagen. Esto significa que el
\textit{kernel} se va ``desplazando'' a lo largo y ancho de la imagen
para generar, en cada ocasión, un escalar que va a formar parte de la
nueva imagen convolucionada. En la Figura \ref{fig:conv} se puede observar
el resultado de una convolución.

\end{enumerate}

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/conv-orig.png}
        \caption{\(A\)}
        \label{fig:conv-orig}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \(\begin{bmatrix}
         0  &  1 & 0 \\
         1  & -4 & 1 \\
         0  &  1 & 0 \\
        \end{bmatrix}\)
        \caption{\textit{Kernel} \(K\)}
        \label{fig:K}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/conv-edge.png}
        \caption{\(A * K\)}
        \label{fig:conv-edge}
    \end{subfigure}
    \caption{Es muy común convolucionar imágenes con ciertos \textit{kernels} para detectar bordes en las mismas.}\label{fig:conv}
\end{figure}
#+END_LaTeX

Las redes convolucionales hacen uso extensivo de estos
\textit{kernels}. De hecho, \textit{aprenden a generar}
\textit{kernels} para encontrar diversas características en las
imágenes, tanto de bajo nivel (bordes o colores) como de alto nivel
(neuronas que se activan cuando una persona sonríe, por ejemplo). Si
sumamos esto con la no linealidad propia de las redes neuronales
artificiales, obtenemos una poderosa herramienta de clasificación de
imágenes.

*** Diferencias con redes neuronales artificales convencionales
Las redes convolucionales cuentan con los mismos artefactos que las
redes convencionales que ya discutimos (neuronas con pesos, funciones
de pérdida, capas completamente conectadas). Incluso los mismos
métodos de entrenamiento pueden ser aplicados. La diferencia radica en
que las redes convolucionales asumen que están trabajando con
imágenes, lo que permite optimizar la arquitectura de las mismas,
reduciendo parámetros y mejorando el proceso de aprendizaje.

Imaginemos por un momento que quisieramos aprender a clasificar un
conjunto de imágenes de 200x200 píxeles con 3 canales de colores. Eso
nos da una dimensión de entrada de 200x200x3, por lo que una neurona
completamente conectada en la primer capa oculta tendría 120000
conexiones y por ende esa misma cantidad de pesos a entrenar. Si
tenemos en cuenta que seguramente vamos a requerir más de una neurona
(comúnmente cientos de ellas en una misma capa) podemos concluir que
este enfoque no escala bien para el procesamiento de imágenes.

Una red neuronal convolucional se aprovecha de la ventaja de que los
datos de entrada son imágenes y organiza las neuronas en 3
dimensiones: ancho, alto y profundidad. Tal como hicimos con las redes
neuronales convencionales, en la siguiente sección analizaremos los
tipos de capas de una red neuronal convolucional. Al final de la
Sección se dará una descripción general de la arquitectura de una red
convolucional y varios ejemplos de redes convolucionales conocidas.

*** Capas de una red convolucional
**** Capas de Entrada y Salida

Las redes convolucionales se manejan con volumenes (datos en mas de
dos dimensiones). Eso significa que para cada capa hay un volumen de
entrada y un volumen de salida.

La capa de entrada de una red es la que provee la imagen original como
un volumen de pixeles. Notar que si una imagen tiene un alto y ancho
de 256 pixeles y tres canales de colores (RGB) entonces el volumen de
salida de esta capa va a ser \(256 \times 256 \times 3\).

En general la capa de salida suele ser una capa completamente
conectada con los puntajes de cada clase en el caso de tareas de
clasificacion o con un vector de numeros reales en el caso de tareas
de regresion. Para clasificacion tambien es muy comun agregar una
ultima capa con un clasificador (por ejemplo, \textit{Softmax}) que
transforme los puntajes obtenidos en probabilidades normalizadas.

**** Capas Convolucionales

Una capa convolucional consta de un conjunto de filtros (o
\textit{kernels}) cuyos parámetros se pueden aprender. En general cada
filtro es pequeño a lo ancho y alto, pero se aplica a toda la
profundidad del volumen de entrada (ej.: los tres canales RGB). Notar
que el volumen de entrada puede bien ser una imagen o bien el volumen
de salida con las activaciones de otra capa.

Durante el entrenamiento o la clasificación de imagenes, estos filtros
se convolucionan a traves del ancho y alto del volumen de entrada,
produciendo un mapa de activaciones en 2-D para cada filtro. Si
``apilamos'' los mapas de activaciones de todos los filtros de una
capa convolucional, obtenemos un \textit{volumen} de salida. De esta
manera cada elemento en el volumen de salida puede ser interpretado
como la salida de una neurona conectada a una pequeña region de los
datos de entrada, la cual comparte parámetros (pesos) con las otras
neuronas que corresponden al mismo filtro.

Esta conexión a una pequeña región en los datos de entrada es un
hiperparámetro de la red llamado \textit{campo receptivo}. Es
importante notar que los \textit{campos receptivos} son locales en una
pequeña área en cuanto al ancho y alto de la entrada, pero abarcan la
totalidad de la profundidad del volumen de entrada.

Otros hiperparámetros relacionados con las capas convolucionales son
la cantidad de filtros (\textit{K}), el espacio en píxeles entre cada
aplicacion de los filtros (\textit{stride}) y por ultimo el relleno
con ceros o \textit{zero-padding}, donde se le agrega un ``marco'' de
1 o más ceros a la entrada de la capa. En la Figura \ref{fig:conv-lay}
se observa un esquema del funcionamiento de una capa convolucional.

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/conv-layer.pdf}
    \caption{Ejemplo de una capa convolucional cuyo volumen de entrada es una imagen (\(x\)) con 3 canales RGB. La capa cuenta con 6 filtros y cada uno convoluciona con la imagen a lo largo y ancho. El volumen de salida de esta capa tiene una profundidad de 6 y un ancho y largo que dependen del tamaño de los filtros y el \textit{stride}. Los recuadros más oscuros sobre \(x\) corresponden con el \textit{campo receptivo} de una neurona particular, cuyo tamaño está dado por el tamaño de los filtros.}\label{fig:conv-lay}
\end{figure}
#+END_LaTeX

**** \textit{Pooling}

Las capas de \textit{pooling} reducen la dimensión espacial de sus
entradas y por ende, la cantidad de parametros en la red,
ayudando a controlar el \textit{overfitting}. Lo más común es
insertar capas de \textit{pooling} luego de capas convolucionales.

Un método de \textit{pooling} muy usado es \textit{MAX Pooling}, en el
cual se calcula el máximo de un área local (generalmente \(2 \times 2\) o \(3 \times 3\)) en
el ancho y largo del volumen de entrada y a través de cada una de las
``rodajas'' que conforman la profundidad del volumen. El área local
está definida por el tamaño del \textit{stride}. De esta manera se
reduce espacialmente la entrada, pero no su profundidad.

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{images/pool-layer.pdf}
        \caption{}
        \label{fig:pool1}
    \end{subfigure}

    \begin{subfigure}[b]{0.5\textwidth}
        \centering

        \colorlet{yel}{yellow!40}
        \colorlet{re}{red!40}
        \colorlet{gre}{green!40}
        \colorlet{blu}{blue!40}

        \begin{tikzpicture}[every node/.style={minimum size=.5cm-\pgflinewidth, outer sep=0pt}]
        \draw[step=0.5cm,color=gray] (-1,-1) grid (1,1);
        \node[fill=yel] at (-0.75,+0.75) {1};
        \node[fill=yel] at (-0.75,+0.25) {2};
        \node[fill=yel] at (-0.25,+0.75) {3};
        \node[fill=yel] at (-0.25,+0.25) {4};
        \node[fill=re] at (-0.75,-0.25) {8};
        \node[fill=re] at (-0.75,-0.75) {7};
        \node[fill=re] at (-0.25,-0.25) {5};
        \node[fill=re] at (-0.25,-0.75) {4};
        \node[fill=gre] at (+0.75,-0.25) {2};
        \node[fill=gre] at (+0.75,-0.75) {3};
        \node[fill=gre] at (+0.25,-0.25) {2};
        \node[fill=gre] at (+0.25,-0.75) {1};
        \node[fill=blu] at (+0.75,+0.25) {0};
        \node[fill=blu] at (+0.75,+0.75) {7};
        \node[fill=blu] at (+0.25,+0.25) {4};
        \node[fill=blu] at (+0.25,+0.75) {3};
        \draw[black, -latex ] (1.1,0) -- (3.9,0) node [pos=0.46,above,font=\footnotesize] {\textit{MAX Pooling}};
        \draw[step=0.5cm,color=gray] (4,4) grid (4,4);
        \node[fill=yel] at (+4.25,+0.25) {4};
        \node[fill=blu] at (+4.75,+0.25) {7};
        \node[fill=gre] at (+4.75,-0.25) {3};
        \node[fill=re] at (+4.25,-0.25) {8};
        \end{tikzpicture}

        \caption{}
        \label{fig:pool2}
    \end{subfigure}
    \caption{Las capas de \textit{Pooling} reducen las dimensiones espaciales. Notar en \ref{fig:pool1} que la profundidad del volumen se mantiene intacta. En \ref{fig:pool2} observamos cómo funciona una capa \textit{MAX Pooling} con un un \textit{stride} de 2.}\label{fig:pool}
\end{figure}
#+END_LaTeX

**** Capas Completamente Conectadas (Fully-Connected)

Como su nombre lo indica, cada neurona de esta capa tiene conexiones a
todas las salidas (o activaciones) de la capa anterior. Por lo tanto
sus activaciones se pueden calcular con una multiplicacion de matrices
junto con el calculo del \textit{bias}, como ya se vio para las redes
neuronales convencionales.

*** Arquitecturas conocidas de redes convolucionales 

Normalmente una red convolucional esta compuesta de varias capas
convolucionales (CONV), capas de \textit{pooling} (POOL), capas
completamente conectadas (FC por sus siglas en ingles) y funciones de
activacion, generalmente rectificadores lineales (RELU).

El patron usual en redes convolucionales es una capa CONV seguida de
una capa RELU seguida de una capa de \textit{pooling}. Esto se repite
una o varias veces hasta reducir espacialmente las dimensiones de la
entrada de la red. Luego es comun utilizar capas FC hasta reducir las
dimensiones a las dimensiones de salida, que en el caso de
clasificacion son las probabilidades de cada clase.

A lo largo de los años ha habido varias arquitecturas de redes
convolucionales que cuentan con nombre propio, como por ejemplo LeNet
\cite{Lecun98gradient-basedlearning}, creada en los 90 por Yann LeCun
y utilizada para el reconocimiento de digitos manuscriptos. Esta red
fue utilizada con exito para leer codigos postales y cheques
bancarios.

En 2012, el ganador del desafio ImageNet ILSVRC, Alex Krizhevsky,
obtuvo un 16% de error utilizando una arquitectura llamada AlexNet
\cite{NIPS2012_4824}. Su arquitectura es muy similar a la de LeNet,
aunque mas profunda y fue una de las primeras en concatenar varias
capas CONV antes de una capa de \textit{pooling}.

Los ganadores del ISLVRC 2013 utilizaron una red llamada ZFNet
\cite{DBLP:journals/corr/ZeilerF13}, que era basicamente una
modificacion de AlexNet, con cambios en los hiperparametros y las
capas convolucionales.

En la misma competencia ILSVRC del 2014 se dieron a conocer dos redes,
GoogLeNet \cite{43022} y VGGNet \cite{Simonyan14c}. Ambas demostraron
que la profundidad de la red es una caracteristica critica a la hora
de obtener buenos resultados.

Si bien GoogLeNet fue la ganadora ese año, luego se demostro que
VGGNet es superior en muchas tareas de transferencia de aprendizaje,
por lo que es mas popular que GoogLeNet y se pueden encontrar muchos
modelos ya preentrenados.

Finalmente, ResNet (Residual Network) \cite{he15deepresidual}, la red
ganadora del ILSRVC 2015, cuenta con nada menos que 152 capas (VGGNet
cuenta con 19) y obteniendo un error de 3.57% en el top-5.

* Redes neuronales convolucionales siamesas
#+LaTeX: \label{sec:siamesa}

Un tipo particular de arquitectura de redes convolucionales es el de
\textit{redes siamesas}. Una red siamesa está compuesta de dos copias
exactas de la misma red cuyos parámetros son compartidos. La
arquitectura toma pares de imágenes (\(x_1\) y \(x_2\)) y las salidas
de ambas redes son redireccionadas a una función de pérdida que
calcula alguna métrica respecto a las dos entradas (Figura
\ref{fig:siamese-diagram}). Las redes siamesas no difieren en más que
eso de las redes convolucionales que ya vimos. Se pueden entrenar
mediante \textit{backpropagation} y, dependiendo del problema,
utilizando hasta las mismas funciones de pérdida.

Las redes siamesas fueron utilizadas por primera vez con éxito para la
verificación de huellas digitales \cite{fingerprint} y de firmas
\cite{Bromley94signatureverification}. Se han logrado también buenos
resultados en verificación de rostros
\cite{Chopra:2005:LSM:1068507.1068961} y más recientemente en tareas
de clasificación \cite{siamese-oneshot}.

Estos trabajos tienen tres factores en común:

\begin{enumerate}
\item La función de pérdida se calcula mediante alguna medida de
similaridad entre las dos imágenes de entrada. Ya sea una probabilidad
correspondiente a la igualdad de las imágenes (como en \cite{fingerprint}),
una distancia cosenoidal calculada entre las \textit{features} obtenidas
de cada red (como en \cite{Bromley94signatureverification}) o una función
de energía contrastiva \cite{Chopra:2005:LSM:1068507.1068961}
\cite{siamese-oneshot}.

\item Se tiene acceso a pocos datos de entrenamiento. Para la verificación
de huellas dactilares, firmas y rostros se utilizaron 200, 1400 y 1100
muestras de entrenamiento respectivamente.

\item Se generan artificialmente pares de imágenes que luego se usarán durante
el entrenamiento. Esto es muy similar a la aumentación de datos, ya que
se valen de rotaciones, traslaciones, etc. A estos pares se les
asigna una o más etiquetas indicando el nivel de similitud entre las imágenes.
\end{enumerate}

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{images/siamese-diagram.pdf}
    \caption{Esquema de la arquitectura de una red siamesa. \(G_W\) es un conjunto de funciones, en este caso una red convolucional, con parámetros \(W\). Ambas redes comparten los parámetros. Luego una función de pérdida \(L\) analiza la similitud entre \(X_1\) y \(X_2\)}\label{fig:siamese-diagram}
\end{figure}
#+END_LaTeX

El propósito de entrenar redes siamesas es aprender buenas features
relacionadas al problema en cuestion.  Lo mas común es tomar los pesos
aprendidos por la red siamesa y realizar una transferencia de
aprendizaje a un nuevo problema.

Es necesario definir qué función de similitud vamos a usar y cómo
vamos a generar los pares de entrenamiento antes de entrenar redes
siamesas. En la siguiente Sección nos vamos a adentrar en trabajo con
redes siamesas de Agrawal et al. \cite{LSM2015}, reproduciendo y
analizando sus resultados en lo que conforma el eje principal de este
trabajo final.

** Entrenamiento de modelos de aprendizaje profundo utilizando información odométrica
#+LaTeX: \label{sec:odometry}

Muchos agentes móviles son conscientes de sus movimientos
(\textit{automovimiento}) gracias a sus sistemas motores. En otras
palabras, esa información está disponible y es accesible
fácilmente. Ya sea un mamífero con un sistema de equilibrio como los
humanos o un robot con giróscopos y otros sensores de movimiento, es
posible acceder a la información visual y espacial para determinar en
dónde se encuentra uno en el espacio.

Agrawal et al. proponen en \cite{LSM2015} que se pueden aprender
representaciones visuales útiles analizando la correlación entre los
estímulos visuales y el automovimiento. De esta manera, un agente
móvil puede ser tratado como una cámara moviéndose, y su información
de automovimiento se corresponde con los movimientos de la
cámara. Establecen entonces el problema de relacionar los estímulos
visuales con el movimiento como la tarea de predecir qué
transformaciones sufren las imágenes a medida que el agente se mueve.


Ese agente móvil es modelado con Redes Neurales Convolucionales que
optimizan sus representaciones visuales minimizando el error entre la
informacion de automovimiento provista por el sistema y la informacion
de eutomovimiento predecida utilizando las imágenes de entrada
solamente. Este procedimiento es, según Agrawal, equivalente a
entrenar una Red Convolucional con dos flujos de entrada (o sea, una
Red Neural Convolucional Siamesa) en el que cada entrada toma una
imagen y la red predice la transformacion que ocurrio entre ambas
imágenes.

Bajo la hipótesis de que esto forzaría al agente a aprender
\textit{features} útiles en la identificación de elementos visuales
que esten presentes en ambas imágenes, entrenaron diferentes versiones
de redes siamesas con distintos conjuntos de datos y obtuvieron
resultados comparables al estado del arte. La diferencia es que ellos
no debieron utilizar tantas imágenes.

En este trabajo final se tratará de validar la hipótesis propuesta por
Agrawal et al. reproduciendo varios de los experimentos propuestos en
su investigación. Para ello nos valdremos de técnicas similares a las
mencionadas en la Sección XX , de pre-entrenamiento de redes y
finalmente de transferencia de aprendizaje para validar las redes
entrenadas.

En la Sección \ref{sec:agrawal} reproduciremos los resultados del
paper previamente citado. Primero haremos una prueba de concepto
utilizando el conjunto de datos MNIST \cite{mnist} para luego realizar
experimentos con el conjunto de datos KITTI \cite{KITTI}.

*** Nomenclatura

Vamos a seguir la misma nomenclatura que en el paper original. Cada componente de la red siamesa se denotará con BCNN por sus siglas
en inglés \textit{Base CNN}. Las \textit{features} extraídas de estas
redes serán concatenadas y pasadas a otra red llamada TCNN
(\textit{Top CNN}). En la TCNN es donde colocaremos nuestras funciones
de pérdida. Las BCNN son las redes sobre las que luego haremos
transferencia de aprendizaje.

Para dar una idea de las arquitecturas de cada red de manera sencilla vamos a
usar las siguientes abreviaciones:

\begin{itemize}
\item C\(k\) para una capa convolucional con \(k\) filtros cuadrados.
\item F\(k\) para una capa completamente conectada (FC) con salida de dimensión \(k\).
\item P para una capa de \textit{Pooling}. A menos que se diga lo contrario, siempre usaremos \textit{MAX Pooling}.
\item D para una capa \textit{Dropout}.
\item Op para la capa de salida. En general nuestras capas de salida van a estar conformadar por una F\(k\) (\(k\) es el número de clases) seguidas por una capa Softmax.
\end{itemize}

Al igual que en \cite{LSM2015}, y como es usual cuando se entrenan
CNN's, colocaremos rectificadores lineales ReLU's luego de cada capa
convolucional y cada cada FC.

*** Medidas de similaridad: \textit{Slow Feature Analysis} y Automovimiento

Agrawal et al. proponen analizar el automovimiento del agente para
aprender \textit{features} útiles y lo comparan con otro método muy
utilizado en redes siamesas, \textit{Slow Feature Analysis} (SFA), que
formulan de la siguiente manera:

\begin{equation}
L(x_{t_1}, x_{t_2}, W) = \begin{cases}
                           D(x_{t_1}, x_{t_2}),& \text{si} |t_1 - t_2| \leq T \\ 
                           1 - \max{(0, m - D(x_{t_1}, x_{t_2}))},& \text{si} |t_1 - t_2| > T
                         \end{cases}
\end{equation}

Donde \(x_{t_1}\) y \(x_{t_2}\) son las \textit{features} obtenidas
por las redes siamesas en los tiempos \(t_1\) y \(t_2\), con
parámetros compartidos \(W\), \(m\) es el margen y \(T\) es el umbral
que determina si dos \textit{features} se consideran cercanas. La
distancia \(D\) se eligió como la norma L2. SFA se basa en que las
características relevantes cambian poco en una ventana de tiempo
pequeña.

Por otro lado, la técnica de analizar el automovimiento se plantea
como una tarea de clasificación en donde las redes tienen que predecir
cuál transformación fue efectuada en cada uno de los ejes. Para ello
agrupan todas las posibles transformaciones en clases de rangos
iguales, y la tarea de la red es predecir a que clase pertenece la
imagen.

*** Reproducción de resultados
#+LaTeX: \label{sec:agrawal}

Todos los experimentos realizados se encuentran en Github
[fn::https://github.com/ezetl/deep-learning-techniques-thesis].
Dentro del repositorio se encuentra toda la documentación pertinente a
la descarga de los conjuntos de datos y la reproducción de los
experimentos. Parte del tiempo fue dedicado a tratar de que los
experimentos sean reproducibles por otras personas, por lo que se
trató de seguir buenas prácticas a la hora de modularizar y organizar
los distintos módulos.

Los experimentos fueron desarrollados en su mayoría en el lenguaje
Python, excepto por algunos scripts de preprocesamiento que
originalmente fueron creados en C++.

**** Prueba de concepto con MNIST
***** Conjunto de datos

El conjunto de datos MNIST \cite{mnist} cuenta con 60000 imágenes de caracteres
numéricos manuscritos para entrenamiento, más 10000 imágenes para
evaluación. La dimensión de cada imagen es \(1 \times 28 \times 28\).
Para el entrenamiento mediante
\textit{automovimiento} se crearon pares de imágenes siguiendo los
lineamientos del paper. Esto significa que cada par está compuesto de la imagen
original y la imagen con transformaciones en los ejes X, Y, Z. Las
transformaciones en X e Y son traslaciones de 3 píxeles, mientras
que la rotación en Z varía entre los -30° y los 30°. Tanto las
rotaciones como las traslaciones son números enteros. Para cada par
creado las transformaciones se eligen de manera aleatoria uniforme.

En la Figura XX se pueden observar varios pares de imágenes generadas durante la creación de 
la base de datos que luego fue usada para entrenar la red siamesa.

***** Descripción de la red

La arquitectura utilizada para las BCNN fue C96-P-C256-P, y para la
TCNN se eligió F1000-D-Op. Tener en cuenta que para el caso del
\textit{automovimiento} es necesario utilizar una combinación
FC-Softmax para calcular la pérdida en cada una de las
transformaciones.

Para transferencia de aprendizaje se añadió F500-D-F10-Softmax a una
BCNN.

***** Entrenamiento y evaluación

Las redes siamesas se pre-entrenaron durante 40000 iteraciones con una
tasa de aprendizaje de 0.01. Siguiendo los lineamientos del paper se
utilizaron márgenes \(m\) de 10 y 100 para SFA por ser los que mejores
resultados lograron. En ambas redes la tasa de aprendizaje se reduce a
la mitad cada 10000 iteraciones. El tamaño del \textit{mini batch} fue
de 125, lo cual equivale a procesar 5 millones de pares de imágenes
durante las 40000 iteraciones del entrenamiento.

La etapa de transferencia de aprendizaje se hizo con 4000 iteraciones
a una tasa de aprendizaje constante de 0.01.

Para evaluar la calidad de las \textit{features} aprendidas por las
redes siamesas se estableció la tasa de aprendizaje de las capas
convolucionales a cero.

En la Tabla \ref{tab:one} se puede observar la exactitud obtenida
mediante la transferencia de aprendizaje con 100, 300, 1000 y 10000
imágenes de los dos métodos utilizados (automovimiento y SFA) y una
comparación con un entrenamiento desde cero utilizando esa misma
cantidad de imágenes.

#+caption: Exactitud de los dos métodos de pre-entrenamiento utilizados (SFA y automovimiento).
#+name: tab:one

|---+-----------------------------+------+------+------+------|
|   | método\#datos entrenamiento |  100 |  300 | 1000 | 1000 |
| / | <>                          |    < |      |      |    > |
| # | Desde cero                  | 0.42 | 0.70 | 0.82 | 0.97 |
| # | SFA(m=10)                   | 0.52 | 0.71 | 0.77 | 0.82 |
| # | SFA(m=100)                  | 0.58 | 0.73 | 0.80 | 0.88 |
| # | Automovimiento              | 0.75 | 0.90 | 0.92 | 0.99 |
|---+-----------------------------+------+------+------+------|

Se puede observar que entrenar mediante automovimiento presenta una
performance claramente superior a entrenar una red desde cero con la
misma cantidad de imágenes en los casos en los que el conjunto de
datos es relativamente pequeño. Es también superior al entrenamiento
utilizando \textit{Slow Feature Analysis}, y dado que no se
modificaron las capas convolucionales aprendidas durante el
pre-entrenamiento, podemos concluir que las \textit{features}
aprendidas son buenas y logran captar las representaciones necesarias
para el domino del problema en cuestión. El siguiente paso es
verificar que efectivamente las \textit{features} aprendidas se puedan
aplicar a diferentes dominios de problemas y sean lo suficientemente
generalizables.

**** Pruebas con KITTI
***** Conjunto de datos

El conjunto de datos KITTI \cite{Geiger2012CVPR} consiste en 11
secuencias que registran el movimiento de un automóvil en una
ciudad. Además de proveer cuadros de video, se encuentra la
información odométrica recolectada por sensores montados en el
automóvil. Esa misma información es la que usa Agrawal et al. a la
hora de computar las transformaciones en la cámara entre pares de
imágenes, y es la que intentaremos reproducir en esta sección.

Se asume que la dirección a la que apunta la cámara es el eje Z y el
plano de la imagen es el plano XY (ejes horizontales y
verticales). Dado que las transformaciones más significativas de la
cámara ocurren en los ejes Z/X (a medida que el automóvil avanza por
la calle) y sobre el eje Y (cuando el automóvil gira), sólo se tomaron
en cuenta esas tres dimensiones a la hora de analizar las
transformaciones.

Nuevamente, la predicción de transformaciones se establece como una
tarea de clasificación, esta vez con 20 clases para las
transformaciones en cada eje. Siguiendo los lineamientos originales
del paper, los pares de entrenamiento se tomaron de cuadros separados
a lo sumo por 7 cuadros intermedios. Similarmente, para entrenamiento
por SFA se consideraron a los cuadros separados por \(\pm 7\) cuadros
intermedios como similares.

Finalmente, las redes siamesas fueron entrenadas a partir de parches
de \(227 \times 227\) extraídos aleatoriamente de las imágenes
originales de \(1241 \times 376\) píxeles. No se aplicaron
transformaciones extras más allá de las otorgadas por el movimiento de
la cámara.

***** Odometria 

KITTI provee anotaciones con las poses de la camara (es decir, su
trayectoria) para las secuencias de imágenes. Estas poses estan dadas
por una matriz de transformación de \(3 \times 4\), siendo la última
columna las traslaciones en X, Y, Z. De esa matriz se puede extraer el
ángulo de Euler correspondiente a las rotaciones sobre el eje Y si
asumimos que el primer bloque \(3 \times 3\) es una matriz de rotación
\(R\).

http://robotics.stackexchange.com/questions/7040/how-to-get-the-projection-matrix-from-odometry-tf-data
http://www.staff.city.ac.uk/~sbbh653/publications/euler.pdf

***** Descripción de la red

La red utilizada como base de las BCNN está inspirada en las primeras 5 capas convolucionales de AlexNet /cite{alexnet}, es 
decir, C96-P-C256-P-C384-C384-C256-P. La TCNN fue definida como C256-C128-F500-D-Op, con filtros convolucionales de \(3 \times 3\).

***** Entrenamiento y evaluación

Se pre-entrenaron las redes siamesas por 60K iteraciones con un tamaño
de \textit{mini batch} de 250 y una tasa de aprendizaje inicial de
\(0.001\), reducida en un factor de 2 cada 20K iteraciones. La
transferencia de aprendizaje se hizo durante 10K iteraciones a una
tasa de aprendizaje constante de 0.001. Para diferenciar los distintos
entrenamientos, al modelo entrenado con SFA lo vamos a llamar
KITTI-SFA y al entrenado con automovimiento, KITTI-EGO.

Para tener un baseline adecuado, se entrenó AlexNet con el conjunto de
datos ILSVRC'12 desde cero utilizando 20 y 1000 imágenes por clase.
El conjunto de datos ILSVRC es el usado en la competencia anual de
Imagenet y contiene mil clases de objetos distintas. Dichos modelos
seran llamados ALEX-20 y ALEX-1000 respectivamente.

Para hacer una comparación justa con ALEX-20 y ALEX-1000, las redes
siamesas fueron entrenadas con aproximadamente 20K pares de
imágenes.

****** Evaluación utilizando el conjunto de datos SUN-397

El conjunto de datos SUN-397 \cite{sun397} consiste de 397 categorías
de paisajes interiores y exteriores y además provee 10 particiones del
dataset para hacer \textit{cross-validation}, pero debido a lo costoso
que es entrenar redes neuronales convolucionales solo se utilizó una
partición.

La evaluación se hizo midiendo la exactitud de clasificadores
\textit{Softmax} utilizando las \textit{features} obtenidas de las
salidas de las primeras 5 capas convolucionales.

|-----------+---+---+-------+-------+-------+-------+-------+----+---+-------+-------+-------+-------+-------|
| imag_1000 | 5 |   | 0.022 | 0.040 | 0.068 | 0.064 | 0.122 | 20 |   | 0.088 | 0.156 | 0.158 | 0.166 | 0.230 |
| imag_20   | 5 |   | 0.024 | 0.022 | 0.038 | 0.016 | 0.032 | 20 |   | 0.042 | 0.066 | 0.050 | 0.054 | 0.042 |
| cont_10   | 5 |   | 0.018 | 0.022 | 0.034 | 0.030 | 0.028 | 20 |   | 0.044 | 0.054 | 0.054 | 0.050 | 0.022 |
| cont_100  | 5 |   | 0.018 | 0.022 | 0.030 | 0.024 | 0.004 | 20 |   | 0.058 | 0.050 | 0.032 | 0.032 | 0.014 |
| egomotion | 5 |   | 0.042 | 0.028 | 0.008 | 0.008 | 0.010 | 20 |   | 0.076 | 0.048 | 0.008 | 0.012 | 0.008 |
|-----------+---+---+-------+-------+-------+-------+-------+----+---+-------+-------+-------+-------+-------|


**** Pruebas con SF
***** Descripción del conjunto de datos y preprocesamiento
***** Descripción de la red
***** Entrenamiento y evaluación

* Conclusiones 
#+LaTeX: \label{sec:concl}

#+LaTeX: \bibliographystyle{babplain}
#+LaTeX: \bibliography{MarcoTeorico}

* Plan de Trabajo                                :noexport:
** Backlog
*** DONE Hacer visualizador de lmdbs para chequear que esten bien guardados los datos (basarse en el que ya hice para mnist)
*** DONE Abstraer creacion de lmdb en una clase.
*** DONE agregar label como parametro al insert_db para poder unificar SFA y egomotion
*** DONE Integrar SFA y egomotion en una sola base de datos en cada script de preprocessing
*** DONE Agregar path de lmdb como argumentdo command line del preprocess_...
*** DONE Remover Caffe de los "preprocess_mnist..etc". Ahora se linkea desde lmdb_creator
*** DONE [100%] Hacer las redes programatically, para poder automatizar los entrenamientos
**** DONE Egomotion para kitti y mnist, alexnet 
**** DONE SFA para kitti y mnist
**** DONE Standar para kitti
**** DONE Standar para mnist
*** DONE Agregar docs para lo que falta de preprocessing (sun, kitti) 
*** TODO [85%] Completar el pipeline de entrenamiento en Python:
**** DONE crear nets
**** DONE solver
**** DONE entrenar
**** DONE finetunear
**** TODO guardar best snapshot, no usar siempre el ultimo
**** DONE Armar todos los entrenamientos para MNIST
**** DONE mostrar resultados
*** DONE Guardar resultados de los entrenamientos grandes (siamese, imagenet) con pickle. Cargar los archivos si existen para evitar reentrenar todo si se corto un entrenamiento 
*** DONE Agregar experimentos de AlexNet con 20k y 1m de imagenes de Imagenet para SUN397
*** DONE Agregar el finetuning con las 5 ultimas capas para SUN397
*** DONE MNIST: cont_10 uso batch size de 800, hacer lo mismo para cont_100
*** TODO Probar pandas para guardar y mostrar los datos y evitar usar tantos diccionarios
*** TODO Hacer video con evolucion de los filtros a lo largo de las iteraciones. Puede ser util para mostrar que los filtros se aprenden mas rapido/lento/etc. 
Para esto tal vez sea mejor tener implementada la feature de guardar el mejor snapshot. De esa manera tenemos una marca de cuando llegamos a los mejores filtros
*** TODO Hacer plots de loss al finalizar el entrenamiento. Guardar .pdf
*** DONE probar con el modelo de lsm: extraer sus dimensiones y replicar en mis redes. CONCLUSION: las redes son identicas
*** DONE Solucionar problema de overfitting en finetuning de kitti
*** DONE Hacer test al finalizar entrenamiento, imprimir en pantalla log.
*** DONE Guardar logs de cada entrenamiento (o guardar pickle con los datos de loss y snapshots)
** Datasets
1. MINST: http://yann.lecun.com/exdb/mnist/

2. KITTI (odometry): http://www.cvlibs.net/datasets/kitti/eval_odometry.php

3. SF: https://embed.stanford.edu/iframe/?url=https%3A%2F%2Fpurl.stanford.edu%2Fvn158kj2087

4. ILSVRC2012: http://www.image-net.org/challenges/LSVRC/2012/nonpub-downloads (hay que crearse una cuenta)

5. SUN Dataset (Scene categorization) http://vision.princeton.edu/projects/2010/SUN/

** Prueba de Concepto con MNIST

EL objetivo es entender como hacer redes siamesas con Caffe

*** [100%] Preprocesamiento
Train set: 60K imagenes. Test set 10K imagenes.

Para pretraining de la red, hay que hacer un preprocesamiento del
dataset:
  1. Traslación relativa en un rango de [-3,3]
  2. Rotación relativa en un rango de [-30°,30°]
Tanto las traslaciones como las rotaciones fueron hechas con numeros
enteros (es decir, los angulos son -30º,-29º,-28º,..,29,30; igual para
las traslaciones).

Para cada par nuevo creado se eligieron las traslaciones y rotaciones
de manera aleatoria, de manera que cada par esta compuesto por la
imagen original y la imagen transformada aleatoriamente.

En total entrenaron con 5 millones de pares de imagenes. Eso significa
que hay que hacer 85 pares para 10000 imagenes y 83 pares para las
otras 50000 imagenes.

DUDA: no queda claro en el paper si el par esta compuesto por la
imagen original mas la imagen transformada o si esta compuesto por una
imagen previamente transformada a la cual se le hace una
transformacion relativa. Por lo pronto voy a efectuar el experimento
con la original+transformada.
**** DONE Crear codigo (c++) para levantar MNIST y devolver una lista de Mat's
**** DONE Modificar codigo para crear las transformaciones de rotacion y traslacion (OpenCV)
**** DONE Hacer que las rotaciones y traslaciones sean aleatorias
**** DONE Modificar codigo para guardar lmdb
**** DONE Modificar algunos aspectos de la parte de lmdb para guardar las nuevas imagenes (dos imagenes mergeadas en una, dimensiones: 28x28x2)
**** DONE Investigar como guardar multiples etiquetas por imagen (traslacion y rotacion)
https://groups.google.com/forum/#!searchin/caffe-users/multilabel/caffe-users/RuT1TgwiRCo/hoUkZOeEDgAJ
https://github.com/BVLC/caffe/issues/2407

Se probaron varias cosas:
***** Armar una DB para cada label (X,Y y Z) pero no anduvo
***** Armar una sola DB para todas las labels y despues slicearla pero no anduvo
para cada clase crear array de bytes asi: 0 0 0 1 donde si hay un cero
significa que esa label esta inactiva y si hay 1 significa que esta
activa

Va a haber un solo 1 por clase claramente

Entonces agarro y creo la base de datos de labels nomas, con la
dimension a lo ancho que sea la dimension de la clase de mayor tama;o
(en este caso las rotaciones, que son 61) y la profundidad que sea la
cantidad de clases a clasificar (en este caso 3: X, Y y Z).

Luego Sliceo la base de datos y con la capa ArgMax obtengo el indice
del mayor elemento de ese arreglo, que termina siendo la clase a la
que corresponde la imagen. A ese argmax obtenido lo mando a la capa de
accuracy/softmax por ejemplo

***** Armar una sola DB para datos y labels y despues slicear los labels
*** [100%] Entrenamiento
**** DONE Armar la red (.prototxt) teniendo en cuenta el Slice, los multiple Softmax y loss
Slice de los labels:
https://groups.google.com/forum/#!searchin/caffe-users/multilabel/caffe-users/RuT1TgwiRCo/hoUkZOeEDgAJ

BCCN: C96-P-C256-P
TCCN: F1000-D-Op

Para finetuning: BCCN-F500-D-Op

Para SFA, los valores optimos del parámetro m fueron 10 y 100.

La predicción es clasificación con tres capas soft-max loss (para
traslaciones en X,Y y rotacion en Z respectivamente). Cada SCNN
minimiza la suma de estas tres ``losses''.

Para que se pueda utilizar clasificación, hay que dividir los rangos
de traslación en en 7 classes y las rotaciones en 20 clases (donde
cada una corresponde a 3°)

Para MNIST, hay que tomar a las imágenes cuya traslación relativa esté
entre [-1,1] y rotación relativa entre [-3°,3°] como temporalmente
cercanas (es el parámetro T de la ecuación en la sección 3.3 del
paper).

**** DONE Iteraciones, learning rate, step
Para pretraining: 40K iteraciones con learning rate inicial de 0.01,
reducido en un factor de 2 cada 10K iteraciones.

Para finetuning: 4K iteraciones con un learning rate constante de
0.01.
**** DONE [100%] Experimentos a realizar
Todos se basan en el pre-entrenamiento descripto en el item y luego
finetuning.

Tanto para egomotion como para SFA, el pretraining se hizo durante 40K
iteraciones, con batches de 125 (5millones de imagenes.)

***** DONE Crear lmdb con 100, 300, 1000 y 10000 imagenes para testing
***** DONE Crear lmdb con 5m de pares de imagenes para egomotion
***** DONE Entrenar Egomotion, lr 0.01. 40K iter
***** DONE Entrenar Egomotion, lr 0.001. 40K iter
***** DONE Entrenar SFA, lr 0.01. 40K iter
***** DONE Entrenar SFA, lr 0.001. 40K iter

***** DONE Egomotion, lr 0.01. Finetuning con 100 imagenes lr 0.01
***** DONE Egomotion, lr 0.01. Finetuning con 300 imagenes lr 0.01
***** DONE Egomotion, lr 0.01. Finetuning con 1000 imagenes lr 0.01
***** DONE Egomotion, lr 0.01. Finetuning con 10000 imagenes lr 0.01
***** DONE Egomotion, lr 0.01. Finetuning con 100 imagenes lr 0.001
***** DONE Egomotion, lr 0.01. Finetuning con 300 imagenes lr 0.001
***** DONE Egomotion, lr 0.01. Finetuning con 1000 imagenes lr 0.001
***** DONE Egomotion, lr 0.01. Finetuning con 10000 imagenes lr 0.001

***** DONE Egomotion, lr 0.001. Finetuning con 100 imagenes lr 0.01
***** DONE Egomotion, lr 0.001. Finetuning con 300 imagenes lr 0.01
***** DONE Egomotion, lr 0.001. Finetuning con 1000 imagenes lr 0.01
***** DONE Egomotion, lr 0.001. Finetuning con 10000 imagenes lr 0.01
***** DONE Egomotion, lr 0.001. Finetuning con 100 imagenes lr 0.001
***** DONE Egomotion, lr 0.001. Finetuning con 300 imagenes lr 0.001
***** DONE Egomotion, lr 0.001. Finetuning con 1000 imagenes lr 0.001
***** DONE Egomotion, lr 0.001. Finetuning con 10000 imagenes lr 0.001
***** DONE Egomotion, lr 0.001. Finetuning con 100 imagenes lr 0.0001
***** DONE Egomotion, lr 0.001. Finetuning con 300 imagenes lr 0.0001
***** DONE Egomotion, lr 0.001. Finetuning con 1000 imagenes lr 0.0001
***** DONE Egomotion, lr 0.001. Finetuning con 10000 imagenes lr 0.0001

***** DONE SFA, lr 0.01. Finetuning con 100 imagenes lr 0.01
***** DONE SFA, lr 0.01. Finetuning con 300 imagenes lr 0.01
***** DONE SFA, lr 0.01. Finetuning con 1000 imagenes lr 0.01
***** DONE SFA, lr 0.01. Finetuning con 10000 imagenes lr 0.01
***** DONE SFA, lr 0.01. Finetuning con 100 imagenes lr 0.001
***** DONE SFA, lr 0.01. Finetuning con 300 imagenes lr 0.001
***** DONE SFA, lr 0.01. Finetuning con 1000 imagenes lr 0.001
***** DONE SFA, lr 0.01. Finetuning con 10000 imagenes lr 0.001

***** DONE SFA, lr 0.001. Finetuning con 100 imagenes lr 0.01
***** DONE SFA, lr 0.001. Finetuning con 300 imagenes lr 0.01
***** DONE SFA, lr 0.001. Finetuning con 1000 imagenes lr 0.01
***** DONE SFA, lr 0.001. Finetuning con 10000 imagenes lr 0.01
***** DONE SFA, lr 0.001. Finetuning con 100 imagenes lr 0.001
***** DONE SFA, lr 0.001. Finetuning con 300 imagenes lr 0.001
***** DONE SFA, lr 0.001. Finetuning con 1000 imagenes lr 0.001
***** DONE SFA, lr 0.001. Finetuning con 10000 imagenes lr 0.001
***** DONE SFA, lr 0.001. Finetuning con 100 imagenes lr 0.0001
***** DONE SFA, lr 0.001. Finetuning con 300 imagenes lr 0.0001
***** DONE SFA, lr 0.001. Finetuning con 1000 imagenes lr 0.0001
***** DONE SFA, lr 0.001. Finetuning con 10000 imagenes lr 0.0001

***** DONE Entrenamiento standar desde cero, lr 0.01. Finetuning con 100 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.01. Finetuning con 300 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.01. Finetuning con 1000 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.01. Finetuning con 10000 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.001. Finetuning con 100 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.001. Finetuning con 300 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.001. Finetuning con 1000 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.001. Finetuning con 10000 imagenes
**** DONE Probar swapeando orden en los pares de imagenes (1. modificada, 2. original) uniformemente.
**** DONE Probar con conv1: 96 x 11 x 11 (en lugar de 96 x 5 x 5) y conv2: 256 x 5 x 5
**** DONE Probar con conv1: 96 x 16 x 16 (en lugar de 96 x 5 x 5) y conv2: 256 x 7 x 7
**** DONE Probar con conv1: 96 x 13 x 13 y conv2: 256 x 7 x 7. lr0.001 para from scratch parece andar bien.
**** DONE Probar con conv1: 96 x 7 x 7 y conv2: 256 x 5 x 5
**** DONE Probar con los mismos settings que AlexNet(stride, group, etc). Ademas, la parte from scratch no usa preentrenamiento
**** DONE Probar sacando la fc100, poner el concat como bottom de las fc{x,y,z}
**** DONE Probar el codigo de Agrawal, la parte de mnist. Resultado: el codigo es horrible, no pude entender bien nada, logre correr un entrenamiento de MNIST pero los resultados fueron los mismos
**** DONE Probar con menos pares (<85)
**** DONE Seguir probando con la red que viene como ejemplo en caffe.
**** DONE Probar con exageraciones en las traslaciones (multiplicar por 3) -> da lo mismo
Sera que no todas las redes son buenos candidatos a siamesas? por
ejemplo, si tiene muchisimos pesos para aprender tal vez sea mala
candidata para una red siamesa.
*** Evaluación
Las features obtenidas de las BCNN preentrenadas se evaluan teniendo
en cuenta el error en la clasificación de dígitos.  Se utilizan
conjuntos de entrenamiento de 100,300, 1K y 10K obtenidos del training
set de MNIST (sin transformaciones).  El test set que viene con MNIST
se utiliza para testing.

*** Resultados
Los mejores resultados para egomotion se obtienen con lr0.01 en ambas
etapas (egomotion y finetuning). Con capas LRN despues de pooling. Con
normalizacion de los valores (de 255 a 0-1). Para contrastive loss
hubo que utilizar un lr de 0.001 y un batch size mas grande, sino las
redes no convergen.

Accuracies
| stand    | 0.426666667064 |  0.71199999253 | 0.837333321571 | 0.981333355109 |
| cont_10  |  0.53066666921 | 0.722666680813 | 0.826666673024 | 0.879999995232 |
| cont_100 | 0.333333333333 | 0.448000003894 | 0.503999988238 | 0.496000001828 |
| ego      | 0.757333318392 | 0.896000027657 | 0.949333349864 | 0.986666679382 |


** Experimentos con KITTI
*** Preprocesamiento
**** KITTI
Tiene 20501 imágenes. Se calculan las transformaciones entre las
imágenes cercanas utilizando los datos odométricos del dataset.
Similar a MNIST: se crean 20 clases para las transformaciones en X,Y,Z
(el paper no explica como). Se toman imágenes que estén separadas a lo
sumo por +-7 frames.  Para el entrenamiento se extraen patches de
227x227 de las imágenes (Caffe tiene la opcion de cropear la imagen a
la hora de entrenar, pero no se como se aplica a redes siamesas,
probablemente tenga que hacerlo como parte del preprocesamiento).

Para SFA, el threshold para imágenes temporalmente cercanas (T) es
también de +-7
El numero total de imagenes usadas para entrenamiento es 20501

**** SF
Análogo a KITTI, solo que además de las transformaciones en X,Y,Z
agregan los 3 ``euler angles'' (no entendi eso).

*** Arquitectura
BCNN: C96-P-C256-P-C384-C384-C256-P (dice que estan inspiradas en las
primeras capas de AlexNet, extraer tamaño de filtros de esa red)
TCNN: C256-C128-F500-D-Op. Los kernels convolucionales con 3x3.

*** Entrenamiento
Se entrena por 60K iteraciones con batch size de 128, learning rate
inicial de 0.001 (reducido en un factor de 2 cada 20K iteraciones)

**** DONE Crear dataset para egomotion
**** DONE Entrenar egomotion
**** DONE Probar con modelo preentrenado de ellos
    Anda bien con su modelo, a pesar de que los filtros no parecen ser
    tan buenos. Acc. 5.2 en scene recognition con 5 imagenes per class
    (el mio 3.2).
**** DONE Rever como se hacen los pares para SFA, solo los -+7 frames mas cercanos son similares. Y para egomotion?
**** DONE [#B] Leer scripts de lsm para ver como preprocesaron las imagenes ellos
**** DONE [60%] Comparar como calculo los tags
***** DONE Chequear que se obtengan bien las matrices de los archivos
***** DONE Comparar como calculo la matriz de rotacion en base a las dos imagenes. RESULTADO:  esta bien esa parte (get_pose_label)
***** DONE Comparar como calculo Euler Angles. RESULTADO: esta bien esa parte (mat2euler)
***** TODO [#C] Agregar normalizacion de valores euler/trans??
***** TODO [#B] Chequear como se sacan labels de los datos obtenidos (traslaciones y rotaciones)
**** DONE [#B] Entrenar egomotion con swap entre LRN y pooling (como estan en AlexNet original) (RESULTADOS: lo mismo)
**** DONE Crear dos lmdbs: una para las imagenes y otra para los labels (guardados como data y luego spliteados).
     De esa forma va a ser mas facil restar la media. Centrar los
     datos mediante mean substraction podria aumentar la performance.
**** DONE Crear lmdb para SFA
**** DONE Entrenar SFA

*** Evaluación
Los modelos KITTI-Net y SF-Net deben ser entrenados utilizando
alrededor de 20K imagenes unicas. Para hacer la comparacion mas justa
con las redes entrenadas con clases de imagenes, un model con AlexNet
sera entrenado con 20K imagenes tomadas de ILSVRC12 (20 ejemplos por
clase).  Las secciones de evaluacion en Intra-Class Keypoint Matching
y Visual Odometry los dejo para mas adelante.
**** Scene Recognition
Utilizar SUN database para el finetuning de las redes (SF-Net,
KITTI-Net y AlexNet-20K). El paper no aporta informacion sobre la
cantidad de iteraciones ni el learning rate usado.  Referirse al paper
para comparar resultados obtenidos.

***** DONE Descargar ILSVRC'12
***** DONE Conseguir AlexNet prototxt
***** DONE Preparar LMDBS de ILSVRC
***** DONE Entrenar AlexNet para clasification con ILSVRC'12 con 20, 10 y 1000 imagenes por clase
***** DONE Preprocesar SUN: crop por el centro con dimension mas chica
***** DONE Crear LMDBS SUN. Utilizar solo 3 splits, como en el paper. Crea un LMDB para cada split
***** DONE Volver a probar su model contra el mio. Probar con y sin pooling en la ultima capa a la hora de finetuning. Quedarse con el mejor.
***** DONE Probar con su deploy identico (finetuneando el de ellos con SUN y el mio con SUN)
***** DONE Modificar KITTI-Net, KITTI-SFA-Net agregando Softmax luego de cada capa convolucional
***** DONE Finetunear KITTI-Net, KITTI-SFA-Net con SUN, una vez con cada uno de los Softmax. Hacer 3 entrenamientos, uno con cada split.
***** DONE Comparar accuracies con AlexNet20, AlexNet1M, Kitti-Net, Kitti-SFA-Net. Usar accuracy normal, no se entiende que usan en el paper

**** Object Recognition
Utilizando subconjuntos de ILSVRC-2012 con 1, 5, 10 y 20 imagenes por
clase, hacer finetuning de KITTI-Net, KITTI-SFA-Net y AlexNet-Scratch
(AlexNet con pesos inicializados de manera aleatoria). Nuevamente el
paper no explica las iteraciones ni el learning rate utilizados.
** Mejoras al sistema
Probar con otras redes.
Otros datasets (elegir datasets pequeños y ver que pasa).


