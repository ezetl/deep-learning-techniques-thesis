#+TITLE: Marco Teorico
#+AUTHOR:  Ruben Ezequiel Torti Lopez
#+EMAIL:   ret0110@famaf.unc.edu.ar
#+OPTIONS: H:5 title:nil creator:nil timestamp:nil skip:nil toc:nil
#+STARTUP: indent hideblocks
#+TAGS: noexport(n)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+PROPERTY: session *R* 

#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{ifthen,figlatex}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \usepackage[american]{babel}
#+LATEX_HEADER: \usepackage{url}\urlstyle{sf}
#+LATEX_HEADER: \usepackage{amscd}
#+LATEX_HEADER: \usepackage{wrapfig}


* Plan de Trabajo

** Prueba de Concepto con MNIST
*** Preprocesamiento
El dataset tiene unas 60K imágenes. A cada dígito se le aplican dos
conjuntos de transformaciones aleatorias diferentes para generar los
pares (rotación y traslación).

*** Egomotion
Para pretraining de la red, hay que hacer un preprocesamiento del dataset:
  1. Traslación relativa en un rango de [-3,3]
  2. Rotación relativa en un rango de [-30°,30°].
La predicción es clasificación con tres capas soft-max loss (para
traslaciones en X,Y y rotacion en Z respectivamente). Cada SCNN
minimiza la suma de estas tres "losses".
Para que se pueda utilizar clasificación, hay que dividir los rangos
de traslación en en 7 classes y las rotaciones en 20 clases (donde
cada una corresponde a 3°)

*** Slow Feature Analysis (SFA)
Ellos comparan sus resultados (egomotion) contra SFA. En el paper
formulan SFA como un Contrastive Loss.
Para MNIST, hay que tomar a las imágenes cuya traslación relativa esté
entre [-1,1] y rotación relativa entre [-3°,3°] como temporalmente
cercanas (es el parámetro T de la ecuación en la sección 3.3 del
paper).

*** Arquitectura
BCCN: C96-P-C256-P
TCCN: F1000-D-Op
Para finetuning: BCCN-F500-D-Op
Para SFA, los valores optimos del parámetro m fueron 10 y 100.

*** Entrenamiento
Para pretraining: 40K iteraciones con learning rate inicial de 0.01,
reducido en un factor de 2 cada 10K iteraciones.
Para finetuning: 4K iteraciones con un learning rate constante de
0.01.

*** Evaluación
Las features obtenidas de las BCNN preentrenadas se evaluan teniendo
en cuenta el error en la clasificación de dígitos.  Se utilizan
conjuntos de entrenamiento de 100,300, 1K y 10K obtenidos del training
set de MNIST (sin transformaciones).  El test set que viene con MNIST
se utiliza para testing.
** Experimentos con KITTI y SF
*** Preprocesamiento KITTI
Tiene 20501 imágenes. Se calculan las transformaciones entre las
imágenes cercanas utilizando los datos odométricos del dataset.
Similar a MNIST: se crean 20 clases para las transformaciones en X,Y,Z
(el paper no explica como). Se toman imágenes que estén separadas a lo
sumo por +-7 frames.  Para el entrenamiento se extraen patches de
227x227 de las imágenes (Caffe tiene la opcion de cropear la imagen a
la hora de entrenar, pero no se como se aplica a redes siamesas,
probablemente tenga que hacerlo como parte del preprocesamiento).
Para SFA, el threshold para imágenes temporalmente cercanas (T) es
también de +-7
El numero total de imagenes usadas para entrenamiento es 20501

*** Preprocesamiento SF
Análogo a KITTI, solo que además de las transformaciones en X,Y,Z
agregan los 3 "euler angles" (no entendi eso).

*** Arquitectura
BCNN: C96-P-C256-P-C384-C384-C256-P (dice que estan inspiradas en las
primeras capas de AlexNet, extraer tamaño de filtros de esa red)
TCNN: C256-C128-F500-D-Op. Los kernels convolucionales con 3x3.

*** Entrenamiento
Se entrena por 60K iteraciones con batch size de 128, learning rate
inicial de 0.001 (reducido en un factor de 2 cada 20K iteraciones)

*** Evaluación
Los modelos KITTI-Net y SF-Net deben ser entrenados utilizando
alrededor de 20K imagenes unicas. Para hacer la comparacion mas justa
con las redes entrenadas con clases de imagenes, un model con AlexNet
sera entrenado con 20K imagenes tomadas de ILSVRC12 (20 ejemplos por
clase).  Las secciones de evaluacion en Intra-Class Keypoint Matching
y Visual Odometry los dejo para mas adelante.
**** Scene Recognition
Utilizar SUN database para el finetuning de las redes (SF-Net,
KITTI-Net y AlexNet-20K). El paper no aporta informacion sobre la
cantidad de iteraciones ni el learning rate usado.  Referirse al paper
para comparar resultados obtenidos.
**** Object Recognition
Utilizando subconjuntos de ILSVRC-2012 con 1, 5, 10 y 20 imagenes por
clase, hacer finetuning de KITTI-Net, KITTI-SFA-Net y AlexNet-Scratch
(AlexNet con pesos inicializados de manera aleatoria). Nuevamente el
paper no explica las iteraciones ni el learning rate utilizados.

* Datasets
1. MINST: http://yann.lecun.com/exdb/mnist/

2. KITTI (odometry): http://www.cvlibs.net/datasets/kitti/eval_odometry.php

3. SF: aparentemente es un challenge de ICMLA 2011, hay que mandar un mail para pedirlo: http://www.icmla-conference.org/icmla11/challenge.htm

4. ILSVRC2012: http://www.image-net.org/download-images (hay que crearse una cuenta)
* Herramientas   
Obtener acceso a algun server con Caffe+Ubuntu.

* Bibliografía
