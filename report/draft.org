
#+TITLE: Reconocimiento visual empleando técnicas de aprendizaje profundo
#+AUTHOR:  Rubén Ezequiel Torti López
#+EMAIL:   ret0110@famaf.unc.edu.ar
#+CREATOR: Rubén Ezequiel Torti López
#+LANGUAGE: es
#+OPTIONS: f:t TeX:t LaTeX:t H:5 title:nil creator:nil timestamp:nil skip:nil toc:3
#+STARTUP: indent hideblocks
#+TAGS: noexport(n)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+PROPERTY: session *R* 

#+BEGIN_COMMENT
######################## ATENCION ###################################

La generacion de figuras de matplotlib esta desactivada para generar
el reporte mas rapido (algunos code blocks incluyen tareas de
clasificacion REALES hechas con scipy/numpy y tardan un toque en
evaluarse).

Como consecuencia, para exportar este documento usando Emacs org-mode
es necesario primero generar todas las figuras de matplotlib mediante
el shortcut C-c C-v C-b.

Luego se exporta a un LaTEX con C-c C-e l (dependiendo de la version
de Emacs, es probable que haya que apretar "l" dos veces para
seleccionar la opcion "LaTEX").

Luego se corre el script ./compile_report.sh en esta misma carpeta
para generar y abrir el pdf.

Una vez generadas las figuras, ya no es necesario correr los code
blocks de nuevo.
#####################################################################
#+END_COMMENT

#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{ifthen,figlatex}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEZ_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage{url}\urlstyle{sf}
#+LATEX_HEADER: \usepackage{amscd}
#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATES_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage{babelbib}

#+LATEX_HEADER: \newcommand{\cl}{\textit{clasificadores lineales}}
#+LATEX_HEADER: \newcommand{\losss}{\textit{funciones de pérdida}}
#+LATEX_HEADER: \newcommand{\dg}{\textit{descenso de gradiente}}
#+LATEX_HEADER: \newcommand{\back}{\textit{backpropagation}}
#+LATEX_HEADER: \newcommand{\nn}{\textit{redes neuronales}}
#+LATEX_HEADER: \newcommand{\svms}{\textit{Support Vector Machines}}
#+LATEX_HEADER: \newcommand{\bow}{\textit{Bag of Words}}
#+LATEX_HEADER: \newcommand{\features}{\textit{features}}
#+LATEX_HEADER: \newcommand{\scores}{\textit{scores}}
#+LATEX_HEADER: \newcommand{\sift}{\textit{SIFT}}
#+LATEX_HEADER: \newcommand{\weights}{\(\boldsymbol{W}\)}
#+LATEX_HEADER: \newcommand{\img}{\(\boldsymbol{x_i}\)}
#+LATEX_HEADER: \newcommand{\bias}{\(\boldsymbol{b}\)}
#+LATEX_HEADER: \newcommand{\func}{\(\boldsymbol{f}\)}
#+LATEX_HEADER: \newcommand{\loss}{\(\boldsymbol{L}\)}

#+LATEX_HEADER: \newcommand{\ml}{\textit{machine learning}}
#+LATEX_HEADER: \newcommand{\ML}{\textit{Machine Learning}}
#+LATEX_HEADER: \newcommand{\dl}{\textit{deep learning}}
#+LATEX_HEADER: \newcommand{\DL}{\textit{Deep Learning}}
#+LATEX_HEADER: \newcommand{\cnn}{\textit{convolutional neural networks}}
#+LATEX_HEADER: \newcommand{\CNN}{\textit{Convolutional Neural Networks}}


#+BIND: org-latex-title-command "\maketitle"

#+LATEX_HEADER: \renewcommand*{\maketitle}{\begingroup
#+LATEX_HEADER: \begin{titlepage} 
#+LATEX_HEADER: \centering 
#+LATEX_HEADER: \vspace*{\baselineskip} 
#+LATEX_HEADER:  
#+LATEX_HEADER:  
#+LATEX_HEADER: {\LARGE Técnicas de deep learning}\\[0.2\baselineskip]
#+LATEX_HEADER: 
#+LATEX_HEADER:  
#+LATEX_HEADER: \vspace*{2\baselineskip}
#+LATEX_HEADER:  
#+LATEX_HEADER: {\Medium Rubén Ezequiel Torti López\par}
#+LATEX_HEADER:  
#+LATEX_HEADER: \vfill
#+LATEX_HEADER:  
#+LATEX_HEADER: \end{titlepage}
#+LATEX_HEADER: \newpage
#+LATEX_HEADER: \endgroup}

#+BEGIN_LaTeX
\newpage
#+END_LaTeX

* Capítulo 1
:PROPERTIES:
:UNNUMBERED: t
:END:
** Introducción

Para los seres humanos, percibir el mundo que nos rodea es una tarea
fácil. Millones de años de evolución nos han dotado con un sistema
visual altamente sofisticado que nos permite reconocer patrones muy
complejos del mundo tridimensional en el que vivimos. Distinguir
formas, sombras y color, o incluso cosas más generales, como
movimiento, potenciales amenazas y rostros de conocidos, son algunas
de las actividades que nuestros cerebros realizan casi de manera
inconsciente. Y si bien estas tareas pueden ser fáciles para nosotros,
no es tal el caso para las computadoras.

Richard Szeliski caracteriza a la visión por computadoras como un
\textit{problema inverso}, es decir, se busca describir el mundo que
uno ve en una o más imágenes y reconstruir sus propiedades tales como
forma, iluminación y distribuciones de color \cite{szeliski}

Una de las áreas principales de de la visión por computadoras es la
clasificación de imágenes. Es decir, dada una imagen y un conjunto de
categorías, determinar a cual de las categorías pertenece esa
imagen. Tener un buen entendimiento de los algoritmos de clasificación
es crucial para desarrollar otras tareas dentro de la visión por
computadoras dado que muchos problemas pueden ser reducidos a
clasificación: detección de objetos, descripción de imágenes y
segmentación entre otros.

Sin embargo, la clasificación de imágenes es un problema más difícil
de lo que aparenta. Una imagen es solamente un conjunto de números
(llamados \textit{píxeles}). Surge la pregunta entonces, ¿cómo darle
significado a un conjunto de números?. Se podría pensar en elaborar
alguna métrica de distancia con los píxeles de otra imagen cuya
categoría sea conocida, y si la distancia es menor a un cierto umbral
sabríamos que ambas pertenecen a la misma clase. Puesto en otras
palabras, lo que hicimos fue elegir una \textit{representación} de la
imagen basada en sus píxeles.

Sin embargo esta representación carece de robustez, ya que el menor cambio de
\textit{iluminación} podría alterar las métricas y confundir a nuestro
modelo. No solamente eso, los objetos en las imágenes podrían estar
parcialmente ocultos por el ambiente (\textit{oclusión}), o en
posiciones diferentes (\textit{deformación}), o ser exactamente
iguales, pero variar en colores y pequeños detalles (\textit{variaciónintraclase}).

Hay mejores métodos para representar imágenes: representaciones
basadas en color \cite{Color1} \cite{Color2} \cite{Color3}
\cite{Color4}, textura \cite{Texture1} \cite{Texture2} y
descriptores locales \cite{Lowe-SIFT} \cite{ORB} \cite{FREAK}
\cite{SURF} \cite{HOG} entre otros. Todos ellos fueron pensados para
atacar problemas puntuales en diversas áreas del procesamiento de
imágenes, lo que significa que utilizarlos para generar una
representacion de una imagen requiere un diseño manual y adaptado al
dominio de cada problema en particular.

Una vez se tiene una representación adecuada de la imagen, podemos
encarar el problema antes mencionado de clasificación. Numerosos
métodos se han utilizado a lo largo de la historia: \textit{máquinas vectores de soporte} \cite{SVM}, árboles de decisión \cite{Tree},
\textit{bolsas de palabras} \cite{BOW} entre otros. Comúnmente el
proceso consiste en obtener buenas representaciones mediante los
métodos manuales previamente mencionados y luego entrenar los
algoritmos.

Sin embargo, en los últimos años todos los algoritmos relativos a la
clasificación de imágenes fueron ampliamente superados por las Redes
Neuronales Convolucionales. Desde el año 2010, todos los equipos
ganadores del desafío ImageNet usaron Redes Neuronales
Convolucionales, cada vez con resultados más precisos
\cite{imagenet}. Un aspecto radicalmente distinto entre las redes
neuronales convolucionales y los métodos mencionados anteriormente es
que las primeras pueden obtener una representación de la imagen por si
mismas. Esto significa que pueden aprender representaciones que sean
más útiles al dominio del problema.

A pesar de haber tenido su golpe de popularidad en los últimos años,
los primeros esbozos de modelar redes neuronales artificiales datan de
1958, cuando Frank Rosenblatt ideó el \textit{perceptron}, un
algoritmo para reconocimiento de patrones basado en una red de dos
capas de aprendizaje \cite{perceptron}. Sin embargo, en 1969 se
estableció que el poder de cómputo disponible en ese entonces no
bastaba para poder entrenar y correr grandes redes neuronales,
implicando que el área se estancara durante años \cite{minsky}. Tan
así es, que recién en 2006, con el abaratamiento de costos en hardware
de alto desempeño se pudieron implementar arquitecturas más complejas
(no necesariamente nuevas) y redes neuronales más profundas, algo que
se conoce como Aprendizaje Profundo (\textit{Deep Learning}).

Para poder entrenar Redes Neuronales Profundas, es necesario contar
con un conjunto de datos anotados muy grande, cuyos tamaños pueden ir
de las decenas de miles hasta millones de imágenes. Generalmente se
requiere un gran esfuerzo humano para etiquetar tantas imágenes, por
lo que los conjuntos de datos de entrenamiento lo suficientemente
grandes suelen ser escasos, por ejemplo ImageNet \cite{imagenet},
SUN397 \cite{sun397}.

Actualmente contamos con una increíble cantidad de imágenes para
etiquetar. Para 2016, Cisco estimaba que el 51\% del tráfico de
Internet iba a provenir de dispositivos WiFi, tales como celulares,
\textit{tablets}, \textit{smart TVs}, etc.  Más aún, se estima que
para el 2019 el 80\% del tráfico IP va a ser en forma de píxeles
(multimedia), superando al 67\% que existente en 2014
\cite{ciscostats}. Como tendencia podemos nombrar a Youtube, donde la
mitad de los videos son subidos desde dispositivos móviles
\cite{youtustats}. Por otro lado, las redes sociales más populares
como Facebook, Flickr o Instagram almacenan una gigantesca cantidad de
imágenes, contando la última con más de 80 millones de imágenes
subidas por día \cite{instastats}. Podríamos concluir que hay
suficientes recursos para generar buenos conjuntos de datos, pero es
prácticamente imposible contar con los recursos suficientes para
anotar tantos videos, imágenes y demás contenido multimedia.

Una posible solución al entrenamiento de redes profundas cuando no se
puede anotar una gran cantidad de datos es la propuesta por Agrawal et
al. \cite{LSM2015}. En la misma proponen utilizar información
odométrica -inclinación, movimiento, rotación- disponible en agentes
móviles (giróscopos, acelerómetros, etc.) para \textit{preparar} los
modelos de redes neuronales profundas, y luego realizar una
transferencia de aprendizaje sobre un conjunto de datos anotados que
se desee, obteniendo la ventaja de no necesitar un conjunto tan grande
para lograr buena precisión. Este trabajo final tiene como objetivo
evaluar experimentalmente las metodologías propuestas por
\cite{LSM2015}.

El trabajo está organizado como sigue: en la Sección \ref{sec:marco}
se introducen conceptos del aprendizaje automático y las redes
neuronales artificiales para concluir con redes convolucionales, en la
Sección \ref{sec:siamesa} se presenta un método para entrenar
modelos de aprendizaje profundo mediante redes siamesas, que luego se
utilizará para reproducir los resultados de \cite{LSM2015} en la
Sección \ref{sec:agrawal}. Finalmente, las secciones \ref{sec:concl} y 
\ref{sec:future} presentan una discusión final y trabajo a futuro.

#+LaTeX: \newpage

* Marco Teórico
#+LaTeX: \label{sec:marco}
** Aprendizaje Automático

Las técnicas de aprendizaje automático tienen como objetivo
identificar patrones en conjuntos de datos utilizando herramientas de
la estadística, teoría de la información, cálculo y optimización entre
otras.

El aprendizaje automático adquiere relevancia cuando las tareas que se
desean automatizar son demasiado complejas para programarse
directamente. Como ejemplo tomemos la tarea de verificación de
rostros. Supongamos que queremos crear un sistema que genere
representaciones de imágenes de caras para luego diferenciarlas. El
sistema debe tener en cuenta detalles como variaciones en sombra,
color, orientación, por no mencionar las diferentes características
que hay que extraer de una cara para diferenciarla de otra (arrugas,
prominencias, etc.) \cite{faces}. Se puede ver que son demasiados
detalles y combinaciones a tener en cuenta como para programar cada
caso posible manualmente, por lo que un sistema que utilice
aprendizaje automático podría ser una mejor opción.

Hay varios paradigmas o ejes dentro del aprendizaje automatico que
definen los tipos de algoritmos, las técnicas de
entrenamiento y las potenciales aplicaciones de esos modelos:

**** Aprendizaje supervisado vs. no supervisado

Cuando se poseen anotaciones o alguna clase de etiqueta sobre los
datos a aprender, hablamos de aprendizaje supervisado. Retomando el
caso del verificador de rostros, las etiquetas serían el nombre o
algún identificador de cada persona y nuestro clasificador aprendería
a diferenciar las caras tomando como referencia las anotaciones.

Por otro lado, cuando los datos no están categorizados de antemano
hablamos de aprendizaje no supervisado. Por ejemplo, si se contara con
una lista de casas con sus respectivos precios, su área en metros
cúbicos y cantidad de habitaciones y quisiéramos encontrar alguna
relación entre ellas.

**** Aprendizaje pasivo vs. activo

El aprendizaje pasivo implica utilizar solamente los datos ya
existentes. El aprendizaje activo se refiere a interactuar con el
ambiente para obtener información, como por ejemplo preguntar a un
usuario si un rostro es de cierta persona y utilizar los datos
proporcionados durante su entrenamiento.

**** Aprendizaje \textit{online} vs. \textit{batch}

En el aprendizaje \textit{online} los datos están disponibles de
manera secuencial, actualizando el modelo en cada paso para lograr
mejores predicciones/clasificaciones. En el aprendizaje estadístico
primero se analiza una gran cantidad de datos (tal vez la totalidad
del conjunto) y solamente luego de haberlos analizado se pueden
obtener conclusiones o un modelo final.

*** Clasificadores lineales

Un clasificador lineal combina linealmente las características (o
\textit{features}) de los datos de entrada para determinar a que clase
pertenecen los mismos, usualmente entrenado mediante técnicas de
aprendizaje supervisado.

Imaginemos que queremos clasificar imágenes, es decir, asignar una
etiquetas a representaciones de imágenes desconocidas. Para ello vamos
a definir una función \func{} que mapee estas representaciones
\(\boldsymbol{x}\) a puntajes (\textit{scores}) para cada
etiqueta. Tomemos por ahora como representación de una imagen a sus
píxeles. Supongamos que contamos con un conjunto de datos de imágenes
\(\boldsymbol{x_i} \in \boldsymbol{R^{D}}\), donde \(\boldsymbol{i =
1\cdots N}\), \(\boldsymbol{D}\) es la dimensión de cada imagen y
\(\boldsymbol{y_i = 1\cdots K}\) es la etiqueta asociada. Es decir,
tenemos \(\boldsymbol{N}\) imágenes y \(\boldsymbol{K}\) categorías.

Definamos ahora una función \(\boldsymbol{f\colon R^{D} \mapsto
R^{K}}\) como un mapeo lineal entre píxeles y \scores:

\begin{equation}
     \boldsymbol{f(x_i, W, b)= W x_i + b}
\end{equation}

Asumimos que la imagen \img{} es un vector de una sola columna
\([D \times 1]\), \weights{} es una matriz \([K \times D]\) y \bias{} es
otro vector \([K \times 1]\). A menudo la matriz \weights{} es llamada
los \textit{pesos} de \func{}, y a \bias{} el \textit{vector de sesgo}
dado que influencia los \scores{} de salida, pero sin interactuar con
\img{}.

Para entender mejor a los clasificadores lineales, podemos verlos de
la siguiente manera: si la imagen tiene \(32 \times 32\) píxeles y la
representamos con un vector columna de dimensión \(D\) (en este caso
\(D=1024=32 \times 32\)), entonces en ese espacio \textit{D-dimensional} la
imagen es solamente un punto. Como se observa en la Figura
\ref{fig:cl} de manera simplificada, nuestro clasificador lineal
define una ``línea'' (un hiperplano) que separa cada clase
dentro de ese espacio multidimensional. Notar que en realidad la
multiplicación \(\boldsymbol{W x_i}\) está evaluado \(\boldsymbol{K}\)
clasificadores en paralelo, donde cada uno es una fila de \weights{}.


#+name: linear-classifier
#+begin_src python :session :exports none :results silent :cache yes :eval no-export
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import style
import os
import random
import numpy as np
from sklearn import svm

style.use("ggplot")

try:
    os.mkdir("images")
except:
    pass

matplotlib.use('Agg')

N = 50
x1 = np.random.normal(2, 0.5, size=50)
y1 = np.random.normal(2, 0.5, size=50)

x2 = np.random.normal(3.5, 0.5, size=50)
y2 = np.random.normal(3.5, 0.5, size=50)

fig = plt.figure()
axes = plt.gca()
axes.set_xlim([0,5])
axes.set_ylim([0,5])
ax1 = fig.add_subplot(111)
ax1.set_axis_bgcolor('white')  
ax1.grid(False, which='both')
#plt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off')
#plt.tick_params(axis='y', which='both', bottom='off', top='off', labelbottom='off')
ax1.axes.get_xaxis().set_ticks([])
ax1.axes.get_yaxis().set_ticks([])
ax1.spines['right'].set_visible(False)
ax1.spines['top'].set_visible(False)
for spine in ['left', 'bottom']:
    ax1.spines[spine].set_color('k')

ax1.scatter(x1, y1, s=20, c='b', marker="s")
ax1.scatter(x2, y2, s=20, c='r', marker="o")

# Fit a linear classifier
X = zip(x1,y1) + zip(x2,y2)
Y = [0]*50 + [1]*50 # 2 classes
linear_clf = svm.LinearSVC()
linear_clf.fit(X, Y)

# Get parameters and plot
coef = linear_clf.coef_[0]
a = -coef[0] / coef[1]
xx = np.linspace(-10,10)

yy = a * xx - linear_clf.intercept_[0] / coef[1]
yy2 = a * xx - linear_clf.intercept_[0] / coef[1] + 1
yy3 = a * xx - linear_clf.intercept_[0] / coef[1] - 1

ax1.plot(xx, yy, 'k-',  c='orchid', label=r'$Wx + b$')
ax1.plot(xx, yy2, '--',  c='mediumaquamarine', label=r'$Wx + (b + m)$')
ax1.plot(xx, yy3, '--',  c='sandybrown', label=r'$Wx + (b - m)$')

plt.legend(frameon=False)

plt.savefig('images/linear-classifier.pdf')
#+end_src

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:cl
#+caption: \fontsize{9}{11}\selectfont Clasificador lineal. Cada punto representa una muestra en un espacio de dimensión \(\boldsymbol{D}=2\) con \(\boldsymbol{K}=2\) categorías. La tarea del clasificador es establecer un hiperplano entre las dos clases de datos, definido por la ecuación \(f(x_i, W, b)= W x_i + b\). A modo de ejemplo están graficados dos clasificadores lineales más con el \textit{vector de sesgo} ligeramente modificado. Se puede observar que \bias{} no afecta al clasificador sino que simplemente lo traslada a lo largo de las dimensiones.
[[file:images/linear-classifier.pdf]]

Más adelante veremos cómo definir \weights{} y \bias{} para obtener un
buen clasificador.

*** Entrenamiento
En el caso de los clasificadores lineales, entrenar un modelo se
traduce en encontrar buenos valores de \weights{} y \bias{} que
minimicen el \textit{error} de acuerdo a algún criterio sobre el
conjunto de entrenamiento.

Es muy común, cuando se cuenta con un conjunto de datos lo
suficientemente grande, dividirlo en al menos 3 subconjuntos
disjuntos: uno para entrenar el modelo, un segundo para validar el
modelo durante el entrenamiento y un tercero para probar el modelo una
vez entrenado. De esta manera se puede medir que tan bien el modelo
aprendió características relevantes a la clasificación y las pudo
aplicar a un conjunto de datos completamente nuevo (conjunto de
pruebas). Si la precisión que obtuvo nuestro modelo sobre este
conjunto de pruebas es muy baja, es un síntoma de que algo no anda
bien (ver problema de \textit{sobreajuste} en la Sección
\ref{sec:regular}).

A grandes rasgos, podemos describir el proceso de entrenamiento de un
clasificador de la siguiente manera:

\begin{enumerate}

\item Primero se mide el error actual del modelo con el conjunto (o un
      subconjunto) de datos de entrenamiento

\item Luego se actualizan los parámetros del clasificador (\weights{} y
      \bias{}) para reducir ese error

\item Se repiten los pasos anteriores hasta lograr la convergencia del modelo

\end{enumerate}

Por lo tanto hay dos aspectos a tener en cuenta antes de entrenar un
modelo: cómo medir efectivamente la tasa de error y cómo actualizar
sus parámetros para minimizar la misma. Para el primer caso se define
lo que se llama una \textit{función de pérdida o costo}, mientras que
para el segundo analizaremos una técnica muy utilizada en aprendizaje
automático denominada \textit{descenso de gradiente}. Esto no
significa que sea la única alternativa para entrenar modelos, pero al
ser ampliamente utilizada en redes neuronales artificiales será la
única que analizaremos.

**** Función de costo

Una función de costo define un criterio de optimalidad que nos ayuda a
saber que tan bien o mal está actuando nuestro clasificador. Es decir,
si la tasa de error del clasificador es muy alta, el costo o la
\textit{pérdida} será muy alta y viceversa. 

Sea \(L\) la función de costo de la predicción la clase de \(x_i\)
cuando la respuesta esperada es \(y_i\) utilizando la función \(f\)
con parámetros \(\theta\), y supongamos que se cuenta con \(m\) datos
de entrenamiento. Entonces el costo total de \(f(x_i;\theta)\) para
todo el conjunto de datos es:

\begin{equation}
\boldsymbol{L}(\theta) = \frac{1}{m} \sum^{m}_{i} L(f(x_i;\theta), y_i)
\end{equation}

Notar que para el caso de un clasificador lineal los parámetros
\(\theta\) son \weights{} y \bias{}. De ahora en adelante utilizaremos
\(\theta\) o \weights{} indiferentemente para hablar de los parámetros
de nuestro modelo.

Un ejemplo de función de pérdida popular es la función de pérdida de
\textit{máquinas de vectores de soporte multiclase}. Sea \(f(x_i;
\theta)_j\) el puntaje (\textit{score}) asignado por el clasificador \(f\) a la clase
\(j\) con \(x_i\) como dato de entrada y parámetros \(\theta\) y sea
\(f(x_i, \theta)_{y_i}\) el puntaje asignado por \(f\) a la clase
verdadera de \(x_i\), o sea \(y_i\), entonces la pérdida para \(x_i\)
se calcula de la siguiente manera:

\begin{equation}
     L_i = \sum_{j \neq y_i} \max{(0, f(x_i; \theta)_j - f(x_i; \theta)_{y_{i}} + \Delta) }
\end{equation}

Se puede observar que esta función de pérdida busca que la clase
correcta tenga un puntaje más alto que las otras por un margen
\(\Delta\).

Cuando tenemos una función con la forma \( \max{(0, \cdot )} \) a menudo se
la llama función de pérdida bisagra (\textit{hinge loss} en inglés).

**** Descenso de Gradiente
#+LaTeX: \label{sec:sgd}

Ya contamos con una función para medir que tan bien o que tan mal está
comportándose nuestro modelo, la \textit{función de pérdida}. Como se
puede observar, esta función depende de nuestro \weights{} y las
imágenes (o \features{} de entrenamiento que estemos usando). Nosotros no tenemos
control sobre nuestro conjunto de datos de entrenamiento, pero sí
podemos modificar los parámetros de \weights{} para producir la menor
pérdida posible.

Para entender el algoritmo de descenso de gradiente tomemos un
escenario hipotético: imaginemos por un momento que una persona con
los ojos vendados está atrapada entre montañas y busca llegar al
valle. Una manera de llegar al valle es probar dando un pequeño paso a
su alrededor, y "sentir" hacia donde desciende más rápido la montaña,
sólo valiéndose de la información local para moverse. Cuando
finalmente esté seguro hacia donde descender, dará varios pasos en esa
dirección, se detendrá y volverá a observar. Sabemos que eventualmente
llegará al fondo del valle, pues lo único que tiene que hacer es
seguir bajando por la pendiente de la montaña.

Formalmente, la pendiente de la montaña es la pendiente de la función
de costo \loss{} que estemos utilizando y la dirección hacia donde
bajar se corresponde con la dirección negativa del gradiente de
\loss{} en ese punto, ya que la función \textit{decrece} en el sentido
opuesto que indica el gradiente. En otras palabras, lo que estamos
haciendo es buscar el mínimo de \loss{}, y en consecuencia, el
conjunto de parámetros de \weights{} que minimicen el costo.

El descenso de gradiente (\textit{SGD} por sus siglas en inglés) se
utiliza para optimizar los pesos partiendo de la premisa que el modelo
es diferenciable localmente (o se puede aproximar su derivada) con
respecto a \weights{}. Dado que queremos minimizar la función de costo
\loss{}, lo que vamos a hacer es calcular su gradiente
\(\boldsymbol{\nabla L}\) respecto a cada parámetro y luego modificar
cada uno ligeramente con el objetivo de acercarlo al mínimo en la
función. Entonces, si \(\theta_{n}\) son nuestros parámetros en el
paso \(n\) del entrenamiento, calculamos \(\theta_{n+1}\) de la
siguiente manera:

\begin{equation}
    \theta_{n+1} = \theta_n - \epsilon \frac{1}{m} \sum^{m}_{i} \nabla_{\theta_{n}} L(f(x_i; \theta_n), y_i)
\end{equation}

Donde \(\epsilon\) es conocido como la \textit{tasa de aprendizaje} y
\(m\) es la cantidad de elementos en el conjunto de datos. Notar que
la tasa de aprendizaje determina una fracción del gradiente a
sustraer. En la ecuación se puede observar que se modifican los
parámetros con respecto a la dirección opuesta al gradiente, dado que
el mismo indica la dirección de crecimiento de una función pero
nosotros queremos encontrar un mínimo (Figura \ref{fig:gd}).

#+name: gradient-descent
#+begin_src python :session :exports none :results silent :cache yes :eval no-export
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.mlab as mlab
from matplotlib import style
import numpy as np
from sklearn import svm

style.use("ggplot")

matplotlib.use('Agg')

fig = plt.figure()
axes = plt.gca()
axes.set_xlim([-2.5,2.5])
axes.set_ylim([-3,3])
ax1 = fig.add_subplot(111)
ax1.set_axis_bgcolor('white')  
ax1.grid(False, which='both')
ax1.axes.get_xaxis().set_ticks([])
ax1.axes.get_yaxis().set_ticks([])
ax1.spines['right'].set_visible(False)
ax1.spines['top'].set_visible(False)
ax1.spines['left'].set_visible(False)
ax1.spines['bottom'].set_visible(False)

delta = 0.025
x = np.arange(-3.0, 3.0, delta)
y = np.arange(-2.0, 2.0, delta)
X, Y = np.meshgrid(x, y)
Z1 = mlab.bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)

CS = plt.contour(X, Y, Z1)

ax1.annotate(r'$w_3$', fontsize=15, xy=(-0.22, 0.16), xytext=(-0.49, 0.49),
             arrowprops=dict(width=2, headwidth=4, facecolor='black', shrink=0.01),)

ax1.annotate(r'$w_2$',  fontsize=15, xy=(-0.53, 0.53), xytext=(-0.85, 0.73),
             arrowprops=dict(width=2, headwidth=4, facecolor='black', shrink=0.01),)

ax1.annotate(r'$w_1$', fontsize=15, xy=(-0.90, 0.76), xytext=(-1.02, 1.27),
             arrowprops=dict(width=2, headwidth=4, facecolor='black', shrink=0.01),)

ax1.annotate(r'$w_0$', fontsize=15, xy=(-1.04, 1.30), xytext=(-1.61, 1.25),
             arrowprops=dict(width=2, headwidth=4, facecolor='black', shrink=0.01),)

plt.savefig('images/gradient-descent.pdf')
#+end_src

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:gd
#+caption: \fontsize{9}{11}\selectfont Descenso de gradiente. Una función de clasificación \(f\) alcaza un mínimo en su parámetro \(w\) a medida que se actualiza su valor mediante la substracción del gradiente calculado en ese parámetro.
[[file:images/gradient-descent.pdf]]

Lo más comun al utilizar SGD es mediante una técnica llamada
\textit{descenso de gradiente por mini-batch}, que se basa en calcular
el gradiente de un subconjunto del total de datos (llamado
\textit{mini-batch}) y actualizar \weights{} al final de cada
iteración. Esto es muy útil dado que es computacionalmente costoso
calcular el gradiente de todo un conjunto de datos con miles de
imágenes a la vez y calcular el gradiente de un \textit{batch}
aproxima bastante bien el gradiente del total \cite{minibatch}.

**** Clasificador \textit{Softmax}
Antes de saltar de lleno a las redes neuronales artificiales vamos a
describir brevemente un tipo de clasificador muy utilizado en las
mismas, el clasificador \textit{Softmax}.

La función \textit{Softmax} tiene la siguiente forma:

\begin{equation}
    \sigma(x)_j =  \frac{e^{f(x;\theta)_{j}}} {\sum_{k} e^{f(x;\theta)_{k}}}
\end{equation}

Dado que exponencia el resultado de \(f(x;\theta)\), se puede observar
que sus resultados son siempre positivos. Además normaliza el valor de
cada salida con respecto a todas las salidas, por lo que su resultado
es siempre un número entre 0 y 1. Por lo tanto, \textit{Softmax}
devuelve la \textit{probabilidad} de \(x\) de pertenecer a cada una de
las clases \(k\).

La función de costo utilizada comúnmente con \textit{Softmax} se
denomina en inglés \textit{log-likelihood}. Sea \(f(x_i,
\theta)_{y_{i}}\) la probabilidad computada mediante \textit{Softmax}
para la clas \(y_i\), entonces \textit{log-likelihood} se expresa:

\begin{equation}
     L_i = - \log \bigg( \frac{e^{f(x_i, \theta)_{y_{i}}}} {\sum_j e^{f(x_i, \theta)_j}}\bigg)
\end{equation}

Observar que si el clasificador se equivocó al predecir (o sea, asignó
una probabilidad \(p\) muy baja a la clase correspondiente \(y_i\))
entonces el costo será muy alto. Como \(log(p) \to -\inf\) cuando \(p
\to 0\), intuitivamente podemos ver que \(-log(p) \to \inf\) cuando
\(p \to 0\). En cambio si clasificador predijo con más probabilidad la
clase \(y_i\), significa que \(p\) es más cercano a \(1\) y por ende,
\(-log(p)\) es más cercano a \(0\).

Nuevamente, notar que la pérdida total de nuestro conjunto de datos en
un determinado paso del entrenamiento es el promedio de las pérdidas
de cada elemento del conjunto.

** Redes Neuronales Artificiales

Hasta ahora analizamos clasificadores lineales y un tipo particular
llamado softmax. Si conectáramos la salida de un clasificador lineal
\(s_1=W_1x+b_1\) con la entrada de otro clasificador \(s_2=W_2y+b_2\),
entonces obtendríamos un tercero:

\begin{equation}
s_3 = W_2 (W_1x + b_1) + b_2 = (W_2 W_1) x + (W_2 b_1) + b_2
\end{equation}

\begin{equation}
s_3 = W_3 x + b_3
\end{equation}

Es fácil hacer un chequeo de dimensiones para ver que efectivamente
podemos ``colapsar'' las matrices \(W_2\) y \(W_1\) en una sola, por lo
cual terminamos con otro clasificador lineal.

Notemos que por más que combinemos miles de clasificadores lineales
vamos a obtener un nuevo clasificador también lineal.  Una manera de
romper la linealidad de estas ``capas'' de clasificadores es, por
ejemplo, agregar lo que se llama \textit{función de activación}:
 
\begin{equation}
    s = W_2 \max{(0, W_1 x + b_1)} + b_2
\end{equation}
 
 
Lo que acabamos de definir es una red neuronal básica de dos capas, de
una neurona cada una.

En la Figura \ref{fig:art-neuron} vemos un modelo formal de una
neurona estándar, en el que las entradas \(x_i\) interactúan
multiplicativamente con los pesos \(w_i\). Luego, esos resultados se
suman junto con un vector de sesgo y sobre eso se computa lo que se
llama \textit{función de activación} que decide si transmitir o no la
salida.

#+name: neuron
#+begin_src python :session :exports none :results silent :cache yes :eval no-export
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.cbook as cbook

image = plt.imread("images/artificial-neuron.png")

fig, ax = plt.subplots()
im = ax.imshow(image)

props = {'ha': 'center', 'va': 'center'}
ax.text(75.50, 31.16, r'$x_1$', props, rotation=0, fontsize=15)
ax.text(75.50, 237.25, r'$x_2$', props, rotation=0, fontsize=15)
ax.text(75.50, 361.72, r'$x_3$', props, rotation=0, fontsize=15)
ax.text(84.50, 650.98, r'$x_{n-1}$', props, rotation=0, fontsize=15)
ax.text(75.50, 822.76, r'$x_n$', props, rotation=0, fontsize=15)

ax.text(419.30, 140.16, r'$\times w_1$', props, rotation=-25, fontsize=15)
ax.text(419.30, 285.41, r'$\times w_2$', props, rotation=-15, fontsize=15)
ax.text(419.30, 387.19, r'$\times w_3$', props, rotation=-8, fontsize=15)
ax.text(450.30, 598.43, r'$\times w_{n-1}$', props, rotation=11, fontsize=15)
ax.text(419.30, 729.36, r'$\times w_n$', props, rotation=28, fontsize=15)

ax.text(856.71, 470.73, r'$\mathbf{\sum_{i=0}^n w_i x_i + b}$', props, rotation=0, fontsize=18)

ax.text(1219.67, 433.80, r'$\mathbf{Y}$', props, rotation=0, fontsize=18)

ax.text(1529.7, 480.88, r'$\mathbf{f(Y)}$', props, rotation=0, fontsize=18)

ax.text(1808, 433.80, r'$\mathbf{\widetilde{Y}}$', props, rotation=0, fontsize=18)

plt.axis('off')

plt.savefig('images/artificial-neuron.pdf')
#+end_src

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:art-neuron
#+caption: \fontsize{9}{11}\selectfont Esquema de una neurona artificial.
[[file:images/artificial-neuron.pdf]]

 
*** Funciones de activación comunes

Se han propuesto varias funciones de activación a lo largo de los
años. Nos concentraremos en las unidades \textit{ReLU}
(\textit{Rectifier Liner Unit} en inglés), actualmente muy populares
en las redes convolucionales.

Hay tres tipos de rectificadores lineales:

**** \textit{ReLU}
Una unidad ReLU establece un umbral en \(0\) a la salida de la
neurona. Es decir, la activación de una neurona va a ser \(0\) si su
salida fue negativa o un numero positivo en caso contrario (Figura
\ref{fig:relu}.a):

\begin{equation}
f(x) = \max{(0,x)}
\end{equation}

Se puede observar que requiere muy pocas operaciones, además está
comprobado empíricamente que los modelos que utilizan \textit{ReLU}
convergen hasta 6 veces más rápido \cite{NIPS2012_4824} que con otras
funcines de activación (como la \textit{sigmoide}
\cite{sigmoid} y \textit{tangente hiperbólica}).

Una desventaja de las \textit{ReLU} es que pueden provocar la
``muerte'' de neuronas durante el entrenamiento. El problema esta
relacionado con el método más común de entrenamiento de redes
neuronales, \back{}. El algoritmo de \back{} será explicado en la
Sección \ref{sec:backprop}, pero por ahora pensemos que el proceso de
optimización de \weights{} implica restar un porcentaje del gradiente
de la función de costo en \weights{}. Si el gradiente es muy grande
entonces los pesos sobre los que se realice la actualización
terminarán siendo muy pequeños (negativos). Como consecuencia, la
unidad \textit{ReLU} no volverá a activarse, pues sus valores de
entrada siempre van a ser valores negativos. Esta situación puede
agravarse si la tasa de aprendizaje es muy grande.

Una vez que la ReLU alcanza este estado, es improbable que vuelva a
activarse, dado que su gradiente (aproximado por lo que se llama el
\textit{subgradiente}) en \(0\) es también \(0\), por lo que un
entrenamiendo mediante descenso de gradiente y \back{} no va a
modificar los pesos locales, dejando a esa neurona ``muerta''.

**** \textit{Leaky ReLU}

Se puede solucionar el problema de la muerte de neuronas agregando
una pequeña pendiente negativa (de 0.01 por ejemplo) en los valores
negativos de la \textit{ReLU}. Esta función de activación es la que se
conoce como \textit{Leaky ReLU} \cite{zhang2014improving} (Figura
\ref{fig:relu}.b):

\begin{equation}
f(x) = 1(x<0)(\alpha x) + 1(x >= 0)(x)
\end{equation}

De esta manera nos aseguramos que al menos un pequeño gradiente fluya
durante \back{} cuando la neurona emite resultados negativos,
permitiendo que se normalicen los pesos a mediano/largo plazo. Sin
embargo no está demostrado que las \textit{Leaky ReLU} presenten una
mejora sustancial en el entrenamiento de las redes, por lo que las
\textit{ReLU} convencionales siguen siendo ampliamente usadas.

**** \textit{Maxout}

\begin{equation}
f(x) = \max{(w^{T}_{1} x + b_{1}, w^{T}_{2} x + b_{2})}
\end{equation}

\textit{Maxout} \cite{Maxout} es una generalización de las funciones
\textit{ReLU}, y obtiene lo mejor de los dos mundos: por un lado la
forma lineal y no saturante de las \textit{ReLUs} y por el otro evita
el problema de la muerte de neuronas. A pesar de ello tiene la
desventaja de duplicar los parámetros para cada neurona, lo cual no
siempre es deseable, pues implica más tiempo de entrenamiento y
más consumo de memoria y recursos, sobre todo en redes profundas.
 
Notar que una \textit{ReLU} normal es básicamente una \textit{maxout}
con \(w_1,b_1 = 0\).

#+name: relus
#+begin_src python :session :exports none :results silent :cache yes :eval no-export
import matplotlib
matplotlib.use('Agg')

import matplotlib.pyplot as plt
from matplotlib import style
import numpy as np

style.use("ggplot")

def config_ax(ax):
    ax.set_axis_bgcolor('white')
    ax.grid(False, which='both')
    ax.axes.get_xaxis().set_ticks([])
    ax.axes.get_yaxis().set_ticks([])
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.plot((0, 0), (0, 5), 'grey')
    ax.plot((-10, 10), (0, 0), 'grey')

#fig, axes = plt.subplots(nrows=1, ncols=1)
fig = plt.figure(figsize=(4, 4))
ax = plt.subplot(111)
#for ax in axes:
config_ax(ax)
ax.set_xlim([-5, 5])
ax.set_ylim([-1, 10])
X = np.arange(-10, 10)
ax.plot(X, np.maximum(0, X))
#ax.set_xlabel('x')
ax.text(0,-0.5, "0")
ax.text(4.7,-0.3, r'$+x$')
ax.text(-5,-0.3, r'$-x$')
ax.text(-0.6,4.6, r'$+y$')
#h = ax.set_ylabel('y')
#h.set_rotation(0)
plt.tight_layout()
plt.savefig('images/relu1.pdf')


fig = plt.figure(figsize=(4, 4))
ax = plt.subplot(111)
#for ax in axes:
config_ax(ax)
ax.set_xlim([-5, 5])
ax.set_ylim([-1, 10])
X = np.arange(0, 10)
Y = np.arange(0, 10)
X2 = np.arange(-10, 0)
Y2 = X2 *  0.1
X = np.append(X2, X)
Y = np.append(Y2, Y)
ax.text(0,-0.5, "0")
ax.text(4.7,-0.3, r'$+x$')
ax.text(-5,-0.3, r'$-x$')
ax.text(-0.6,4.6, r'$+y$')
ax.plot(X, Y)
plt.tight_layout()
plt.savefig('images/relu2.pdf')
#+end_src

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{images/relu1.pdf}
        \caption{\textit{ReLU}}
        \label{fig:relu1}
    \end{subfigure}
\quad
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{images/relu2.pdf}
        \caption{\textit{Leaky ReLU}}
        \label{fig:relu2}
    \end{subfigure}
    \caption{\fontsize{9}{11}\selectfont \textit{ReLU} vs. \textit{Leaky ReLU}. Se puede observar la pendiente negativa en \ref{fig:relu2} para \(x<0\), la cual produce un gradiente \(\neq 0\) y evita la muerte de neuronas.}\label{fig:relu}
\end{figure}
#+END_LaTeX

** Entrenamiento de redes neuronales artificiales

Entrenar una red neuronal artificial no es muy distinto a entrenar un
clasificador lineal. Necesitamos definir una función de pérdida y un
método para ajustar los parámetros. Veremos además, como en otras
tareas de aprendizaje automático, que hay que tener en cuenta el
formato de los datos de entrada al model (tal vez eliminar ruido o
redundancia, normalizar las dimensiones). Esta tarea se denomina
preprocesamiento de datos.

También analizaremos varias técnicas para evitar el sobre-ajuste de
modelos. El sobre-ajuste surge cuando un modelo aprende ``ruido'' y
detalles particulares del conjunto de datos en vez de características
generales que ayuden a la tarea de clasificación. 

Finalizaremos esta sección con un análisis de la organización interna
de las redes neuronales artificiales y qué algoritmos se utilizan
para entrenar.

*** Preprocesamiento de datos

Antes de comenzar con el entrenamiento de una red neuronal artificial
es conveniente analizar los datos y si es necesario normalizarlos para
que todos estén en el mismo rango de valores.

El preprocesamiento de datos, como alinear imágenes o normalizar
valores ayuda a una mejor convergencia de los modelos. Las dos
técnicas más comunes de preprocesamiento de datos para redes
neuronales son la substracción de la media y la normalización.

**** Substracción de la media

Como su nombre lo indica, se le resta la media a cada elemento del
conjunto de datos con el objetivo de \textit{centrar} los datos
alrededor del origen en todas las dimensiones (Figura
\ref{fig:mean2}). Si hablamos de un conjunto de datos de imágenes esto equivale a
restarle el valor medio de los píxeles a cada píxel de la imagen de
entrada.

#+name: mean-substraction
#+begin_src python :session :exports none :results silent :cache yes :eval no-export
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import style
from matplotlib import gridspec
import numpy as np

matplotlib.use('Agg')
style.use("ggplot")

def plot_norm(x, y, color, index='0'):
    fig = plt.figure(figsize=(4, 4))
    ax = plt.subplot(111)
    ax.set_axis_bgcolor('white')
    ax.grid(False, which='both')
    ax.axes.get_xaxis().set_ticks([])
    ax.axes.get_yaxis().set_ticks([])
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.plot((0, 0), (-4, 4), 'grey')
    ax.plot((-4, 4), (0, 0), 'grey')
    ax.set_xlim([-4, 4])
    ax.set_ylim([-4, 4])
    ax.scatter(x, y, s=30, c=color, marker="o")
    plt.tight_layout()
    plt.savefig('images/mean'+index+'.pdf')

   
N = 150
x1 = np.random.normal(2, 0.5, size=N)
y1 = np.random.normal(2, 0.5, size=N)
plot_norm(x1, y1, 'royalblue', '1')

mx = np.mean(x1)
my = np.mean(y1)
x2 = x1 - mx
y2 = y1 - my
plot_norm(x2, y2, 'seagreen', '2')

x3 = x2 / 2.0
y3 = y2 / 2.0
plot_norm(x3, y3, 'firebrick', '3')
#+end_src

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/mean1.pdf}
        \caption{Original}
        \label{fig:mean1}
    \end{subfigure}
\quad
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/mean2.pdf}
        \caption{Substraer media}
        \label{fig:mean2}
    \end{subfigure}
\quad
   \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/mean3.pdf}
        \caption{Normalizacion}
        \label{fig:mean3}
    \end{subfigure}
    \caption{\fontsize{9}{11}\selectfont Se puede observar en \ref{fig:mean2} cómo al substraer la media de \ref{fig:mean1} logramos ``centrar'' nuestro conjunto de datos. En \ref{fig:mean3} podemos apreciar los resultados de la normalización de datos, logrando que todos los elementos pertenezcan al mismo rango de valores.}\label{fig:mean}
\end{figure}
#+END_LaTeX

**** Normalización

Una manera de normalizar los datos es dividir cada dimensión por su
desviación estándar una vez que haya sido centrada en cero. De esta
manera se logra que las dimensiones tengan aproximadamente la misma
escala (Figura \ref{fig:mean3}). Notar que en general los píxeles
tienen valores en el rango de 0 a 255, por lo que sus dimensiones ya
se encuentran en escalas parecidas y cuando se trabaja con redes
convolucionales no es estrictamente necesario normalizar los datos de
entrada.

**** Otras maneras de preprocesar datos

A la hora de entrenar redes convolucionales importan dos cosas: la
calidad de los datos y la cantidad. Es necesario que además de
preprocesar los datos con las técnicas usuales (substracción de media
por ejemplo), se tengan en cuenta aspectos de más alto nivel. Por
ejemplo, si estuvieramos entrenando una red de reconocimiento de
rostros, puede ser mejor contar con un conjunto de datos de caras
alineadas en vez de uno de caras en diferentes posiciones y ángulos
que tenga más ruido. De esa manera vamos a lograr que la red aprenda
mejor qué aspectos extraer de las imágenes.

Además no siempre se puede contar con un dataset de millones de
imágenes para entrenar nuestra red, por lo que suele ser necesario aumentar
nuestros datos con técnicas de \textit{data augmentation}: repetir la
misma imagen pero con diferentes variaciones en el color, brillo,
saturación, incluso hacer leves desplazamientos y rotaciones.

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{images/face-original.jpg}
        \caption{Original}
        \label{fig:forig}
    \end{subfigure}
\quad
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{images/face-aligned.jpg}
        \caption{Alineacion}
        \label{fig:falign}
    \end{subfigure}
\quad
   \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{images/face-augmented1.jpg}
        \caption{Color}
        \label{fig:faug1}
    \end{subfigure}
\quad
   \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{images/face-augmented2.jpg}
        \caption{Traslaciones}
        \label{fig:faug2}
    \end{subfigure}
    \caption{\fontsize{9}{11}\selectfont Diferentes formas de aumentar datos. Si trabajamos con rostros es muy común alinearlos, por ejemplo, sobre el eje que conforman los ojos \ref{fig:falign}. Traslaciones, recortes en la imagen y cambios en el brillo, contraste y color son otras técnicas muy usadas (\ref{fig:faug1} y \ref{fig:faug2}).}\label{fig:augm}
\end{figure}
#+END_LaTeX

*** Inicialización de pesos

A la hora de inicializar los pesos es escencial romper con la
simetría. Imaginemos que inicializamos todos los pesos en \(0\), algo
que podría parecer razonable. En una capa completamente conectada,
entonces todas las neuronas van a recibir el mismo valor de entrada
\(0\) (\(f(x)=\sum_i w_i x\) con \(w_i=0\)) por lo que sus salidas van
a ser todas iguales y por ende los gradientes que se calculen serán
los mismos, produciendo que los pesos se actualicen todos iguales y la
red no aprenda.

En cambio podemos inicializar los pesos con pequeños números
aleatorios cercanos a cero. Una opción común es utilizar una
distribución gaussiana con media cero y desviación estándar 0.01. Este
método, si bien es bastante \textit{ad-hoc}, es bastante usado. Hay
muchas otras maneras más sofisticadas de inicializar los pesos de una
red, pero su análisis escapa al alcance de este trabajo.

*** Evitando el sobre-ajuste: Regularización y Dropout
#+LaTeX: \label{sec:regular}

Cuando se aprende un modelo sobre un conjunto de datos, puede surgir
el problema del \textit{sobre-ajuste}, más conocido por su nombre en
inglés \textit{overfitting}. El \textit{overfitting} significa que
nuestro modelo ajustó sus parámetros demasiado bien al conjunto de
datos de entrenamiento, provocando que aprendiera detalles
insignificantes del mismo, principalmente \textit{ruido}
aleatorio. Como consecuencia, cuando se lo aplica en un conjunto de
datos nuevo, el modelo presenta un bajo rendimiento. En contrapartida
al \textit{overfitting}, a veces puede pasar que nuestro modelo
aprendió pocas características de nuestro conjunto de entrenamiento y
termina siendo muy genérico e inflexible a la hora de ser aplicado en
un conjunto nuevo, obteniendo también baja precisión 

A modo de ejemplo, en la Figura \ref{fig:over1} se puede observar un
gran sesgo en el caso de \textit{underfitting}, que si bien permite
una mayor generalización no logra distinguir el límite entre ambas
clases, lo cual se traduce en menor precisión a la hora de evaluar el
modelo. En la Figura \ref{fig:over3} observamos un típico caso de
\textit{overfitting} con mucha variación y sensibilidad a los datos de
entrenamiento, lo cual implica poca generalización a nuevos datos,
mientras que en la Figura \ref{fig:over2} se observa un buen ajuste
del conjunto de datos.


#+name: overfitting
#+begin_src python :session :exports none :results silent :cache yes :eval no-export
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.datasets import make_moons
from sklearn.cross_validation import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
import numpy as np
from sklearn.cross_validation import train_test_split, cross_val_score


def detect_plot_dimension(X, h=0.02, b=0.05):
    x_min, x_max = X[:, 0].min() - b, X[:, 0].max() + b
    y_min, y_max = X[:, 1].min() - b, X[:, 1].max() + b
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    dimension = xx, yy
    return dimension


def detect_decision_boundary(dimension, model):
    xx, yy = dimension # unpack the dimensions
    boundary = model.predict(np.c_[xx.ravel(), yy.ravel()])
    boundary = boundary.reshape(xx.shape) # Put the result into a color plot
    return boundary


def plot_decision_boundary(panel, dimension, boundary, colors=['paleturquoise', 'khaki']):
    xx, yy = dimension # unpack the dimensions
    panel.contourf(xx, yy, boundary, cmap=ListedColormap(colors), alpha=1)
    panel.contour(xx, yy, boundary, colors="brown", alpha=1, linewidths=0.5) # the decision boundary in green


def plot_dataset(panel, X, y, colors=["darkorange","darkcyan"], markers=["s", "o"]):
    panel.scatter(X[y == 1, 0], X[y == 1, 1], color=colors[0], marker=markers[0])
    panel.scatter(X[y == 0, 0], X[y == 0, 1], color=colors[1], marker=markers[1])


def calculate_prediction_error(model, X, y):
    yPred = model.predict(X)
    score = 1 - round(metrics.accuracy_score(y, yPred), 2)
    return score


def explore_fitting_boundaries(model, n_neighbors, datasets, width, index):
    # determine the height of the plot given the
    # aspect ration of each panel should be equal
    height = float(width)/len(n_neighbors) * len(datasets.keys())
    nrows = len(datasets.keys())
    ncols = len(n_neighbors)
    # set up the plot
    figure, axes = plt.subplots(
        1,
        1,
        figsize=(width, height),
        sharex=True,
        sharey=True
    )
    dimension = detect_plot_dimension(X, h=0.02) # the dimension each subplot based on the data
    # Plotting the dataset and decision boundaries
    i = 0
    for n in n_neighbors:
        model.n_neighbors = n
        model.fit(datasets["Training Set"][0], datasets["Training Set"][1])
        boundary = detect_decision_boundary(dimension, model)
        j = 0
        for d in datasets.keys():
            try:
                panel = axes[j, i]
            except (TypeError, IndexError):
                if (nrows * ncols) == 1:
                    panel = axes
                elif nrows == 1: # if you only have one dataset
                    panel = axes[i]
                elif ncols == 1: # if you only try one number of neighbors
                    panel = axes[j]
            plot_decision_boundary(panel, dimension, boundary) # plot the decision boundary
            plot_dataset(panel, X=datasets[d][0], y=datasets[d][1]) # plot the observations
            score = calculate_prediction_error(model, X=datasets[d][0], y=datasets[d][1])
            # make compacted layout
            panel.set_frame_on(False)
            panel.set_xticks([])
            panel.set_yticks([])
            j += 1
        i += 1
        plt.subplots_adjust(hspace=0, wspace=0) # make compacted layout
    plt.savefig('images/overfitting'+index+'.pdf')


X, y = make_moons(
n_samples=500,
random_state=1,
noise=0.3
)
# Split into training and test sets
XTrain, XTest, yTrain, yTest = train_test_split(X, y, random_state=1, test_size=0.5)

# specify the model and settings
model = KNeighborsClassifier()
datasets = {"Training Set": [XTrain, yTrain]}
width = 20

# explore_fitting_boundaries(model, n_neighbors, datasets, width)
explore_fitting_boundaries(model=model, n_neighbors=[200], datasets=datasets, width=width, index='1')
explore_fitting_boundaries(model=model, n_neighbors=[23], datasets=datasets, width=width, index='2')
explore_fitting_boundaries(model=model, n_neighbors=[1], datasets=datasets, width=width, index='3')
#+end_src

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/overfitting1.pdf}
        \caption{\textit{Underfitting}}
        \label{fig:over1}
    \end{subfigure}
\quad
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/overfitting2.pdf}
        \caption{Aceptable}
        \label{fig:over2}
    \end{subfigure}
\quad
   \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/overfitting3.pdf}
        \caption{\textit{Overfitting}}
        \label{fig:over3}
    \end{subfigure}
    \caption{\fontsize{9}{11}\selectfont Ejemplo de \textit{overfitting} \ref{fig:over1}, \textit{underfitting} \ref{fig:over2} y un buen ajuste al conjunto de datos \ref{fig:over3}.}\label{fig:overfit}
\end{figure}
#+END_LaTeX

Queremos elegir los mejores parámetros de \weights{} para evitar estos
problemas, y eso lo podemos hacer agregando una penalidad de
regularización \(R(W)\). Lo que buscamos con esto es poner
preferencias para algunos conjuntos de \weights{} sobre otros.

De esta manera, nuestra función de costo ahora cuenta con dos
componentes, la función de costo propiamente dicha y la
\textit{componente de regularización}. Sea \(\lambda\) un número real
(\textit{término de regularización}, entonces nuestra nueva función de
costo es:

\begin{equation}
     \boldsymbol{ L =\frac{1}{N} \sum_{i} L_i + \lambda R(W)}
\end{equation}

Notar que la pérdida total es el promedio de las pérdidas de cada
imagen, y que la penalización de la regularización sólo se suma una
vez.

Las técnicas de regularizacion más usadas son:

**** Regularización de norma L2

Para cada peso de la red se calcula \(\frac{1}{2} \lambda \mathbf{w}^{2}_{2}\) donde
\(\lambda\) es la tasa de regularización y se le suma a la función
objetivo. 

Una buena propiedad de la regularizacion es que al penalizar los pesos
grandes, obliga a \weights{} a generalizar y contemplar todas las
clases a la hora de clasificar. De esa manera, nuestro clasificador
final va a tomar en cuenta todas las dimensiones de entrada (algunas
con más o menos probabilidad) sin dar prioridad a una sola.

**** Regularización de norma L1

Similar a la aterior, sólo que se le adiciona \(\lambda \mathbf{w}_{1}\)
a la función objetivo. Los pesos tienden a converger
a cero bajo la regularización L1 y las redes tienden a usar un
subconjunto de los datos de entrada, convirtiendose en invariantes al
ruido. En general se prefiere la regularización L2 por obtenerse
mejores resultados.

**** \textit{Dropout}

La técnica de \textit{dropout} \cite{dropout} consiste en mantener
activa una neurona con una probabilidad \(\boldsymbol{p}\) (a veces
\(\boldsymbol{1 - p}\)). Esta técnica se aplica solamente durante el
\textit{entrenamiento} de las redes neuronales.

Si consideramos una red neuronal con \(L\) capas, sea \(l \in
\{1,\dots,L\}\) el índice de cada capa oculta de la red. Sea
\(\boldsymbol{z}^{(l)}\) el vector de entrada a la capa \(l\),
\(\boldsymbol{y}^{(l)}\) el vector de salidas de la capa \(l\)
(\(\boldsymbol{y}^{(0)} = \boldsymbol{x}\) son los datos de entrada a
la red). \(W^{(l)}\) y \(\boldsymbol{b}^{(l)}\) son los parametros de
la capa \(l\). Dada una neurona \(i\) de la capa \(l\), la operacion
de \textit{feed-forward} (o sea, cuando se procesa una imagen a traves de
todas las capas de la red) de la red puede ser descripta como:

\begin{equation}
z^{(l+1)}_{i} = \boldsymbol{w}^{(l+1)}_{i} \boldsymbol{y}^{l} + b^{(l+1)}_{i},
\end{equation}

\begin{equation}
y^{(l+1)}_{i} = A(z^{(l+1)}_{i})
\end{equation}

Donde \(A\) es una función de activación. Si ahora agregamos \textit{dropout}:

\begin{equation}
r^{(l)}_{j} \sim Bernoulli(p),
\end{equation}

\begin{equation}
\tilde{\boldsymbol{y}}^{(l)} = \boldsymbol{r}^{(l)} \odot \boldsymbol{y}^{(l)},
\end{equation}

\begin{equation}
z^{(l+1)}_{i} = \boldsymbol{w}^{(l+1)}_{i} \tilde{\boldsymbol{y}}^{l} + b^{(l+1)}_{i},
\end{equation}

\begin{equation}
y^{(l+1)}_{i} = A(z^{(l+1)}_{i})
\end{equation}

Aquí \(\odot\) denota el producto elemento a elemento y
\(\boldsymbol{r}^{(l)}\) es un vector de variables aleatorias de
Bernoulli con probabilidad \(p\) de ser \(1\). Para cada capa se
calcula este vector \(\boldsymbol{r}^{(l)}\) y luego se lo multiplica
elemento a elemento por \(\boldsymbol{y}^{(l)}\) para reducir la
cantidad de neuronas activas, obteniendo como resultado
\(\tilde{\boldsymbol{y}}^{(l)}\) que a su vez va a ser la entrada de
la capa siguiente.

En un entrenamiento sin \textit{dropout}, la red actualiza todas sus
neuronas en cada iteración (Figura \ref{fig:dropout}-izquierda),
mientras que utilizando \textit{dropout} se eliminan neuronas
aleatoriamente y se utiliza una \textit{subred} de la original, lo
cual impide a las neuronas co-adaptarse entre si (Figura
\ref{fig:dropout}-derecha). La co-adaptación ocurre cuando dos o más
neuronas consecutivas dependen mucho entre ellas para detectar
\textit{features}, en vez de que cada neurona busque un tipo
particular de \textit{feature}.

Otra manera de pensarlo es la siguiente: una red neuronal con \(n\)
neuronas puede ser vista como una colección de \(2^n\)
\textit{subredes} que comparten todas los mismos pesos (\weights{}),
por lo cual la cantidad total de parámetros sigue siendo a lo sumo
\(O(n^2)\). Para cada elemento en el conjunto de entrenamiento se
elige una de estas \(2^n\) redes y apenas se la entrena. Esto es
comparable a entrenar distintos modelos y luego promediar sus
predicciones, algo que en general es muy útil en aprendizaje
automático pero rara vez se hace en aprendizaje profundo debido a que
cada modelo tarda mucho en entrenarse y es muy tedioso elegir buenos
hiperparámetros.

#+attr_latex: width=0.8\textwidth,placement=[p]
#+label: fig:dropout
#+caption: A la izquierda, una red neuronal normal; a la derecha, una red neuronal luego de aplicar \textit{dropout}.
[[file:images/dropout.pdf]]

*** Organización de las redes neuronales

Las redes neuronales estan organizadas como un grafo acíclico de
neuronas, donde las salidas de unas se transforma en la entrada de
otras. Las neuronas se organizan en distintas capas conectadas, de esa
manera los cálculos se hacen con operaciones entre matrices, algo que
no podríamos hacer tan fácil si tuvieramos neuronas conectadas
aleatoriamente entre ellas.

El tipo más común de capa de neuronas es la capa \textit{totalmente conectada} 
(de ahora en más FC, abreviación de su nombre en inglés
\textit{Fully Connected}), en donde cada neurona de la capa anterior
se conecta con todas las neuronas de la capa siguiente, pero no
comparten conexiones dentro de la misma capa.

Usualmente no se cuenta a la capa de entrada de las redes como una
capa más, y la capa de salida no tiene funciones de activación, dado
que generalmente representan las puntuaciones de cada clase (en
clasificación) o alguna métrica (en regresión). Las redes neuronales
suelen tener una o más capas intermedias entre la entrada y la salida,
denominadas \textit{capas ocultas} (Figura \ref{fig:nnet}).

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{images/neural-net.pdf}
    \caption{\fontsize{9}{11}\selectfont Diagrama de una red neuronal con entrada de dimensión \(D=3\), una capa oculta completamente conectada y dos neuronas de salida.}\label{fig:nnet}
\end{figure}
#+END_LaTeX

Las redes neuronales se entrenan partiendo del principio del descenso
del gradiente que se explicó en la Sección \ref{sec:sgd}. Notemos que
\loss{} es una función que depende de las imágenes de entrada \img{},
\weights{} y \bias{}. Sin embargo, como ya dijimos, el conjunto de
datos de entrenamiento es algo fijo en nuestro modelo, por lo que sólo
nos interesa calcular el gradiente sobre \weights{} y \bias{} para
poder actualizar sus parámetros. Ahora bien, derivar una función con
millones de parámetros (cantidad que suelen tener las redes neuronales
artificiales) es computacionalmente costoso, por lo que para
actualizar \weights{} con los nuevos pesos se utiliza el algoritmo
\textit{retropropagación} (\back{} en inglés).

*** Entrenamiento: \textit{Backpropagation}
#+LaTeX: \label{sec:backprop}

La retropropagación (o \textit{backpropagation} en inglés) es un
algoritmo utilizado comúnmente con la técnica de optimización de
descenso de gradiente.

Para lograr una intuición del mismo empecemos por recordar la
definición de derivada de una función. Sea \(g:\mathbb{R} \to
\mathbb{R}\), entonces su derivada se expresa como:

\begin{equation}
     \boldsymbol{\frac{dg(x)}{dx} = \lim_{h \to 0} \frac{g(x + h) - g(x)}{h} }
\end{equation}

Si el dominio de \(g\) estuviera en \(\mathbb{R}^n\), se calculan
derivadas parciales y llamamos \textit{gradiente} es simplemente un
vector conteniendo derivadas. Por ejemplo, sea \(n=2\) y por lo tanto
\(g\) una función que toma dos parámetros \(x\) e \(y\), entonces el
gradiente de \(g\) es \(\nabla g = [\frac{\partial g}{\partial x},
\frac{\partial g}{\partial y}]\)

Usualmente podemos derivar con métodos numéricos, pero es lento y sólo
aproxima los resultados. Veremos más adelante que la función \loss{}
de las redes neuronales suele tener decenas de millones de parámetros,
y realizar tantas operaciones para una sola actualización de
\weights{} no es conveniente. En la práctica usaremos el cálculo
analítico del gradiente, en el cual derivamos una fórmula directa que
es muy rápida de computar valiéndonos de la \textit{regla de la
cadena}.

La \textit{regla de la cadena} nos ayuda a descomponer el cálculo del
gradiente de expresiones complejas en pequeños pasos. Tomemos por
ejemplo una función \(g:\mathbb{R}^3 \to \mathbb{R}\):

\begin{equation}
    g(x,y,z) = \frac{x}{y^2} + z
\end{equation}

Si quisiéramos obtener su gradiente en \(x\) de la manera tradicional,
calculando el cociente de \(g(x+h) - g(x)\) con \(h\) cuando \({h \to
0}\) deberíamos realizar muchos cálculos computacionalmente
costosos. En cambio, podemos ver a \(g\) como una composición de
funciones:

\begin{equation}
    g(x,y,z) = \frac{x}{y^2} + z = q + z
\end{equation}

Y calcular su gradiente valiéndonos de la \textit{regla de la cadena}:

\begin{equation}
\frac{\partial g}{\partial z} = 1
\end{equation}

\begin{equation}
\frac{\partial g}{\partial q} = 1
\end{equation}

\begin{equation}
\frac{\partial g}{\partial x} = \frac{\partial g}{\partial q} \frac{\partial q}{\partial x} = \frac{1}{y^2}
\end{equation}

\begin{equation}
\frac{\partial g}{\partial y} = \frac{\partial g}{\partial q} \frac{\partial q}{\partial y} = \frac{-2x}{y^3}
\end{equation}

Ahora podemos comenzar a estructurar nuestro algoritmo de optimización
en dos pasos: primero, evaluamos nuestra función \loss{} en los
parámetros actuales (\textit{forward pass}). Luego, partiendo de ese
resultado calculamos el gradiente en cada variable utilizando la
\textit{regla de la cadena}. De esta manera ``propagamos'' el error de la
predicción hacia atrás (\back{}) y corregimos
ligeramente los pesos para mejorar las futuras predicciones.

Una vez que contamos con el gradiente, actualizamos los parámetros de
\loss{} restándole un porcentaje del gradiente negativo calculado
(negativo porque queremos ir en dirección opuesta a donde crece la
función, o sea, ir a su mínimo). Ese porcentaje es llamado
\textit{tasa de aprendizaje} (\textit{learning rate}) y suele ser uno
de los parámetros más difíciles de elegir, ya que la calidad y rapidez
de aprendizaje dependen de él.

Idealmente computaríamos el gradiente sobre todo el conjunto de datos,
actualizaríamos los parámetros, y repetiríamos el ciclo hasta lograr
la convergencia. Sin embargo los conjuntos de datos para entrenar las
redes neuronales suelen tener cientos de miles o incluso millones de
imágenes, por lo cual se utiliza una técnica llamada 
\textit{Descenso de Gradiente Estocástico con mini-batches} o simplemente \textit{SGD}
por sus siglas en inglés, en el cual se calcula el gradiente para una
cantidad predeterminada de imágenes (\textit{mini-batches}), se
actualizan los parámetros y se vuelve a repetir el ciclo con otro
subconjunto distinto. Esto parte de la suposición de que todas las
imágenes del conjunto de datos estan correlacionadas entre sí. El
tamaño de los \textit{mini-batches} no es estrictamente un
hiperparámetro que uno pueda validar durante el entrenamiento, sino
que más bien depende del hardware sobre el que se esté entrenando la
red (en general se eligen potencias de dos por cuestiones de
eficiencia). No obstante puede haber casos en los que se elija un
tamaño de \textit{mini-batch} chico para tener más varianza en los
datos de entrada de la red y evitar que la se caiga en un mínimo
local.

**** Transferencia de aprendizaje

Entrenar un modelo con un tipo específico de problema y luego utilizar
su \textit{conocimiento} para resolver otro problema nuevo, tal vez
incluso en un área distinta a la que fue pensado originalmente, es lo
que se llama transferencia de aprendizaje. Esta técnica ha cobrado
importancia en \textit{deep learning} dado que a menudo las
arquitecturas son muy complejas y se tardan semanas en lograr la
convergencia deseada, por lo que contar con modelos preentrenados
sobre los cuales se puedan ajustar ligeramente los parámetros para
resolver un nuevo problema es una ventaja. La mayoría de los
\textit{frameworks} modernos para implementar redes neuronales
soportan realizar transferencia de aprendizaje utilizando modelos
pre-entrenados.

** Redes Neuronales Convolucionales

Podemos representar a una imagen como una función \(f:\mathbb{R}^2 \to \mathbb{R}^n\) 
con \(n\) igual número de canales que pueda tener (usualmente \(1\) ó
\(3\)). Es decir, toma un punto en coordenadas cartesianas y devuelve la
intensidad en ese punto.

El caso general de un \textit{operador} de procesamiento de imágenes es el de
una función que toma una o más imágenes de entrada y produce una
imagen de salida.  Para el caso discreto en el que el dominio consiste
de un número finito de píxeles donde representamos a cada píxel como
su posición en la imagen, \(\boldsymbol{x} = (i, j)\), podemos
expresar a un operador de píxeles como:

\begin{equation}
g(i,j) = h(f(i,j))
\end{equation}

Un operador muy utilizado en la visión por computadoras es el
\textit{filtro lineal}. El mismo es un tipo de \textit{operador local}
dado que usa el conjuto de píxeles en la vecindad de uno para
determinar su nuevo valor. En un filtro lineal cada píxel de salida se
determina como la suma ponderada de los valores de entrada:

\begin{equation}\label{eq:corr}
g(i,j) = \sum_{k,l} f(i+k,j+l)h(k,l)
\end{equation}

Donde las entradas en el \textit{kernel} o \textit{máscara} de pesos
\(h(k,l)\) son los \textit{coeficientes del filtro}. El operador en la
ecuación Ecuación \ref{eq:corr} se denonima \textit{correlación}, y
puede ser escrito como:

\begin{equation}
g = f \bigotimes h
\end{equation}

Una variante de la Ecuación \ref{eq:corr} es invirtiendo el signo de
los \textit{offsets}, y se denomina \textit{convolución}:

\begin{equation}\label{eq:conv}
g(i,j) = \sum_{k,l} f(i-k,j-l)h(k,l) = \sum_{k,l} f(k,l)h(i-k,j-l)
\end{equation}

Que también puede ser escrito como:

\begin{equation}
g = f * h
\end{equation}

En la Figura \ref{fig:convmatrix} se puede observar un ejemplo sobre
el uso de este operador en una imagen de un solo canal donde la
intensidad de cada píxel está representada con un número.


#+BEGIN_LaTeX
\begin{figure}
\centering
\begin{tikzpicture}[every node/.style={minimum size=.5cm-\pgflinewidth, outer sep=0pt}]
    \colorlet{gre}{green!40}
    \colorlet{blu}{blue!40}
    \node[fill=gre] at (-0.25,2.25)  {};
    \node[fill=gre] at (-0.25,1.75)  {};
    \node[fill=gre] at (-0.25,1.25)  {};
    \node[fill=gre] at (0.25,2.25)  {};
    \node[fill=gre] at (0.25,1.75)  {};
    \node[fill=gre] at (0.25,1.25)  {};
    \node[fill=gre] at (0.75,2.25)  {};
    \node[fill=gre] at (0.75,1.75)  {};
    \node[fill=gre] at (0.75,1.25)  {};
    % result fill
    \node[fill=blu] at (7.25, 2.25) {};
    \draw[step=0.5cm,color=black] (-1,-1) grid (3, 3);
    % first row
    \node at (-0.75,2.75) {\tiny 45};
    \node at (-0.25,2.75) {\tiny 60};
    \node at (0.25,2.75) {\tiny 98};
    \node at (0.75,2.75) {\tiny 127};
    \node at (1.25,2.75) {\tiny 132};
    \node at (1.75,2.75) {\tiny 133};
    \node at (2.25,2.75) {\tiny 137};
    \node at (2.75,2.75) {\tiny 133};
    % second row
    \node at (-0.75,2.25) {\tiny 46};
    \node at (-0.25,2.25) {\tiny 65};
    \node at (0.25,2.25) {\tiny 98};
    \node at (0.75,2.25) {\tiny 123};
    \node at (1.25,2.25) {\tiny 126};
    \node at (1.75,2.25) {\tiny 128};
    \node at (2.25,2.25) {\tiny 131};
    \node at (2.75,2.25) {\tiny 133};
    % third row
    \node at (-0.75,1.75) {\tiny 47};
    \node at (-0.25,1.75) {\tiny 65};
    \node at (0.25,1.75) {\tiny 96};
    \node at (0.75,1.75) {\tiny 115};
    \node at (1.25,1.75) {\tiny 119};
    \node at (1.75,1.75) {\tiny 123};
    \node at (2.25,1.75) {\tiny 135};
    \node at (2.75,1.75) {\tiny 137};
    % fourth row
    \node at (-0.75,1.25) {\tiny 47};
    \node at (-0.25,1.25) {\tiny 63};
    \node at (0.25,1.25) {\tiny 91};
    \node at (0.75,1.25) {\tiny 107};
    \node at (1.25,1.25) {\tiny 113};
    \node at (1.75,1.25) {\tiny 122};
    \node at (2.25,1.25) {\tiny 138};
    \node at (2.75,1.25) {\tiny 134};
    % fifth row
    \node at (-0.75,0.75) {\tiny 50};
    \node at (-0.25,0.75) {\tiny 59};
    \node at (0.25,0.75) {\tiny 80};
    \node at (0.75,0.75) {\tiny 97};
    \node at (1.25,0.75) {\tiny 110};
    \node at (1.75,0.75) {\tiny 123};
    \node at (2.25,0.75) {\tiny 133};
    \node at (2.75,0.75) {\tiny 134};
    % sixth row
    \node at (-0.75,0.25) {\tiny 49};
    \node at (-0.25,0.25) {\tiny 53};
    \node at (0.25,0.25) {\tiny 68};
    \node at (0.75,0.25) {\tiny 83};
    \node at (1.25,0.25) {\tiny 97};
    \node at (1.75,0.25) {\tiny 113};
    \node at (2.25,0.25) {\tiny 128};
    \node at (2.75,0.25) {\tiny 133};
    % seventh row
    \node at (-0.75,-0.25) {\tiny 50};
    \node at (-0.25,-0.25) {\tiny 50};
    \node at (0.25,-0.25) {\tiny 58};
    \node at (0.75,-0.25) {\tiny 70};
    \node at (1.25,-0.25) {\tiny 84};
    \node at (1.75,-0.25) {\tiny 102};
    \node at (2.25,-0.25) {\tiny 116};
    \node at (2.75,-0.25) {\tiny 126};
    % eight row
    \node at (-0.75,-0.75) {\tiny 50};
    \node at (-0.25,-0.75) {\tiny 50};
    \node at (0.25,-0.75) {\tiny 52};
    \node at (0.75,-0.75) {\tiny 58};
    \node at (1.25,-0.75) {\tiny 69};
    \node at (1.75,-0.75) {\tiny 86};
    \node at (2.25,-0.75) {\tiny 101};
    \node at (2.75,-0.75) {\tiny 120};
    % Convolution operator
    \node[text width=6cm, anchor=west, right] at (3.29,1.25) {$*$};
    % Filter (end right, end down)grid(start left, start up)
    \draw[step=0.5cm,color=black] (5.50,0.5) grid (3.994,2);
    % first row
    \node at (4.25, 1.75) {\tiny 0.1};
    \node at (4.75, 1.75) {\tiny 0.1};
    \node at (5.25, 1.75) {\tiny 0.1};
    % second row
    \node at (4.25, 1.25) {\tiny 0.1};
    \node at (4.75, 1.25) {\tiny 0.2};
    \node at (5.25, 1.25) {\tiny 0.1};
    % third row
    \node at (4.25, 0.75) {\tiny 0.1};
    \node at (4.75, 0.75) {\tiny 0.1};
    \node at (5.25, 0.75) {\tiny 0.1};
    % Equal
    \node[text width=6cm, anchor=west, right] at (5.75,1.25) {$=$};
    % Result
    \draw[step=0.5cm,color=black] (9.50,0) grid (6.4999,3);
    % first row
    \node at (6.75, 2.75) {\tiny 69};
    \node at (7.25, 2.75) {\tiny 95};
    \node at (7.75, 2.75) {\tiny 116};
    \node at (8.25, 2.75) {\tiny 125};
    \node at (8.75, 2.75) {\tiny 129};
    \node at (9.25, 2.75) {\tiny 132};
    % second row
    \node at (6.75, 2.25) {\tiny 68};
    \node at (7.25, 2.25) {\tiny 92};
    \node at (7.75, 2.25) {\tiny 110};
    \node at (8.25, 2.25) {\tiny 120};
    \node at (8.75, 2.25) {\tiny 126};
    \node at (9.25, 2.25) {\tiny 132};
    % third row
    \node at (6.75, 1.75) {\tiny 66};
    \node at (7.25, 1.75) {\tiny 86};
    \node at (7.75, 1.75) {\tiny 104};
    \node at (8.25, 1.75) {\tiny 114};
    \node at (8.75, 1.75) {\tiny 124};
    \node at (9.25, 1.75) {\tiny 132};
    % fourth row
    \node at (6.75, 1.25) {\tiny 62};
    \node at (7.25, 1.25) {\tiny 78};
    \node at (7.75, 1.25) {\tiny 94};
    \node at (8.25, 1.25) {\tiny 108};
    \node at (8.75, 1.25) {\tiny 120};
    \node at (9.25, 1.25) {\tiny 129};
    % fifth row
    \node at (6.75, 0.75) {\tiny 57};
    \node at (7.25, 0.75) {\tiny 69};
    \node at (7.75, 0.75) {\tiny 83};
    \node at (8.25, 0.75) {\tiny 98};
    \node at (8.75, 0.75) {\tiny 112};
    \node at (9.25, 0.75) {\tiny 124};
    % sixth row
    \node at (6.75, 0.25) {\tiny 53};
    \node at (7.25, 0.25) {\tiny 60};
    \node at (7.75, 0.25) {\tiny 71};
    \node at (8.25, 0.25) {\tiny 85};
    \node at (8.75, 0.25) {\tiny 100};
    \node at (9.25, 0.25) {\tiny 114};
    % Captions
    \node[text width=6cm, anchor=west, right] at (0.25,-1.30) {\scriptsize $f(i,j)$};
    \node[text width=6cm, anchor=west, right] at (4.30,-1.30) {\scriptsize $h(i,j)$};
    \node[text width=6cm, anchor=west, right] at (7.50,-1.30) {\scriptsize $g(i,j)$};
\end{tikzpicture}
\caption{\fontsize{9}{11}\selectfont Convolución. La imagen en la izquierda se convoluciona con el filtro $h$ para dar lugar a la imagen de la derecha. El píxel marcado en azul es el resultado de la combinación lineal de $h$ con el \textit{parche} verde en la imagen original.}\label{fig:convmatrix}
\end{figure}
#+END_LaTeX

Dado que la convolución en el caso discreto es una combinación lineal,
su resultado para cada elemento \(f(i,j) \in \mathbb{R}^n\) es un
escalar en \(\mathbb{R}\). Notar que la Ecuación \ref{eq:conv} puede
ser generalizada a más dimensiones, por lo que el mismo concepto
aplica también a imágenes con \(n\) canales.


Si convolucionamos todos los píxeles de la imagen de entrada
obtendremos una nueva imagen de un canal. Para aplicar la convolución
a la totalidad de una imagen se deben tener en cuenta ciertas
configuraciones espaciales. Una de ellas es el \textit{stride}, o
cantidad de \textit{pasos} cada cuales convolucionar. Cuando se aplica
el filtro convolucional a cada píxel de la imagen, se tiene un
\textit{stride} igual a 1, pero también podría elegirse aplicar el
filtro cada dos o más píxeles. Además, al aplicar el filtro
convolucional a lo largo y alto de la imagen surgen casos en los que
el filtro se aplica a píxeles fuera de los bordes de la misma, por lo
que se agrega un borde artificial mediante la técnicas de
\textit{padding}.

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/conv-orig.png}
        \caption{\(A\)}
        \label{fig:conv-orig}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \(\begin{bmatrix}
         0  &  1 & 0 \\
         1  & -4 & 1 \\
         0  &  1 & 0 \\
        \end{bmatrix}\)
        \caption{\textit{Kernel} \(K\)}
        \label{fig:K}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{images/conv-edge.png}
        \caption{\(A * K\)}
        \label{fig:conv-edge}
    \end{subfigure}
    \caption{\fontsize{9}{11}\selectfont Es muy común convolucionar imágenes con ciertos \textit{kernels} para detectar bordes en las mismas.}\label{fig:conv}
\end{figure}
#+END_LaTeX

Una buena propiedad de la convolución es que toma ventaja de la
\textit{localidad} (vecinos) de cada píxel, que usualmente son los más
relacionados, y produce una salida \textit{comprimiendo} esa
información local en un escalar. De esta manera se puede utilizar una
convolución para extraer \textit{features} de una imagen valiéndose de
su estructura espacial. Otra buena propiedad es el principio de
\textit{invarianza traslacional}, que básicamente significa que la
respuesta del operador no depende explícitamente del lugar en la
imagen donde se aplique.

Las redes convolucionales cuentan con los mismos artefactos que las
redes convencionales que ya discutimos (neuronas con pesos, funciones
de pérdida, capas completamente conectadas). Incluso los mismos
métodos de entrenamiento pueden ser aplicados. La diferencia radica en
que las redes convolucionales asumen que están trabajando con
imágenes, lo que permite optimizar la arquitectura de las mismas,
reduciendo parámetros y mejorando el proceso de aprendizaje.

Imaginemos por un momento que quisieramos aprender a clasificar un
conjunto de imágenes de 200x200 píxeles con 3 canales de colores. Eso
nos da una dimensión de entrada de 200x200x3, por lo que una neurona
completamente conectada en la primer capa oculta tendría 120000
conexiones y por ende esa misma cantidad de pesos a entrenar. Si
tenemos en cuenta que seguramente vamos a requerir más de una neurona
(comúnmente cientos de ellas en una misma capa) podemos concluir que
este enfoque no escala bien para el procesamiento de imágenes.

Una red neuronal convolucional utiliza capas con filtros
convolucionales. Además de las ventajas de invarianza traslacional y
localidad antes mencionadas, un filtro convolucional utiliza pocos
parámetros.

Tal como hicimos con las redes neuronales convencionales, en la
siguiente sección analizaremos los tipos de capas de una red neuronal
convolucional. Al final de la Sección se dará una descripción general
de la arquitectura de una red convolucional y varios ejemplos de redes
convolucionales conocidas.

*** Capas de una red convolucional
**** Capas de Entrada y Salida

Las redes convolucionales manipulan arreglos tridimensionales. Eso
significa que para cada capa hay un volumen de entrada y un volumen de
salida.

La capa de entrada de una red es la que provee la imagen original como
un volumen de pixeles. Notar que si una imagen tiene un alto y ancho
de 256 pixeles y tres canales de colores (RGB) entonces el volumen de
salida de esta capa va a ser \(256 \times 256 \times 3\).

En general la capa de salida de una red suele ser una capa
completamente conectada con los puntajes de cada clase en el caso de
tareas de clasificacion o con un vector de numeros reales en el caso
de tareas de regresion. Para clasificación también es muy común
utilizar un clasificador (por ejemplo, \textit{Softmax}) que
transforme los puntajes obtenidos en probabilidades normalizadas.

**** Capas Convolucionales

Una capa convolucional consta de un conjunto de filtros (o
\textit{kernels}) cuyos parámetros se pueden aprender. En general cada
filtro es pequeño a lo ancho y alto, pero se aplica a toda la
profundidad del volumen de entrada (ej.: los tres canales RGB). Notar
que el volumen de entrada puede bien ser una imagen o bien el volumen
de salida con las activaciones de otra capa.

Como se puede observar en la Figura \ref{fig:conv-lay} estos filtros
se convolucionan a traves del ancho y alto del volumen de entrada,
produciendo un mapa de activaciones en 2-D para cada filtro. Si
``apilamos'' los mapas de activaciones de todos los filtros de una
capa convolucional, obtenemos un \textit{volumen} de salida. De esta
manera cada elemento en el volumen de salida puede ser interpretado
como la salida de una neurona conectada a una pequeña region de los
datos de entrada, la cual comparte parámetros (pesos) con las otras
neuronas que corresponden al mismo filtro.

Esta conexión a una pequeña región en los datos de entrada es
denominada \textit{campo receptivo}. Es importante notar que los
\textit{campos receptivos} son locales en una pequeña área en cuanto
al ancho y alto de la entrada, pero abarcan la totalidad de la
profundidad del volumen de entrada.

Otros hiperparámetros relacionados con las capas convolucionales son
la cantidad de filtros (\textit{K}), el espacio en píxeles entre cada
aplicacion de los filtros (\textit{stride}) y por ultimo el relleno
con ceros o \textit{zero-padding}, donde se le agrega un ``marco'' de
1 o más ceros a la entrada de la capa.

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/conv-layer.pdf}
    \caption{Ejemplo de una capa convolucional cuyo volumen de entrada es una imagen (\(x\)) con 3 canales RGB. La capa cuenta con 6 filtros y cada uno convoluciona con la imagen a lo largo y ancho.}\label{fig:conv-lay}
\end{figure}
#+END_LaTeX

**** \textit{Pooling}

Con el objetivo de reducir la cantidad de parámetros y en consecuencia
las dimensiones de las representaciones obtenidas de la imagen de
entrada, se suelen intercalar capas \textit{pooling} en reducen la
dimensión espacial de sus entradas.

Reducir la cantidad de parámetros no sólo favorece la rapidez del
entrenamiento sino también ayuda a controlar el
\textit{overfitting}. Lo más común es insertar capas de
\textit{pooling} luego de capas convolucionales.

Un método de \textit{pooling} muy usado es \textit{MAX Pooling}, en el
cual se calcula el máximo de un área local (generalmente \(2 \times 2\) ó
 \(3 \times 3\)) en el ancho y largo del volumen de entrada y a través
de cada una de las ``rodajas'' que conforman la profundidad del
volumen, como se puede ver en la Figura \ref{fig:pool2}. El área local
está definida por el tamaño del \textit{stride}. De esta manera se
reduce espacialmente la entrada, pero no su profundidad.

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{images/pool-layer.pdf}
        \caption{}
        \label{fig:pool1}
    \end{subfigure}

    \begin{subfigure}[b]{0.5\textwidth}
        \centering

        \colorlet{yel}{yellow!40}
        \colorlet{re}{red!40}
        \colorlet{gre}{green!40}
        \colorlet{blu}{blue!40}

        \begin{tikzpicture}[every node/.style={minimum size=.5cm-\pgflinewidth, outer sep=0pt}]
        \draw[step=0.5cm,color=gray] (-1,-1) grid (1,1);
        \node[fill=yel] at (-0.75,+0.75) {1};
        \node[fill=yel] at (-0.75,+0.25) {2};
        \node[fill=yel] at (-0.25,+0.75) {3};
        \node[fill=yel] at (-0.25,+0.25) {4};
        \node[fill=re] at (-0.75,-0.25) {8};
        \node[fill=re] at (-0.75,-0.75) {7};
        \node[fill=re] at (-0.25,-0.25) {5};
        \node[fill=re] at (-0.25,-0.75) {4};
        \node[fill=gre] at (+0.75,-0.25) {2};
        \node[fill=gre] at (+0.75,-0.75) {3};
        \node[fill=gre] at (+0.25,-0.25) {2};
        \node[fill=gre] at (+0.25,-0.75) {1};
        \node[fill=blu] at (+0.75,+0.25) {0};
        \node[fill=blu] at (+0.75,+0.75) {7};
        \node[fill=blu] at (+0.25,+0.25) {4};
        \node[fill=blu] at (+0.25,+0.75) {3};
        \draw[black, -latex ] (1.1,0) -- (3.9,0) node [pos=0.46,above,font=\footnotesize] {\textit{MAX Pooling}};
        \draw[step=0.5cm,color=gray] (4,4) grid (4,4);
        \node[fill=yel] at (+4.25,+0.25) {4};
        \node[fill=blu] at (+4.75,+0.25) {7};
        \node[fill=gre] at (+4.75,-0.25) {3};
        \node[fill=re] at (+4.25,-0.25) {8};
        \end{tikzpicture}

        \caption{}
        \label{fig:pool2}
    \end{subfigure}
    \caption{Las capas de \textit{Pooling} reducen las dimensiones espaciales. Notar en \ref{fig:pool1} que la profundidad del volumen se mantiene intacta. En \ref{fig:pool2} observamos cómo funciona una capa \textit{MAX Pooling} con un un \textit{stride} de 2.}\label{fig:pool}
\end{figure}
#+END_LaTeX

**** Capas Completamente Conectadas (Fully-Connected)

Como su nombre lo indica, cada neurona de esta capa tiene conexiones a
todas las salidas (o activaciones) de la capa anterior. Por lo tanto
sus activaciones se pueden calcular con una multiplicacion de matrices
junto con el calculo del \textit{bias}, como ya se vio para las redes
neuronales convencionales.

*** Arquitecturas conocidas de redes convolucionales 

Normalmente una red convolucional esta compuesta de varias capas
convolucionales (CONV), capas de \textit{pooling} (POOL), capas
completamente conectadas (FC por sus siglas en ingles) y funciones de
activacion, generalmente rectificadores lineales (RELU).

El patrón usual en redes convolucionales es generar \textit{bloques}
con una o más capas CONV seguidas de capas RELU seguidas de capas de
\textit{pooling}. Luego es común utilizar capas FC hasta reducir las
dimensiones a las dimensiones de salida, que en el caso de
clasificacion son las probabilidades de cada clase.

A lo largo de los años ha habido varias arquitecturas de redes
convolucionales que cuentan con nombre propio, como por ejemplo LeNet
\cite{Lecun98gradient-basedlearning}, creada en los 90 por Yann LeCun
y utilizada para el reconocimiento de dígitos manuscriptos. Esta red
fue utilizada con éxito para leer códigos postales y cheques
bancarios.

En 2012, el ganador del desafío ImageNet ILSVRC, Alex Krizhevsky,
obtuvo un 16% de error utilizando una arquitectura llamada AlexNet
\cite{NIPS2012_4824}. Su arquitectura es muy similar a la de LeNet,
aunque mas profunda y fue una de las primeras en concatenar varias
capas CONV antes de una capa de \textit{pooling}. El desafío ImageNet
consiste en varias tareas de procesamiento de imágenes, entre ellas la
clasificación de 1000 clases distintas de objetos.

Los ganadores del ISLVRC 2013 utilizaron una red llamada ZFNet
\cite{DBLP:journals/corr/ZeilerF13} con un error del 11.2% en ImageNet. Su
propuesta era básicamente una modificación en los hiperparámetros y
capas convolucionales de AlexNet.

En la misma competencia ILSVRC del 2014 se dieron a conocer dos redes,
GoogLeNet \cite{43022} con un error del 6.67% y VGGNet
\cite{Simonyan14c} con un error del 7.32%. Ambas demostraron que la
profundidad de la red es una característica crítica a la hora de
obtener buenos resultados. Si bien GoogLeNet fue la ganadora ese año,
luego se demostró que VGGNet es superior en muchas tareas de
transferencia de aprendizaje, por lo que es más popular que GoogLeNet
y se pueden encontrar muchos modelos ya preentrenados.

Finalmente, ResNet (Residual Network) \cite{he15deepresidual}, la red
ganadora del ILSRVC 2015, cuenta con nada menos que 152 capas (VGGNet
cuenta con 19) y obteniendo un error de 3.57% en el top-5. A finales
de 2016 se publicó una nueva variante de ResNet denominada DenseNet
\cite{densenet} que incluye conexiones entre todas las capas de la
misma, obteniendo resultados de estado del arte pero consumiendo menos
memoria y tiempo de cómputo.

#+LaTeX: \newpage

* Redes neuronales convolucionales siamesas
#+LaTeX: \label{sec:siamesa}

Los métodos discriminativos tradicionales generalmente requieren
conocer las categorías del conjunto de datos de antemano y suelen
estar limitados a un número reducido de categorías (en el orden de las
100). Eso se convierte en una limitación para ciertas tareas donde la
cantidad de clases es muy grande, como la verificación de rostros o
motores de búsqueda visuales, donde se cuenta con pocos elementos por
clase o sólo algunas clases son conocidas durante el
entrenamiento. Generalizar bien para todas las clases se convierte
entonces en una tarea muy cara (es necesario contar con una cantidad
aceptable de ejemplos por clase) o prácticamente imposible.

Se puede abordar el problema aprendiendo mejores representaciones
según el dominio en el que se esté trabajando. Dentro del dominio de
las redes neuronales, una manera de lograrlo es mediante un tipo
particular de arquitectura denominado \textit{redes siamesas}. Una red
siamesa consiste de dos redes \textit{gemelas} cuyos parámetros son
compartidos. La arquitectura toma pares de imágenes (\(x_1\) y
\(x_2\)) y las representaciones obtenidas de ambas redes son redireccionadas a una
función de energía que calcula alguna métrica respecto a las dos
entradas (Figura \ref{fig:siamese-diagram}). Las redes siamesas no
difieren en más que eso de las redes convolucionales que ya vimos.

Las redes siamesas fueron utilizadas por primera vez con éxito para la
verificación de huellas digitales \cite{fingerprint} y de firmas
\cite{Bromley94signatureverification}. Se han logrado también buenos
resultados en verificación de rostros
\cite{Chopra:2005:LSM:1068507.1068961} y más recientemente en tareas
de clasificación \cite{siamese-oneshot}.

Estos trabajos tienen tres factores en común:

\begin{enumerate}
\item La función de pérdida se calcula mediante alguna medida de
similaridad entre las dos imágenes de entrada. Ya sea una probabilidad
correspondiente a la igualdad de las imágenes (como en \cite{fingerprint}),
una distancia cosenoidal calculada entre las \textit{features} obtenidas
de cada red (como en \cite{Bromley94signatureverification}) o una función
de energía contrastiva \cite{Chopra:2005:LSM:1068507.1068961}
\cite{siamese-oneshot}.

\item Se tiene acceso a pocos datos de entrenamiento. Para la verificación
de huellas dactilares, firmas y rostros se utilizaron 200, 1400 y 1100
muestras de entrenamiento respectivamente.

\item Se generan artificialmente pares de imágenes que luego se usarán durante
el entrenamiento. Esto es muy similar a la aumentación de datos, ya que
se valen de rotaciones, traslaciones, etc. A estos pares se les
asigna una o más etiquetas indicando el nivel de similitud entre las imágenes.
\end{enumerate}

#+BEGIN_LaTeX
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{images/siamese-diagram.pdf}
    \caption{\fontsize{9}{11}\selectfont Esquema de la arquitectura de una red siamesa. \(G_W\) es un conjunto de funciones, en este caso una red convolucional, con parámetros \(W\). Ambas redes comparten los parámetros. Luego una función de pérdida \(L\) analiza la similitud entre \(X_1\) y \(X_2\)}\label{fig:siamese-diagram}
\end{figure}
#+END_LaTeX

Es importante saber definir qué función de costo vamos a usar y cómo
vamos a generar los pares de entrenamiento antes de entrenar redes
siamesas. Las opciones dependen del dominio del problema naturalmente,
aunque para funciones de costo suelen elegirse ser alguna medida de
similitud entre las representaciones aprendidas por la red y los pares
se arman con ejemplos negativos/positivos (es decir, de imágenes no
similares/similares). Una manera de probar que las representaciones
obtenidas por la red siamesa pueden generalizar bien es realizar
transferencia de aprendizaje de los parámetros aprendidos a un nuevo
problema (clasificación, búsqueda de imágenes, detección, etc).

Si bien mencionamos que en las redes siamesas se suelen comparar las
representaciones generadas utilizando métricas de similitud, también
podemos definir nuestra función de costo de diferente manera. Se
podría, por ejemplo, intentar predecir cuando dos parches pertenecen o
no a la misma imagen, o cuando dos objetos distintos pertenecen a la
misma clase, o cuando dos fotos son la misma a pesar haber sufrido
alguna transformación (traslacion, rotación, deformación, etc).
Agrawal et al. \cite{LSM2015} proponen que para el último caso, las
representaciones que se obtienen de entrenar la red siamesa son casi
tan buenas como en el estado del arte. La información disponible para
la red relacionada a la posición relativa de una foto y sus
transformaciones (traslación, etc) respecto al \textit{tiempo} se
denomina \textit{información odométrica}.

En la siguiente Sección vamos a evaluar diferentes métodos de
entrenamiento mediante información odométrica y como pueden
generalizar las representaciones a nuevos dominios, basándonos y
evaluando algunas propuestas de Agrawal et al. \cite{LSM2015}.

** Entrenamiento de modelos de aprendizaje profundo utilizando información odométrica
#+LaTeX: \label{sec:odometry}

Muchos agentes móviles son conscientes de sus movimientos
(\textit{automovimiento}) gracias a sus sistemas motores. En otras
palabras, esa información está disponible y es accesible
fácilmente. Ya sea un mamífero con un sistema de equilibrio como los
humanos o un robot con giróscopos y otros sensores de movimiento, es
posible acceder a la información visual y espacial para determinar en
dónde se encuentra uno en el espacio.

Agrawal et al. \cite{LSM2015} proponen que se pueden aprender
representaciones visuales útiles analizando la correlación entre los
estímulos visuales y el automovimiento. De esta manera, un agente
móvil puede ser tratado como una cámara moviéndose, y su información
de automovimiento se corresponde con los movimientos de la
cámara. Establecen entonces el problema de relacionar los estímulos
visuales con el movimiento como la tarea de predecir qué
transformaciones sufren las imágenes a medida que el agente se mueve.


Ese agente móvil es modelado con Redes Neurales Convolucionales que
optimizan sus representaciones visuales minimizando el error entre la
informacion de automovimiento provista por el sistema y la información
de automovimiento predecida utilizando las imágenes de entrada
solamente. Este procedimiento es, según Agrawal, equivalente a
entrenar una Red Convolucional con dos flujos de entrada (o sea, una
Red Neural Convolucional Siamesa) en el que se procesan dos imágenes
desplazadas y la red predice la transformación que ocurrió entre ambas
imágenes.

Bajo la hipótesis de que esto forzaría al agente a aprender
\textit{features} útiles en la identificación de elementos visuales
que esten presentes en ambas imágenes, entrenaron diferentes versiones
de redes siamesas con distintos conjuntos de datos y obtuvieron
resultados comparables al estado del arte sin utilizar conjuntos de
datos masivos.

*** Nomenclatura

Cada componente de la red siamesa se denotará con BCNN por sus siglas
en inglés \textit{Base CNN}. Las \textit{features} extraídas de estas
redes serán concatenadas y pasadas a otra red llamada TCNN
(\textit{Top CNN}). En la TCNN es donde colocaremos nuestras funciones
de pérdida. Las BCNN son las redes sobre las que luego haremos
transferencia de aprendizaje.

Para dar una idea de las arquitecturas de cada red de manera sencilla vamos a
usar las siguientes abreviaciones:

\begin{itemize}
\item C\(k\) para una capa convolucional con \(k\) filtros cuadrados.
\item F\(k\) para una capa completamente conectada (FC) con salida de dimensión \(k\).
\item P para una capa de \textit{Pooling}. A menos que se diga lo contrario, siempre usaremos \textit{MAX Pooling}.
\item D para una capa \textit{Dropout}.
\item Op para la capa de salida. En general nuestras capas de salida van a estar conformadar por una F\(k\) (\(k\) es el número de clases) seguidas por una capa Softmax.
\end{itemize}

Al igual que en \cite{LSM2015}, y como es usual cuando se entrenan
CNN's, colocaremos rectificadores lineales ReLU's luego de cada capa
convolucional y cada cada FC.

*** Medidas de similaridad: \textit{Slow Feature Analysis} y Automovimiento

Agrawal et al. proponen analizar el automovimiento del agente para
aprender \textit{features} útiles y lo comparan con otro método muy
utilizado en redes siamesas, \textit{Slow Feature Analysis} (SFA), que
formulan de la siguiente manera:

\begin{equation}
L(x_{t_1}, x_{t_2}, W) = \begin{cases}
                           D(x_{t_1}, x_{t_2}),& \text{si} |t_1 - t_2| \leq T \\ 
                           1 - \max{(0, m - D(x_{t_1}, x_{t_2}))},& \text{si} |t_1 - t_2| > T
                         \end{cases}
\end{equation}

Donde \(x_{t_1}\) y \(x_{t_2}\) son las \textit{features} obtenidas
por las redes siamesas en los tiempos \(t_1\) y \(t_2\), con
parámetros compartidos \(W\), \(m\) es el margen y \(T\) es el umbral
que determina si dos \textit{features} se consideran cercanas. La
distancia \(D\) se eligió como la norma L2. SFA se basa en que las
características relevantes cambian poco en una ventana de tiempo
pequeña.

Por otro lado, la técnica de analizar el automovimiento se plantea
como una tarea de clasificación en donde las redes tienen que predecir
cuál transformación fue efectuada en cada uno de los ejes. Para ello
agrupan todas las posibles transformaciones en clases de rangos
iguales, y la tarea de la red es predecir a que clase pertenece la
imagen.

#+LaTeX: \newpage

* Experimentos
#+LaTeX: \label{sec:agrawal}

Todos los experimentos realizados se encuentran en Github
[fn::https://github.com/ezetl/deep-learning-techniques-thesis].
Dentro del repositorio se encuentra toda la documentación pertinente a
la descarga de los conjuntos de datos y la reproducción de los
experimentos. Parte del tiempo fue dedicado a tratar de que los
experimentos sean reproducibles por otras personas, por lo que se
trató de seguir buenas prácticas a la hora de modularizar y organizar
los el código.

Los experimentos fueron desarrollados en su mayoría en el lenguaje
Python, excepto por algunos scripts de preprocesamiento que
originalmente fueron creados en C++.

** Prueba de concepto con MNIST
*** Conjunto de datos

El conjunto de datos MNIST \cite{mnist} cuenta con 60000 imágenes de
caracteres numéricos manuscritos para entrenamiento, más 10000
imágenes para evaluación. La dimensión de cada imagen es \(1 \times 28
\times 28\).  Para el entrenamiento mediante \textit{automovimiento}
se crearon pares de imágenes siguiendo los lineamientos del
paper. Esto significa que cada par está compuesto de la imagen
original y la imagen con transformaciones en los ejes X, Y, Z. Las
transformaciones en X e Y son traslaciones de 3 píxeles, mientras que
la rotación en Z varía entre los -30° y los 30°. Tanto las rotaciones
como las traslaciones son números enteros. Para cada par creado las
transformaciones se eligen de manera aleatoria uniforme.

En la Figura \ref{fig:mnist-sample} se pueden observar varios pares de
imágenes generadas durante la creación de la base de datos que luego
fue usada para entrenar la red siamesa.

#+BEGIN_LaTeX
\begin{figure}
\centering
\resizebox{.9\linewidth}{!}{
\begin{tabular}{cccccc}
\includegraphics[width = 1.5in]{./images/mnist/a1.png} &
\includegraphics[width = 1.5in]{./images/mnist/a.png} &
\includegraphics[width = 1.5in]{./images/mnist/b1.png} &
\includegraphics[width = 1.5in]{./images/mnist/b.png} &
\includegraphics[width = 1.5in]{./images/mnist/c.png} &
\includegraphics[width = 1.5in]{./images/mnist/c1.png}\\
\end{tabular}
}
\caption{\fontsize{9}{11}\selectfont Ejemplo de transformaciones aplicadas a las imagenes de MNIST. Cada par contiene la imagen original y la transformada con traslaciones en los ejes X e Y, y con rotación en Z.}
\label{fig:mnist-sample}
\end{figure}
#+END_LaTeX

*** Descripción de la red

La arquitectura utilizada para las BCNN fue C96-P-C256-P, y para la
TCNN se eligió F1000-D-Op. Tener en cuenta que para el caso del
\textit{automovimiento} es necesario utilizar una combinación
FC-Softmax para calcular la pérdida en cada una de las
transformaciones.

Para transferencia de aprendizaje se añadió F500-D-F10-Softmax a una
BCNN.

*** Entrenamiento y evaluación

Las redes siamesas se pre-entrenaron durante 40000 iteraciones con una
tasa de aprendizaje de 0.01. Siguiendo los lineamientos del paper se
utilizaron márgenes \(m\) de 10 y 100 para SFA por ser los que mejores
resultados lograron. En ambas redes la tasa de aprendizaje se reduce a
la mitad cada 10000 iteraciones. El tamaño del \textit{mini batch} fue
de 125, lo cual equivale a procesar 5 millones de pares de imágenes
durante las 40000 iteraciones del entrenamiento.

La etapa de transferencia de aprendizaje se hizo con 4000 iteraciones
a una tasa de aprendizaje constante de 0.01.

Para evaluar la calidad de las \textit{features} aprendidas por las
redes siamesas se estableció la tasa de aprendizaje de las capas
convolucionales a cero.

En el Cuadro \ref{tab:one} se puede observar la exactitud obtenida
mediante la transferencia de aprendizaje con 100, 300, 1000 y 10000
imágenes de los dos métodos utilizados (automovimiento y SFA) y una
comparación con un entrenamiento desde cero utilizando esa misma
cantidad de imágenes.

#+BEGIN_LaTeX
\begin{table*}[htb]
\centering
\begin{tabular}{l|rrrr}
\hline
\multicolumn{1}{r}{}
& \multicolumn{4}{c}{datos entrenamiento}
& \multicolumn{1}{l}{Método}
& \multicolumn{1}{r}{100}
& \multicolumn{1}{r}{300}
& \multicolumn{1}{r}{1000}
& \multicolumn{1}{r}{10000} \\ \cline{1-5}
\hline
Desde cero & 0.42 & 0.70 & 0.82 & 0.97\\
SFA(m=10) & 0.52 & 0.71 & 0.77 & 0.82\\
SFA(m=100) & 0.58 & 0.73 & 0.80 & 0.88\\
Automovimiento & 0.75 & 0.90 & 0.92 & 0.99\\
\hline
\end{tabular}%
\caption{\fontsize{9}{11}\selectfont \label{tab:one} Exactitud de los dos métodos de pre-entrenamiento utilizados (SFA y automovimiento).}
\end{table*}
#+END_LaTeX

Se puede observar que entrenar mediante automovimiento presenta una
performance claramente superior a entrenar una red desde cero con la
misma cantidad de imágenes en los casos en los que el conjunto de
datos es relativamente pequeño. Es también superior al entrenamiento
utilizando \textit{Slow Feature Analysis}, y dado que no se
modificaron los pesos de las capas convolucionales aprendidas durante
el pre-entrenamiento, podemos concluir que las \textit{features}
aprendidas son buenas y logran captar las representaciones necesarias
para el domino del problema en cuestión. El siguiente paso es
verificar que efectivamente las \textit{features} aprendidas se puedan
aplicar a diferentes dominios de problemas y sean lo suficientemente
generalizables.

** Pruebas con KITTI
*** Conjunto de datos

El conjunto de datos KITTI \cite{Geiger2012CVPR} consiste en 11
secuencias que registran el movimiento de un automóvil en una
ciudad. Además de proveer cuadros de video, se encuentra la
información odométrica recolectada por sensores montados en el
automóvil. Esa misma información es la que usa Agrawal et al. a la
hora de computar las transformaciones en la cámara entre pares de
imágenes, y es la que intentaremos reproducir en esta sección.

Se asume que la dirección a la que apunta la cámara es el eje Z y el
plano de la imagen es el plano XY (ejes horizontales y
verticales). Dado que las transformaciones más significativas de la
cámara ocurren en los ejes Z/X (a medida que el automóvil avanza por
la calle) y sobre el eje Y (cuando el automóvil gira), sólo se tomaron
en cuenta esas tres dimensiones a la hora de analizar las
transformaciones.

Nuevamente, la predicción de transformaciones se establece como una
tarea de clasificación, esta vez con 20 clases para las
transformaciones en cada eje. Siguiendo los lineamientos originales
del paper, los pares de entrenamiento se tomaron de cuadros separados
a lo sumo por 7 cuadros intermedios. Similarmente, para entrenamiento
por SFA se consideraron a los cuadros separados por \(\pm 7\) cuadros
intermedios como similares.

Finalmente, las redes siamesas fueron entrenadas a partir de parches
de \(227 \times 227\) extraídos aleatoriamente de las imágenes
originales de \(1241 \times 376\) píxeles. No se aplicaron
transformaciones extras más allá de las otorgadas por el movimiento de
la cámara.

En la Figura \ref{fig:kitti-sample} se pueden observar pares de
imágenes utilizados durante el entrenamiento de las redes siamesas.


#+BEGIN_LaTeX
\begin{figure}
\centering
\resizebox{.9\linewidth}{!}{
\begin{tabular}{cccccc}
\includegraphics[width = 1.5in]{./images/kitti/a.png} &
\includegraphics[width = 1.5in]{./images/kitti/a1.png} &
\includegraphics[width = 1.5in]{./images/kitti/b.png} &
\includegraphics[width = 1.5in]{./images/kitti/b1.png} &
\includegraphics[width = 1.5in]{./images/kitti/c.png} &
\includegraphics[width = 1.5in]{./images/kitti/c1.png}\\
\end{tabular}
}
\caption{\fontsize{9}{11}\selectfont Ejemplo de imágenes extraídos de KITTI. Las transformaciones en X, Y y Z se obtuvieron de las anotaciones provistas por los creadores del conjunto de datos. Para las rotaciones en el eje Y se calculó el ángulo de Euler correspondiente al cambio entre dos \textit{frames}.}
\label{fig:kitti-sample}
\end{figure}
#+END_LaTeX

*** Odometria 

KITTI provee anotaciones con las poses de la camara (es decir, su
trayectoria) para las secuencias de imágenes. Estas poses estan dadas
por una matriz de transformación de \(3 \times 4\), siendo la última
columna las traslaciones en X, Y, Z. De esa matriz se puede extraer el
ángulo de Euler correspondiente a las rotaciones sobre el eje Y si
asumimos que el primer bloque \(3 \times 3\) es una matriz de rotación
\(R\).

*** Descripción de la red

La red utilizada como base de las BCNN está inspirada en las primeras
5 capas convolucionales de AlexNet \cite{alexnet}, es decir,
C96-P-C256-P-C384-C384-C256-P. La TCNN fue definida como
C256-C128-F500-D-Op, con filtros convolucionales de \(3 \times 3\).

*** Entrenamiento y evaluación

Se pre-entrenaron las redes siamesas por 60K iteraciones con un tamaño
de \textit{mini batch} de 125 y una tasa de aprendizaje inicial de
\(0.001\), reducida en un factor de 2 cada 20K iteraciones. La
transferencia de aprendizaje se hizo durante 10K iteraciones a una
tasa de aprendizaje constante de 0.001. Para diferenciar los distintos
entrenamientos, al modelo entrenado con SFA lo vamos a llamar
KITTI-SFA y al entrenado con automovimiento, KITTI-EGO.

Para tener un baseline adecuado, se entrenó AlexNet con el conjunto de
datos ILSVRC'12 desde cero utilizando 20 y 1000 imágenes por clase.
El conjunto de datos ILSVRC es el usado en la competencia anual de
Imagenet y contiene mil clases de objetos distintas. Dichos modelos
seran llamados ALEX-20 y ALEX-1000 respectivamente.

Para hacer una comparación justa con ALEX-20 y ALEX-1000, las redes
siamesas fueron entrenadas con aproximadamente 20K pares de
imágenes.

**** Evaluación utilizando el conjunto de datos SUN-397

El conjunto de datos SUN-397 \cite{sun397} consiste de 397 categorías
de paisajes interiores y exteriores y además provee 10 particiones del
dataset para hacer \textit{cross-validation}, pero debido a lo costoso
que es entrenar redes neuronales convolucionales solo se utilizaron
tres particiones. En la Figura \ref{fig:sun-sample} se pueden observar
algunas de las clases que provee este conjunto de datos.

#+BEGIN_LaTeX
\begin{figure}
\centering
\resizebox{.9\linewidth}{!}{
\begin{tabular}{cccc}
\includegraphics[width = 1.5in]{./images/sun397/mountain_snowy.jpg} &
\includegraphics[width = 1.5in]{./images/sun397/oilrig.jpg} &
\includegraphics[width = 1.5in]{./images/sun397/nuclear_power_plant.jpg} &
\includegraphics[width = 1.5in]{./images/sun397/rock_arch.jpg} \\
\includegraphics[width = 1.5in]{./images/sun397/subway_station.jpg} &
\includegraphics[width = 1.5in]{./images/sun397/kennel.jpg} &
\includegraphics[width = 1.5in]{./images/sun397/pilothouse.jpg} &
\includegraphics[width = 1.5in]{./images/sun397/abbey.jpg} \\
\includegraphics[width = 1.5in]{./images/sun397/vegetable_garden.jpg} &
\includegraphics[width = 1.5in]{./images/sun397/music_studio.jpg} &
\includegraphics[width = 1.5in]{./images/sun397/waterfall.jpg} &
\includegraphics[width = 1.5in]{./images/sun397/building.jpg}\\
\end{tabular}
}
\caption{\fontsize{9}{11}\selectfont Ejemplo de clases del conjunto de datos SUN-397: paisajes exteriores, interiores, construcciones, etc.}
\label{fig:sun-sample}
\end{figure}
#+END_LaTeX

La evaluación se hizo midiendo la exactitud de clasificadores
\textit{Softmax} utilizando las \textit{features} obtenidas de las
salidas de las primeras 5 capas convolucionales. Los resultados pueden
verse en el Cuadro \ref{tab:two}

#+BEGIN_LaTeX
\begin{table*}[htb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|r|r|ccccc|r|ccccc}
\hline
\multicolumn{1}{l}{Método}
& \multicolumn{1}{r}{\#preentr.}
& \multicolumn{1}{r}{\#finet.}
& \multicolumn{1}{c}{L1}
& \multicolumn{1}{c}{L2}
& \multicolumn{1}{c}{L3}
& \multicolumn{1}{c}{L4}
& \multicolumn{1}{c}{L5}
& \multicolumn{1}{r}{\#finet.}
& \multicolumn{1}{c}{L1}
& \multicolumn{1}{c}{L2}
& \multicolumn{1}{c}{L3}
& \multicolumn{1}{c}{L4}
& \multicolumn{1}{c}{L5} \\ \cline{1-14}
\hline

ALEX-1000 & 1M & 5 & 3.73 & 5.07 & 5.07 & 8.53 & 10.40 & 20 & 9.07 & 12.53 & 16.27 & 17.60 & 10.67\\
ALEX-20 & 20K & 5 & 2.93 & 1.87 & 3.73 & 5.07 & 3.20 & 20 & 6.13 & 5.33 & 5.33 & 4.53 & 5.07\\
KITTI-SFA & 20.7K & 5 & 2.13 & 3.20 & 2.40 & 1.60 & 1.87 & 20 & 4.53 & 3.73 & 2.13 & 2.40 & 2.93\\
KITTI-EGO & 20.7K & 5 & 2.93 & 1.87 & 3.20 & 5.87 & 1.33 & 20 & 6.67 & 7.47 & 9.87 & 9.33 & 4.00\\

\hline
\end{tabular}%
}
\caption{\fontsize{9}{11}\selectfont \label{tab:two}Exactitud de pre-entramiento con Egomotion y SFA comparado con un entrenamiendo de cero de AlexNet utilizando el conjunto de datos ILSVRC'12. L1-L5 significa que se agregó un clasificador a la salida de las capas convolucionales 1 a 5. Se reporta la exactitud de cada clasificador utilizando el conjunto de datos SUN397.}
\end{table*}
#+END_LaTeX

Se puede observar que el pre-entrenamiento con automoviento no supera
la performance que se obtiene al pre-entrenar la misma red con
ILSVRC'12 con 1000 imagenes por clase. Sin embargo, sí supera la
performance de pre-entrenar la red con 20 imágenes por clase en
ILSVRC'12. No solamente eso sino que también supera la performance de
la red siamesa entrenada con SFA. Esto nos indica que cuando se tiene
un conjunto acotado de datos de entrenamiento, se puede lograr
entrenar un modelo que logre resultados similares al estado del arte
si utilizamos redes siamesas entrenadas con automovimiento.

**** Evaluacion utilizando el conjunto de imagenes Imagenet ILSVRC 2012

Para evaluar que los filtros aprendidos mediante automovimiento son
buenos para tareas de clasificación, se procedió a realizar
transferencia de aprendizaje en todas las capaz convolucionales (es
decir, reentrenar toda la red utilizando un nuevo conjunto de datos).

Se evaluó la exactitud de una red preentrenada con automovimiento
contra una entrenada con \textit{SFA} y una con pesos inicializados
aleatoriamente. Para ello se utilizó el conjunto de datos ILSVRC 2012
utilizado en la competencia de Imagenet. Dicho conjunto de datos
cuenta con 1000 clases de objetos. Ejemplos de clases en este conjunto
de datos se pueden ver en la Figura \ref{fig:imagenet-sample}. 

#+BEGIN_LaTeX
\begin{figure}
\centering
\resizebox{.9\linewidth}{!}{
\begin{tabular}{cccc}
\includegraphics[width = 1.5in]{./images/imagenet/n01531178_108.JPEG} &
\includegraphics[width = 1.5in]{./images/imagenet/n01675722_126.JPEG} &
\includegraphics[width = 1.5in]{./images/imagenet/n01753488_177.JPEG} &
\includegraphics[width = 1.5in]{./images/imagenet/n01773797_48.JPEG} \\
\includegraphics[width = 1.5in]{./images/imagenet/n02859443_1229.JPEG} &
\includegraphics[width = 1.5in]{./images/imagenet/n03000134_789.JPEG} &
\includegraphics[width = 1.5in]{./images/imagenet/n03017168_743.JPEG} &
\includegraphics[width = 1.5in]{./images/imagenet/n03085013_2149.JPEG} \\
\includegraphics[width = 1.5in]{./images/imagenet/n03089624_632.JPEG} &
\includegraphics[width = 1.5in]{./images/imagenet/n03207941_946.JPEG} &
\includegraphics[width = 1.5in]{./images/imagenet/n03218198_298.JPEG} &
\includegraphics[width = 1.5in]{./images/imagenet/n04154565_5799.JPEG} \\
\end{tabular}
}
\caption{\fontsize{9}{11}\selectfont Ejemplo de clases del conjunto de datos ILSVRC'12.}
\label{fig:imagenet-sample}
\end{figure}
#+END_LaTeX

Para el entrenamiento de las redes se utilizaron subconjuntons de
todas las clases con 1, 5, 10, 20 y 1000 elementos por cada una.  Los
resultados se muestran en el Cuadro \ref{tab:three}. Se puede observar
que los pesos de una red pre-entrenada con automovimiento (KITTI-EGO)
supera en todos los casos a los pesos inicializados aleatoriamente
(ALEXNET), mientras que los pesos aprendidos mediante \textit{SFA}
(KITTI-SFA) presentan un rendimiento incluso peor que el de ALEXNET.

#+BEGIN_LaTeX
\begin{table*}[htb]
\centering
\begin{tabular}{l|ccccc}
\hline
\multicolumn{1}{l}{Método}
& \multicolumn{1}{c}{1}
& \multicolumn{1}{c}{5}
& \multicolumn{1}{c}{10}
& \multicolumn{1}{c}{20}
& \multicolumn{1}{c}{1000} \\ \cline{1-6}
\hline

KITTI-EGO & 0.49 & 1.27 & 2.14 & 4.13 & 20.8\\
KITTI-SFA & 0.35 & 0.75 & 1.34 & 2.64 & 11.83\\
ALEXNET & 0.45 & 0.95 & 1.91 & 3.69 & 18.35\\

\hline
\end{tabular}%
\caption{\fontsize{9}{11}\selectfont \label{tab:three}Exactitud de los modelos aprendidos mediante redes siamesas (KITTI-EGO corresponde a la red de automovimiento, KITTI-SFA a la que utiliza \textit{slow feature analysis}) contra una red cuyos pesos fueron inicializados aleatoriamente (ALEXNET). Se entrenaron las 3 redes utilizando el conjunto de datos ILSVRC 2012 con 1, 5, 10, 20 y 1000 imágenes por clase.}
\end{table*}
#+END_LaTeX

#+LaTeX: \newpage

* Conclusiones
#+LaTeX: \label{sec:concl}

Si bien actualmente se suele recurrir a la transferencia de
aprendizaje de redes del estado del arte pre-entrenadas por otras
personas (por ejemplo, por grandes empresas como Google, Facebook, o
algunas universidades), puede haber casos en los que se requiera
entrenar los pesos de una red desde cero. Y como ya se dijo, los
conjuntos de datos para tareas específicas suelen ser muy reducidos
como para obtener buenos resultados con redes convolucionales
profundas.

En este trabajo se propuso reproducir algunos de los resultados de
Agrawal et al. \cite{LSM2015}.  Se logró demostrar la hipótesis
principal de ese trabajo en cada uno de los experimentos: que
utilizando información de \textit{automovimiento} libremente
disponible se puede entrenar una red neuronal convolucional profunda y
obtener resultados similares a los del estado del arte. Los
experimentos propuestos aquí son todas tareas de clasificación, aunque
bien podría extenderse este trabajo a otros problemas.

Se demostró que utilizando unos 20 mil pares de imágenes durante el
pre-entrenamiento de una red (mediante redes siamesas) los resultados
son equiparables a los que se obtendrían entrenando normalmente
utilizando clasificación, lo cual hace al entrenamiento con
\textit{automovimiento} un candidato ideal para para los tipos de
tareas mencionados.

* Trabajo a futuro
#+LaTeX: \label{sec:future}

Hay que destacar que todas las pruebas se hicieron con arquitecturas
basadas en AlexNet, que ya resulta anticuada en lo que se refiere al
estado del arte, donde las redes se han vuelto más profundas y
complejas. Sin embargo puede haber casos en los que se busque una red
menos profunda por cuestiones de rendimiento en velocidad,
característica de los datos, etc. Queda como trabajo a futuro probar
nuevas arquitecturas con la metodología propuesta.

Además sólo se probó la técnica propuesta en Agrawal et
al. \cite{LSM2015} con pocos conjuntos de datos por cuestión de tiempo
(ILSVRC'12, SUN-397, MNIST, KITTI). Probar esta técnica en otros
dominios de problemas queda como trabajo a futuro.


#+LaTeX: \newpage

#+LaTeX: \bibliographystyle{babplain}
#+LaTeX: \bibliography{draft}

* Plan de Trabajo                                :noexport:
** Backlog
*** DONE Hacer visualizador de lmdbs para chequear que esten bien guardados los datos (basarse en el que ya hice para mnist)
*** DONE Abstraer creacion de lmdb en una clase.
*** DONE agregar label como parametro al insert_db para poder unificar SFA y egomotion
*** DONE Integrar SFA y egomotion en una sola base de datos en cada script de preprocessing
*** DONE Agregar path de lmdb como argumentdo command line del preprocess_...
*** DONE Remover Caffe de los "preprocess_mnist..etc". Ahora se linkea desde lmdb_creator
*** DONE [100%] Hacer las redes programatically, para poder automatizar los entrenamientos
**** DONE Egomotion para kitti y mnist, alexnet 
**** DONE SFA para kitti y mnist
**** DONE Standar para kitti
**** DONE Standar para mnist
*** DONE Agregar docs para lo que falta de preprocessing (sun, kitti) 
*** DONE [100%] Completar el pipeline de entrenamiento en Python:
**** DONE crear nets
**** DONE solver
**** DONE entrenar
**** DONE finetunear
**** DONE guardar best snapshot, no usar siempre el ultimo
**** DONE Armar todos los entrenamientos para MNIST
**** DONE mostrar resultados
*** DONE Guardar resultados de los entrenamientos grandes (siamese, imagenet) con pickle. Cargar los archivos si existen para evitar reentrenar todo si se corto un entrenamiento 
*** DONE Agregar experimentos de AlexNet con 20k y 1m de imagenes de Imagenet para SUN397
*** DONE Agregar el finetuning con las 5 ultimas capas para SUN397
*** DONE MNIST: cont_10 uso batch size de 800, hacer lo mismo para cont_100
*** DONE Hacer video con evolucion de los filtros a lo largo de las iteraciones. Puede ser util para mostrar que los filtros se aprenden mas rapido/lento/etc. 
*** TODO Hacer plots de loss al finalizar el entrenamiento. Guardar .pdf
*** DONE Actualizar script de MNIST con los nuevos cambios (pickle, defaultdict)
*** DONE Hacer que se continue el entrenamiento desde el solverstate si se corta (leer del pickle y chequear si es la ultima iteracion)
*** DONE probar con el modelo de lsm: extraer sus dimensiones y replicar en mis redes. CONCLUSION: las redes son identicas
*** DONE Solucionar problema de overfitting en finetuning de kitti
*** DONE Hacer test al finalizar entrenamiento, imprimir en pantalla log.
*** DONE Guardar logs de cada entrenamiento (o guardar pickle con los datos de loss y snapshots)
** Datasets
1. MINST: http://yann.lecun.com/exdb/mnist/

2. KITTI (odometry): http://www.cvlibs.net/datasets/kitti/eval_odometry.php

3. SF: https://embed.stanford.edu/iframe/?url=https%3A%2F%2Fpurl.stanford.edu%2Fvn158kj2087

4. ILSVRC2012: http://www.image-net.org/challenges/LSVRC/2012/nonpub-downloads (hay que crearse una cuenta)

5. SUN Dataset (Scene categorization) http://vision.princeton.edu/projects/2010/SUN/

** Prueba de Concepto con MNIST

EL objetivo es entender como hacer redes siamesas con Caffe

*** [100%] Preprocesamiento
Train set: 60K imagenes. Test set 10K imagenes.

Para pretraining de la red, hay que hacer un preprocesamiento del
dataset:
  1. Traslación relativa en un rango de [-3,3]
  2. Rotación relativa en un rango de [-30°,30°]
Tanto las traslaciones como las rotaciones fueron hechas con numeros
enteros (es decir, los angulos son -30º,-29º,-28º,..,29,30; igual para
las traslaciones).

Para cada par nuevo creado se eligieron las traslaciones y rotaciones
de manera aleatoria, de manera que cada par esta compuesto por la
imagen original y la imagen transformada aleatoriamente.

En total entrenaron con 5 millones de pares de imagenes. Eso significa
que hay que hacer 85 pares para 10000 imagenes y 83 pares para las
otras 50000 imagenes.

DUDA: no queda claro en el paper si el par esta compuesto por la
imagen original mas la imagen transformada o si esta compuesto por una
imagen previamente transformada a la cual se le hace una
transformacion relativa. Por lo pronto voy a efectuar el experimento
con la original+transformada.
**** DONE Crear codigo (c++) para levantar MNIST y devolver una lista de Mat's
**** DONE Modificar codigo para crear las transformaciones de rotacion y traslacion (OpenCV)
**** DONE Hacer que las rotaciones y traslaciones sean aleatorias
**** DONE Modificar codigo para guardar lmdb
**** DONE Modificar algunos aspectos de la parte de lmdb para guardar las nuevas imagenes (dos imagenes mergeadas en una, dimensiones: 28x28x2)
**** DONE Investigar como guardar multiples etiquetas por imagen (traslacion y rotacion)
https://groups.google.com/forum/#!searchin/caffe-users/multilabel/caffe-users/RuT1TgwiRCo/hoUkZOeEDgAJ
https://github.com/BVLC/caffe/issues/2407

Se probaron varias cosas:
***** Armar una DB para cada label (X,Y y Z) pero no anduvo
***** Armar una sola DB para todas las labels y despues slicearla pero no anduvo
para cada clase crear array de bytes asi: 0 0 0 1 donde si hay un cero
significa que esa label esta inactiva y si hay 1 significa que esta
activa

Va a haber un solo 1 por clase claramente

Entonces agarro y creo la base de datos de labels nomas, con la
dimension a lo ancho que sea la dimension de la clase de mayor tama;o
(en este caso las rotaciones, que son 61) y la profundidad que sea la
cantidad de clases a clasificar (en este caso 3: X, Y y Z).

Luego Sliceo la base de datos y con la capa ArgMax obtengo el indice
del mayor elemento de ese arreglo, que termina siendo la clase a la
que corresponde la imagen. A ese argmax obtenido lo mando a la capa de
accuracy/softmax por ejemplo

***** Armar una sola DB para datos y labels y despues slicear los labels
*** [100%] Entrenamiento
**** DONE Armar la red (.prototxt) teniendo en cuenta el Slice, los multiple Softmax y loss
Slice de los labels:
https://groups.google.com/forum/#!searchin/caffe-users/multilabel/caffe-users/RuT1TgwiRCo/hoUkZOeEDgAJ

BCCN: C96-P-C256-P
TCCN: F1000-D-Op

Para finetuning: BCCN-F500-D-Op

Para SFA, los valores optimos del parámetro m fueron 10 y 100.

La predicción es clasificación con tres capas soft-max loss (para
traslaciones en X,Y y rotacion en Z respectivamente). Cada SCNN
minimiza la suma de estas tres ``losses''.

Para que se pueda utilizar clasificación, hay que dividir los rangos
de traslación en en 7 classes y las rotaciones en 20 clases (donde
cada una corresponde a 3°)

Para MNIST, hay que tomar a las imágenes cuya traslación relativa esté
entre [-1,1] y rotación relativa entre [-3°,3°] como temporalmente
cercanas (es el parámetro T de la ecuación en la sección 3.3 del
paper).

**** DONE Iteraciones, learning rate, step
Para pretraining: 40K iteraciones con learning rate inicial de 0.01,
reducido en un factor de 2 cada 10K iteraciones.

Para finetuning: 4K iteraciones con un learning rate constante de
0.01.
**** DONE [100%] Experimentos a realizar
Todos se basan en el pre-entrenamiento descripto en el item y luego
finetuning.

Tanto para egomotion como para SFA, el pretraining se hizo durante 40K
iteraciones, con batches de 125 (5millones de imagenes.)

***** DONE Crear lmdb con 100, 300, 1000 y 10000 imagenes para testing
***** DONE Crear lmdb con 5m de pares de imagenes para egomotion
***** DONE Entrenar Egomotion, lr 0.01. 40K iter
***** DONE Entrenar Egomotion, lr 0.001. 40K iter
***** DONE Entrenar SFA, lr 0.01. 40K iter
***** DONE Entrenar SFA, lr 0.001. 40K iter

***** DONE Egomotion, lr 0.01. Finetuning con 100 imagenes lr 0.01
***** DONE Egomotion, lr 0.01. Finetuning con 300 imagenes lr 0.01
***** DONE Egomotion, lr 0.01. Finetuning con 1000 imagenes lr 0.01
***** DONE Egomotion, lr 0.01. Finetuning con 10000 imagenes lr 0.01
***** DONE Egomotion, lr 0.01. Finetuning con 100 imagenes lr 0.001
***** DONE Egomotion, lr 0.01. Finetuning con 300 imagenes lr 0.001
***** DONE Egomotion, lr 0.01. Finetuning con 1000 imagenes lr 0.001
***** DONE Egomotion, lr 0.01. Finetuning con 10000 imagenes lr 0.001

***** DONE Egomotion, lr 0.001. Finetuning con 100 imagenes lr 0.01
***** DONE Egomotion, lr 0.001. Finetuning con 300 imagenes lr 0.01
***** DONE Egomotion, lr 0.001. Finetuning con 1000 imagenes lr 0.01
***** DONE Egomotion, lr 0.001. Finetuning con 10000 imagenes lr 0.01
***** DONE Egomotion, lr 0.001. Finetuning con 100 imagenes lr 0.001
***** DONE Egomotion, lr 0.001. Finetuning con 300 imagenes lr 0.001
***** DONE Egomotion, lr 0.001. Finetuning con 1000 imagenes lr 0.001
***** DONE Egomotion, lr 0.001. Finetuning con 10000 imagenes lr 0.001
***** DONE Egomotion, lr 0.001. Finetuning con 100 imagenes lr 0.0001
***** DONE Egomotion, lr 0.001. Finetuning con 300 imagenes lr 0.0001
***** DONE Egomotion, lr 0.001. Finetuning con 1000 imagenes lr 0.0001
***** DONE Egomotion, lr 0.001. Finetuning con 10000 imagenes lr 0.0001

***** DONE SFA, lr 0.01. Finetuning con 100 imagenes lr 0.01
***** DONE SFA, lr 0.01. Finetuning con 300 imagenes lr 0.01
***** DONE SFA, lr 0.01. Finetuning con 1000 imagenes lr 0.01
***** DONE SFA, lr 0.01. Finetuning con 10000 imagenes lr 0.01
***** DONE SFA, lr 0.01. Finetuning con 100 imagenes lr 0.001
***** DONE SFA, lr 0.01. Finetuning con 300 imagenes lr 0.001
***** DONE SFA, lr 0.01. Finetuning con 1000 imagenes lr 0.001
***** DONE SFA, lr 0.01. Finetuning con 10000 imagenes lr 0.001

***** DONE SFA, lr 0.001. Finetuning con 100 imagenes lr 0.01
***** DONE SFA, lr 0.001. Finetuning con 300 imagenes lr 0.01
***** DONE SFA, lr 0.001. Finetuning con 1000 imagenes lr 0.01
***** DONE SFA, lr 0.001. Finetuning con 10000 imagenes lr 0.01
***** DONE SFA, lr 0.001. Finetuning con 100 imagenes lr 0.001
***** DONE SFA, lr 0.001. Finetuning con 300 imagenes lr 0.001
***** DONE SFA, lr 0.001. Finetuning con 1000 imagenes lr 0.001
***** DONE SFA, lr 0.001. Finetuning con 10000 imagenes lr 0.001
***** DONE SFA, lr 0.001. Finetuning con 100 imagenes lr 0.0001
***** DONE SFA, lr 0.001. Finetuning con 300 imagenes lr 0.0001
***** DONE SFA, lr 0.001. Finetuning con 1000 imagenes lr 0.0001
***** DONE SFA, lr 0.001. Finetuning con 10000 imagenes lr 0.0001

***** DONE Entrenamiento standar desde cero, lr 0.01. Finetuning con 100 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.01. Finetuning con 300 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.01. Finetuning con 1000 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.01. Finetuning con 10000 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.001. Finetuning con 100 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.001. Finetuning con 300 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.001. Finetuning con 1000 imagenes
***** DONE Entrenamiento standar desde cero, lr 0.001. Finetuning con 10000 imagenes
**** DONE Probar swapeando orden en los pares de imagenes (1. modificada, 2. original) uniformemente.
**** DONE Probar con conv1: 96 x 11 x 11 (en lugar de 96 x 5 x 5) y conv2: 256 x 5 x 5
**** DONE Probar con conv1: 96 x 16 x 16 (en lugar de 96 x 5 x 5) y conv2: 256 x 7 x 7
**** DONE Probar con conv1: 96 x 13 x 13 y conv2: 256 x 7 x 7. lr0.001 para from scratch parece andar bien.
**** DONE Probar con conv1: 96 x 7 x 7 y conv2: 256 x 5 x 5
**** DONE Probar con los mismos settings que AlexNet(stride, group, etc). Ademas, la parte from scratch no usa preentrenamiento
**** DONE Probar sacando la fc100, poner el concat como bottom de las fc{x,y,z}
**** DONE Probar el codigo de Agrawal, la parte de mnist. Resultado: el codigo es horrible, no pude entender bien nada, logre correr un entrenamiento de MNIST pero los resultados fueron los mismos
**** DONE Probar con menos pares (<85)
**** DONE Seguir probando con la red que viene como ejemplo en caffe.
**** DONE Probar con exageraciones en las traslaciones (multiplicar por 3) -> da lo mismo
Sera que no todas las redes son buenos candidatos a siamesas? por
ejemplo, si tiene muchisimos pesos para aprender tal vez sea mala
candidata para una red siamesa.
*** Evaluación
Las features obtenidas de las BCNN preentrenadas se evaluan teniendo
en cuenta el error en la clasificación de dígitos.  Se utilizan
conjuntos de entrenamiento de 100,300, 1K y 10K obtenidos del training
set de MNIST (sin transformaciones).  El test set que viene con MNIST
se utiliza para testing.

*** Resultados
Los mejores resultados para egomotion se obtienen con lr0.01 en ambas
etapas (egomotion y finetuning). Con capas LRN despues de pooling. Con
normalizacion de los valores (de 255 a 0-1). Para contrastive loss
hubo que utilizar un lr de 0.001 y un batch size mas grande, sino las
redes no convergen.

Accuracies
| stand    | 0.426666667064 |  0.71199999253 | 0.837333321571 | 0.981333355109 |
| cont_10  |  0.53066666921 | 0.722666680813 | 0.826666673024 | 0.879999995232 |
| cont_100 | 0.333333333333 | 0.448000003894 | 0.503999988238 | 0.496000001828 |
| ego      | 0.757333318392 | 0.896000027657 | 0.949333349864 | 0.986666679382 |

** Experimentos con KITTI
*** Preprocesamiento
**** KITTI
Tiene 20501 imágenes. Se calculan las transformaciones entre las
imágenes cercanas utilizando los datos odométricos del dataset.
Similar a MNIST: se crean 20 clases para las transformaciones en X,Y,Z
(el paper no explica como). Se toman imágenes que estén separadas a lo
sumo por +-7 frames.  Para el entrenamiento se extraen patches de
227x227 de las imágenes (Caffe tiene la opcion de cropear la imagen a
la hora de entrenar, pero no se como se aplica a redes siamesas,
probablemente tenga que hacerlo como parte del preprocesamiento).

Para SFA, el threshold para imágenes temporalmente cercanas (T) es
también de +-7
El numero total de imagenes usadas para entrenamiento es 20501

**** SF
Análogo a KITTI, solo que además de las transformaciones en X,Y,Z
agregan los 3 ``euler angles'' (no entendi eso).

*** Odometria
http://robotics.stackexchange.com/questions/7040/how-to-get-the-projection-matrix-from-odometry-tf-data
http://www.staff.city.ac.uk/~sbbh653/publications/euler.pdf

*** Arquitectura
BCNN: C96-P-C256-P-C384-C384-C256-P (dice que estan inspiradas en las
primeras capas de AlexNet, extraer tamaño de filtros de esa red)
TCNN: C256-C128-F500-D-Op. Los kernels convolucionales con 3x3.

*** Entrenamiento
Se entrena por 60K iteraciones con batch size de 128, learning rate
inicial de 0.001 (reducido en un factor de 2 cada 20K iteraciones)

**** DONE Crear dataset para egomotion
**** DONE Entrenar egomotion
**** DONE Probar con modelo preentrenado de ellos
    Anda bien con su modelo, a pesar de que los filtros no parecen ser
    tan buenos. Acc. 5.2 en scene recognition con 5 imagenes per class
    (el mio 3.2).
**** DONE Rever como se hacen los pares para SFA, solo los -+7 frames mas cercanos son similares. Y para egomotion?
**** DONE [#B] Leer scripts de lsm para ver como preprocesaron las imagenes ellos
**** DONE [60%] Comparar como calculo los tags
***** DONE Chequear que se obtengan bien las matrices de los archivos
***** DONE Comparar como calculo la matriz de rotacion en base a las dos imagenes. RESULTADO:  esta bien esa parte (get_pose_label)
***** DONE Comparar como calculo Euler Angles. RESULTADO: esta bien esa parte (mat2euler)
***** TODO [#C] Agregar normalizacion de valores euler/trans??
***** TODO [#B] Chequear como se sacan labels de los datos obtenidos (traslaciones y rotaciones)
**** DONE [#B] Entrenar egomotion con swap entre LRN y pooling (como estan en AlexNet original) (RESULTADOS: lo mismo)
**** DONE Crear dos lmdbs: una para las imagenes y otra para los labels (guardados como data y luego spliteados).
     De esa forma va a ser mas facil restar la media. Centrar los
     datos mediante mean substraction podria aumentar la performance.
**** DONE Crear lmdb para SFA
**** DONE Entrenar SFA
**** DONE Aplicar los cambios de la siamese_egomotion a las otras redes

*** Evaluación
Los modelos KITTI-Net y SF-Net deben ser entrenados utilizando
alrededor de 20K imagenes unicas. Para hacer la comparacion mas justa
con las redes entrenadas con clases de imagenes, un model con AlexNet
sera entrenado con 20K imagenes tomadas de ILSVRC12 (20 ejemplos por
clase).  Las secciones de evaluacion en Intra-Class Keypoint Matching
y Visual Odometry los dejo para mas adelante.
**** Scene Recognition
Utilizar SUN database para el finetuning de las redes (SF-Net,
KITTI-Net y AlexNet-20K). El paper no aporta informacion sobre la
cantidad de iteraciones ni el learning rate usado.  Referirse al paper
para comparar resultados obtenidos.

***** DONE Descargar ILSVRC'12
***** DONE Conseguir AlexNet prototxt
***** DONE Preparar LMDBS de ILSVRC
***** DONE Entrenar AlexNet para clasification con ILSVRC'12 con 20, 10 y 1000 imagenes por clase
***** DONE Preprocesar SUN: crop por el centro con dimension mas chica
***** DONE Crear LMDBS SUN. Utilizar solo 3 splits, como en el paper. Crea un LMDB para cada split
***** DONE Volver a probar su model contra el mio. Probar con y sin pooling en la ultima capa a la hora de finetuning. Quedarse con el mejor.
***** DONE Probar con su deploy identico (finetuneando el de ellos con SUN y el mio con SUN)
***** DONE Modificar KITTI-Net, KITTI-SFA-Net agregando Softmax luego de cada capa convolucional
***** DONE Finetunear KITTI-Net, KITTI-SFA-Net con SUN, una vez con cada uno de los Softmax. Hacer 3 entrenamientos, uno con cada split.
***** DONE Comparar accuracies con AlexNet20, AlexNet1M, Kitti-Net, Kitti-SFA-Net. Usar accuracy normal, no se entiende que usan en el paper
***** DONE Probar con scale, mean substraction
**** Object Recognition
Utilizando subconjuntos de ILSVRC-2012 con 1, 5, 10 y 20 imagenes por
clase, hacer finetuning de KITTI-Net, KITTI-SFA-Net y AlexNet-Scratch
(AlexNet con pesos inicializados de manera aleatoria). Nuevamente el
paper no explica las iteraciones ni el learning rate utilizados.
** Mejoras al sistema
Probar con otras redes.
Otros datasets (elegir datasets pequeños y ver que pasa).


