{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Bringing evolution of conv filters to life with Matplotlib\n",
    "\n",
    "In this notebook Im going to explore how to animate the evolution of convolutional filters throughout the training of a convolutional neural network. By analyzing the first convolutional layer we can spot if the filters are learning how to detect some low level features that can be useful for visual intelligence tasks, like classification. These low level features can be for example, edge detection and color detection. I think this is a good exploratory tool that can shed some light on the training process\n",
    "\n",
    "For this particular notebook, I will use pretrained models using the framework Caffe, although this could be adapted to other frameworks in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Loading pretrained weights\n",
    "\n",
    "Lets load first a list of pretrained weights that I have in a pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best snap', '/media/eze/Datasets/snapshots/kitti/egomotion/kitti_siamese_iter_56000.caffemodel', ' Best snap iteration number: ', 56000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>snaps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.023580</td>\n",
       "      <td>/media/eze/Datasets/snapshots/kitti/egomotion/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.594708</td>\n",
       "      <td>/media/eze/Datasets/snapshots/kitti/egomotion/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.518335</td>\n",
       "      <td>/media/eze/Datasets/snapshots/kitti/egomotion/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.923508</td>\n",
       "      <td>/media/eze/Datasets/snapshots/kitti/egomotion/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.306622</td>\n",
       "      <td>/media/eze/Datasets/snapshots/kitti/egomotion/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss                                              snaps\n",
       "0  9.023580  /media/eze/Datasets/snapshots/kitti/egomotion/...\n",
       "1  5.594708  /media/eze/Datasets/snapshots/kitti/egomotion/...\n",
       "2  5.518335  /media/eze/Datasets/snapshots/kitti/egomotion/...\n",
       "3  5.923508  /media/eze/Datasets/snapshots/kitti/egomotion/...\n",
       "4  5.306622  /media/eze/Datasets/snapshots/kitti/egomotion/..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "#pickle_name = \"/home/eze/Projects/tesis/experiments/results/kitti/egomotion.pickle\"\n",
    "pickle_name = \"/home/eze/Projects/tesis/experiments/results/kitti/egomotion.pickle\"\n",
    "\n",
    "with open(pickle_name, 'rb') as handle:\n",
    "    results = pickle.load(handle)\n",
    "\n",
    "# When using egomotion:\n",
    "loss = zip(list(results['loss']['loss_z'][0::1000])[:-1], list(results['loss']['loss_y'][0::1000])[:-1], list(results['loss']['loss_x'][0::1000])[:-1])\n",
    "results['loss'] = [sum(elem) for elem in loss]\n",
    "results['snaps']= results['snaps'][:-1]\n",
    " \n",
    "# When using contrastive/standar training\n",
    "#results['loss'] = list(results['loss']['loss_z'][0::1000])[:-1]\n",
    "\n",
    "best_snap = results['best_snap']\n",
    "best_snap_iter = int(results['best_snap'].split('iter_')[1].replace('.caffemodel',''))\n",
    "print(\"Best snap\", best_snap, \" Best snap iteration number: \", best_snap_iter)\n",
    "\n",
    "del results['best_snap']\n",
    "#del results['acc']\n",
    "\n",
    "df = pd.DataFrame.from_records(results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Visualize only the evolution of filters\n",
    "\n",
    "We must generate a visualization for each snapshot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import caffe\n",
    "\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "\n",
    "def set_net(deploy, caffemodel, num_channels, im_size):\n",
    "    net = caffe.Net(deploy, caffemodel, caffe.TEST)  # test mode (e.g., don't perform dropout)\n",
    "\n",
    "    # create transformer for the input called 'data'\n",
    "    transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "    transformer.set_transpose('data', (2, 0, 1))  # move image channels to outermost dimension\n",
    "    transformer.set_raw_scale('data', 255)      # rescale from [0, 1] to [0, 255]\n",
    "    if num_channels == 3:\n",
    "        transformer.set_channel_swap('data', (2, 1, 0))  # swap channels from RGB to BGR\n",
    "\n",
    "    net.blobs['data'].reshape(1,                 # batch size\n",
    "                              num_channels,      # 3-channel (BGR) images\n",
    "                              im_size, im_size)  # image size is 227x227\n",
    "    return (net, transformer)\n",
    "\n",
    "\n",
    "def get_conv_filters(net, conv_name):\n",
    "    filters = net.params[conv_name][0].data\n",
    "    return filters.transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.io\n",
    "from os.path import join, exists\n",
    "from os import makedirs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# set display defaults\n",
    "plt.rcParams['figure.figsize'] = (20, 20)        # large images\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap\n",
    "\n",
    "\n",
    "shape = (96, 11, 11, 3)\n",
    "\n",
    "deploy = \"/home/eze/Projects/tesis/experiments/utils/deploys/kitti.prototxt\"\n",
    "imsize = 227\n",
    "imchannel = 3\n",
    "\n",
    "def preprocess_data(caffemodel, imsize, imchannel, deploy):\n",
    "    net, transformer = set_net(deploy, caffemodel, imchannel, imsize)\n",
    "    data = get_conv_filters(net, 'conv1')\n",
    "    # normalize data for display\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = (((0, n ** 2 - data.shape[0]),\n",
    "                (0, 1), (0, 1))                # add some space between filters\n",
    "               + ((0, 0),) * (data.ndim - 3))  # don't pad the last dimension (if there is one)\n",
    "    data = np.pad(data, padding, mode='constant', constant_values=1)  # pad with ones (white)\n",
    "    # tile the filters into an image\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    return data\n",
    "\n",
    "def save_filters_as_jpg(snaps, root_path):\n",
    "    try:\n",
    "        makedirs(root_path)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    for model in snaps:\n",
    "        data = preprocess_data(model, imsize, imchannel, deploy)\n",
    "        plt.imshow(data)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(join(root_path, \"{}.jpg\".format(int(model.split('iter_')[1].replace('.caffemodel','')))))\n",
    "\n",
    "class SubplotAnimation(animation.TimedAnimation):\n",
    "    def __init__(self, losses, caffemodels, imsize, imchannel, deploy_prototxt):\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(1, 2, 1)\n",
    "        ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "        xlim = len(losses)\n",
    "        # caffemodels each 1000 iterations\n",
    "        self.t = np.arange(0,1000*xlim, 1000)\n",
    "       \n",
    "        self.imsize = imsize\n",
    "        self.imchannel = imchannel\n",
    "        self.deploy = deploy_prototxt\n",
    "        self.losses = losses\n",
    "        self.caffemodels = caffemodels\n",
    "        \n",
    "        self.filters = np.array([preprocess_data(caffemodel, self.imsize, self.imchannel, self.deploy) for caffemodel in self.caffemodels])\n",
    "        self.im = ax1.imshow(self.filters[0])\n",
    "        ax1.set_aspect('equal', 'datalim')\n",
    "        ax1.set_axis_off()\n",
    "\n",
    "        ax2.set_xlabel('iterations')\n",
    "        ax2.set_ylabel('loss')\n",
    "        self.line2 = Line2D([], [], color='black')\n",
    "        ax2.add_line(self.line2)\n",
    "        ax2.set_xlim(0, xlim*1000)\n",
    "        ax2.set_ylim(0,max(self.losses)+1)\n",
    "        ax2.spines['top'].set_visible(False)\n",
    "        ax2.spines['right'].set_visible(False)\n",
    "\n",
    "        animation.TimedAnimation.__init__(self, fig, interval=50, blit=True)\n",
    "\n",
    "    def _draw_frame(self, framedata):\n",
    "        i = framedata\n",
    "        self.line2.set_data(self.t[:i], self.losses[:i])\n",
    "        self.im.set_data(self.filters[i])\n",
    "\n",
    "        self._drawn_artists = [self.im, self.line2]\n",
    "        \n",
    "    def new_frame_seq(self):\n",
    "        return iter(range(self.t.size))\n",
    "\n",
    "    def _init_draw(self):\n",
    "        self.line2.set_data([], [])\n",
    "        self.im.set_data(preprocess_data(self.caffemodels[0], self.imsize, self.imchannel, self.deploy))\n",
    "\n",
    "ani = SubplotAnimation(results['loss'], results['snaps'], imsize, imchannel, deploy)\n",
    "#ani.save('filters.gif', writer='imagemagick', fps=10)\n",
    "ani.save('filters_contrastive.mp4', fps=10)\n",
    "#save_filters_as_jpg(results['snaps'], 'filters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
